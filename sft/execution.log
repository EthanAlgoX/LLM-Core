warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.
[INFO|configuration_utils.py:667] 2026-02-15 17:05:23,013 >> loading configuration file config.json from cache at /Users/yunxuanhan/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/config.json
[INFO|configuration_utils.py:739] 2026-02-15 17:05:23,016 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 28,
  "model_type": "qwen3",
  "num_attention_heads": 16,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pad_token_id": null,
  "rms_norm_eps": 1e-06,
  "rope_parameters": {
    "rope_theta": 1000000,
    "rope_type": "default"
  },
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "5.0.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|configuration_utils.py:667] 2026-02-15 17:05:26,638 >> loading configuration file config.json from cache at /Users/yunxuanhan/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/config.json
[INFO|configuration_utils.py:739] 2026-02-15 17:05:26,639 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 28,
  "model_type": "qwen3",
  "num_attention_heads": 16,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pad_token_id": null,
  "rms_norm_eps": 1e-06,
  "rope_parameters": {
    "rope_theta": 1000000,
    "rope_type": "default"
  },
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "5.0.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|configuration_utils.py:667] 2026-02-15 17:05:27,560 >> loading configuration file config.json from cache at /Users/yunxuanhan/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/config.json
[INFO|configuration_utils.py:739] 2026-02-15 17:05:27,561 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 28,
  "model_type": "qwen3",
  "num_attention_heads": 16,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pad_token_id": null,
  "rms_norm_eps": 1e-06,
  "rope_parameters": {
    "rope_theta": 1000000,
    "rope_type": "default"
  },
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "5.0.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|configuration_utils.py:667] 2026-02-15 17:05:32,830 >> loading configuration file config.json from cache at /Users/yunxuanhan/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/config.json
[INFO|configuration_utils.py:739] 2026-02-15 17:05:32,831 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 28,
  "model_type": "qwen3",
  "num_attention_heads": 16,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pad_token_id": null,
  "rms_norm_eps": 1e-06,
  "rope_parameters": {
    "rope_theta": 1000000,
    "rope_type": "default"
  },
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "5.0.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:732] 2026-02-15 17:05:33,569 >> loading weights file model.safetensors from cache at /Users/yunxuanhan/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors
[INFO|modeling_utils.py:801] 2026-02-15 17:05:33,569 >> Will use dtype=torch.bfloat16 as defined in model's config object
[INFO|configuration_utils.py:1014] 2026-02-15 17:05:33,571 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "output_attentions": false,
  "output_hidden_states": false,
  "use_cache": false
}


Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]
Loading weights:   0%|          | 1/311 [00:00<00:00, 13315.25it/s, Materializing param=lm_head.weight]
Loading weights:   0%|          | 1/311 [00:00<00:00, 8830.11it/s, Materializing param=lm_head.weight] 
Loading weights:   1%|          | 2/311 [00:00<01:11,  4.31it/s, Materializing param=lm_head.weight]  
Loading weights:   1%|          | 2/311 [00:00<01:11,  4.31it/s, Materializing param=model.embed_tokens.weight]
Loading weights:   1%|          | 2/311 [00:00<01:11,  4.31it/s, Materializing param=model.embed_tokens.weight]
Loading weights:   1%|          | 3/311 [00:00<01:20,  3.80it/s, Materializing param=model.embed_tokens.weight]
Loading weights:   1%|          | 3/311 [00:00<01:20,  3.80it/s, Materializing param=model.layers.0.input_layernorm.weight]
Loading weights:   1%|          | 3/311 [00:00<01:20,  3.80it/s, Materializing param=model.layers.0.input_layernorm.weight]
Loading weights:   1%|▏         | 4/311 [00:00<01:20,  3.80it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  
Loading weights:   1%|▏         | 4/311 [00:00<01:20,  3.80it/s, Materializing param=model.layers.0.mlp.down_proj.weight]
Loading weights:   2%|▏         | 5/311 [00:00<01:20,  3.80it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]
Loading weights:   2%|▏         | 5/311 [00:00<01:20,  3.80it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]
Loading weights:   2%|▏         | 6/311 [00:00<01:20,  3.80it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  
Loading weights:   2%|▏         | 6/311 [00:00<01:20,  3.80it/s, Materializing param=model.layers.0.mlp.up_proj.weight]
Loading weights:   2%|▏         | 7/311 [00:00<01:19,  3.80it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]
Loading weights:   2%|▏         | 7/311 [00:00<01:19,  3.80it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]
Loading weights:   3%|▎         | 8/311 [00:00<01:19,  3.80it/s, Materializing param=model.layers.0.self_attn.k_norm.weight]        
Loading weights:   3%|▎         | 8/311 [00:00<01:19,  3.80it/s, Materializing param=model.layers.0.self_attn.k_norm.weight]
Loading weights:   3%|▎         | 9/311 [00:00<01:19,  3.80it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]
Loading weights:   3%|▎         | 9/311 [00:00<01:19,  3.80it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]
Loading weights:   3%|▎         | 10/311 [00:00<01:19,  3.80it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]
Loading weights:   3%|▎         | 10/311 [00:00<01:19,  3.80it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]
Loading weights:   4%|▎         | 11/311 [00:00<01:18,  3.80it/s, Materializing param=model.layers.0.self_attn.q_norm.weight]
Loading weights:   4%|▎         | 11/311 [00:00<01:18,  3.80it/s, Materializing param=model.layers.0.self_attn.q_norm.weight]
Loading weights:   4%|▍         | 12/311 [00:00<01:18,  3.80it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]
Loading weights:   4%|▍         | 12/311 [00:00<01:18,  3.80it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]
Loading weights:   4%|▍         | 13/311 [00:00<01:18,  3.80it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]
Loading weights:   4%|▍         | 13/311 [00:00<01:18,  3.80it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]
Loading weights:   5%|▍         | 14/311 [00:00<01:18,  3.80it/s, Materializing param=model.layers.1.input_layernorm.weight] 
Loading weights:   5%|▍         | 14/311 [00:00<01:18,  3.80it/s, Materializing param=model.layers.1.input_layernorm.weight]
Loading weights:   5%|▍         | 15/311 [00:00<01:17,  3.80it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  
Loading weights:   5%|▍         | 15/311 [00:00<01:17,  3.80it/s, Materializing param=model.layers.1.mlp.down_proj.weight]
Loading weights:   5%|▌         | 16/311 [00:00<01:17,  3.80it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]
Loading weights:   5%|▌         | 16/311 [00:00<01:17,  3.80it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]
Loading weights:   5%|▌         | 17/311 [00:00<01:17,  3.80it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  
Loading weights:   5%|▌         | 17/311 [00:00<01:17,  3.80it/s, Materializing param=model.layers.1.mlp.up_proj.weight]
Loading weights:   6%|▌         | 18/311 [00:00<01:17,  3.80it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]
Loading weights:   6%|▌         | 18/311 [00:00<01:17,  3.80it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]
Loading weights:   6%|▌         | 19/311 [00:00<01:16,  3.80it/s, Materializing param=model.layers.1.self_attn.k_norm.weight]        
Loading weights:   6%|▌         | 19/311 [00:00<01:16,  3.80it/s, Materializing param=model.layers.1.self_attn.k_norm.weight]
Loading weights:   6%|▋         | 20/311 [00:00<01:16,  3.80it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]
Loading weights:   6%|▋         | 20/311 [00:00<01:16,  3.80it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]
Loading weights:   7%|▋         | 21/311 [00:00<01:16,  3.80it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]
Loading weights:   7%|▋         | 21/311 [00:00<01:16,  3.80it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]
Loading weights:   7%|▋         | 22/311 [00:00<01:15,  3.80it/s, Materializing param=model.layers.1.self_attn.q_norm.weight]
Loading weights:   7%|▋         | 22/311 [00:00<01:15,  3.80it/s, Materializing param=model.layers.1.self_attn.q_norm.weight]
Loading weights:   7%|▋         | 23/311 [00:00<01:15,  3.80it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]
Loading weights:   7%|▋         | 23/311 [00:00<01:15,  3.80it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]
Loading weights:   8%|▊         | 24/311 [00:00<01:15,  3.80it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]
Loading weights:   8%|▊         | 24/311 [00:00<01:15,  3.80it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]
Loading weights:   8%|▊         | 25/311 [00:00<01:15,  3.80it/s, Materializing param=model.layers.2.input_layernorm.weight] 
Loading weights:   8%|▊         | 25/311 [00:00<01:15,  3.80it/s, Materializing param=model.layers.2.input_layernorm.weight]
Loading weights:   8%|▊         | 26/311 [00:00<01:14,  3.80it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  
Loading weights:   8%|▊         | 26/311 [00:00<01:14,  3.80it/s, Materializing param=model.layers.2.mlp.down_proj.weight]
Loading weights:   9%|▊         | 27/311 [00:00<01:14,  3.80it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]
Loading weights:   9%|▊         | 27/311 [00:00<01:14,  3.80it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]
Loading weights:   9%|▉         | 28/311 [00:00<01:14,  3.80it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  
Loading weights:   9%|▉         | 28/311 [00:00<01:14,  3.80it/s, Materializing param=model.layers.2.mlp.up_proj.weight]
Loading weights:   9%|▉         | 29/311 [00:00<01:14,  3.80it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]
Loading weights:   9%|▉         | 29/311 [00:00<01:14,  3.80it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]
Loading weights:  10%|▉         | 30/311 [00:00<01:13,  3.80it/s, Materializing param=model.layers.2.self_attn.k_norm.weight]        
Loading weights:  10%|▉         | 30/311 [00:00<01:13,  3.80it/s, Materializing param=model.layers.2.self_attn.k_norm.weight]
Loading weights:  10%|▉         | 31/311 [00:00<01:13,  3.80it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]
Loading weights:  10%|▉         | 31/311 [00:00<01:13,  3.80it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]
Loading weights:  10%|█         | 32/311 [00:00<01:13,  3.80it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]
Loading weights:  10%|█         | 32/311 [00:00<01:13,  3.80it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]
Loading weights:  11%|█         | 33/311 [00:00<01:13,  3.80it/s, Materializing param=model.layers.2.self_attn.q_norm.weight]
Loading weights:  11%|█         | 33/311 [00:00<01:13,  3.80it/s, Materializing param=model.layers.2.self_attn.q_norm.weight]
Loading weights:  11%|█         | 34/311 [00:00<01:12,  3.80it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]
Loading weights:  11%|█         | 34/311 [00:00<01:12,  3.80it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]
Loading weights:  11%|█▏        | 35/311 [00:00<01:12,  3.80it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]
Loading weights:  11%|█▏        | 35/311 [00:00<01:12,  3.80it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]
Loading weights:  12%|█▏        | 36/311 [00:00<01:12,  3.80it/s, Materializing param=model.layers.3.input_layernorm.weight] 
Loading weights:  12%|█▏        | 36/311 [00:00<01:12,  3.80it/s, Materializing param=model.layers.3.input_layernorm.weight]
Loading weights:  12%|█▏        | 37/311 [00:00<01:12,  3.80it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  
Loading weights:  12%|█▏        | 37/311 [00:00<01:12,  3.80it/s, Materializing param=model.layers.3.mlp.down_proj.weight]
Loading weights:  12%|█▏        | 38/311 [00:00<01:11,  3.80it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]
Loading weights:  12%|█▏        | 38/311 [00:00<01:11,  3.80it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]
Loading weights:  13%|█▎        | 39/311 [00:00<01:11,  3.80it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  
Loading weights:  13%|█▎        | 39/311 [00:00<01:11,  3.80it/s, Materializing param=model.layers.3.mlp.up_proj.weight]
Loading weights:  13%|█▎        | 40/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.mlp.up_proj.weight]
Loading weights:  13%|█▎        | 40/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]
Loading weights:  13%|█▎        | 40/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]
Loading weights:  13%|█▎        | 41/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.k_norm.weight]        
Loading weights:  13%|█▎        | 41/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.k_norm.weight]
Loading weights:  14%|█▎        | 42/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]
Loading weights:  14%|█▎        | 42/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]
Loading weights:  14%|█▍        | 43/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]
Loading weights:  14%|█▍        | 43/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]
Loading weights:  14%|█▍        | 44/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.q_norm.weight]
Loading weights:  14%|█▍        | 44/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.q_norm.weight]
Loading weights:  14%|█▍        | 45/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]
Loading weights:  14%|█▍        | 45/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]
Loading weights:  15%|█▍        | 46/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]
Loading weights:  15%|█▍        | 46/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]
Loading weights:  15%|█▌        | 47/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.input_layernorm.weight] 
Loading weights:  15%|█▌        | 47/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.input_layernorm.weight]
Loading weights:  15%|█▌        | 48/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  
Loading weights:  15%|█▌        | 48/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.mlp.down_proj.weight]
Loading weights:  16%|█▌        | 49/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]
Loading weights:  16%|█▌        | 49/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]
Loading weights:  16%|█▌        | 50/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  
Loading weights:  16%|█▌        | 50/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.mlp.up_proj.weight]
Loading weights:  16%|█▋        | 51/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]
Loading weights:  16%|█▋        | 51/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]
Loading weights:  17%|█▋        | 52/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.k_norm.weight]        
Loading weights:  17%|█▋        | 52/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.k_norm.weight]
Loading weights:  17%|█▋        | 53/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]
Loading weights:  17%|█▋        | 53/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]
Loading weights:  17%|█▋        | 54/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]
Loading weights:  17%|█▋        | 54/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]
Loading weights:  18%|█▊        | 55/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.q_norm.weight]
Loading weights:  18%|█▊        | 55/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.q_norm.weight]
Loading weights:  18%|█▊        | 56/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]
Loading weights:  18%|█▊        | 56/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]
Loading weights:  18%|█▊        | 57/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]
Loading weights:  18%|█▊        | 57/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]
Loading weights:  19%|█▊        | 58/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.input_layernorm.weight] 
Loading weights:  19%|█▊        | 58/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.input_layernorm.weight]
Loading weights:  19%|█▉        | 59/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  
Loading weights:  19%|█▉        | 59/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.mlp.down_proj.weight]
Loading weights:  19%|█▉        | 60/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]
Loading weights:  19%|█▉        | 60/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]
Loading weights:  20%|█▉        | 61/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  
Loading weights:  20%|█▉        | 61/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.mlp.up_proj.weight]
Loading weights:  20%|█▉        | 62/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]
Loading weights:  20%|█▉        | 62/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]
Loading weights:  20%|██        | 63/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.k_norm.weight]        
Loading weights:  20%|██        | 63/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.k_norm.weight]
Loading weights:  21%|██        | 64/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]
Loading weights:  21%|██        | 64/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]
Loading weights:  21%|██        | 65/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]
Loading weights:  21%|██        | 65/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]
Loading weights:  21%|██        | 66/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.q_norm.weight]
Loading weights:  21%|██        | 66/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.q_norm.weight]
Loading weights:  22%|██▏       | 67/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]
Loading weights:  22%|██▏       | 67/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]
Loading weights:  22%|██▏       | 68/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]
Loading weights:  22%|██▏       | 68/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]
Loading weights:  22%|██▏       | 69/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.6.input_layernorm.weight] 
Loading weights:  22%|██▏       | 69/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.6.input_layernorm.weight]
Loading weights:  23%|██▎       | 70/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  
Loading weights:  23%|██▎       | 70/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.6.mlp.down_proj.weight]
Loading weights:  23%|██▎       | 71/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]
Loading weights:  23%|██▎       | 71/311 [00:00<00:03, 70.75it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]
Loading weights:  23%|██▎       | 72/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]
Loading weights:  23%|██▎       | 72/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  
Loading weights:  23%|██▎       | 72/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.mlp.up_proj.weight]
Loading weights:  23%|██▎       | 73/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]
Loading weights:  23%|██▎       | 73/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]
Loading weights:  24%|██▍       | 74/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.k_norm.weight]        
Loading weights:  24%|██▍       | 74/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.k_norm.weight]
Loading weights:  24%|██▍       | 75/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]
Loading weights:  24%|██▍       | 75/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]
Loading weights:  24%|██▍       | 76/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]
Loading weights:  24%|██▍       | 76/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]
Loading weights:  25%|██▍       | 77/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.q_norm.weight]
Loading weights:  25%|██▍       | 77/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.q_norm.weight]
Loading weights:  25%|██▌       | 78/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]
Loading weights:  25%|██▌       | 78/311 [00:00<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]
Loading weights:  25%|██▌       | 79/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]
Loading weights:  25%|██▌       | 79/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]
Loading weights:  26%|██▌       | 80/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.input_layernorm.weight] 
Loading weights:  26%|██▌       | 80/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.input_layernorm.weight]
Loading weights:  26%|██▌       | 81/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  
Loading weights:  26%|██▌       | 81/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.mlp.down_proj.weight]
Loading weights:  26%|██▋       | 82/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]
Loading weights:  26%|██▋       | 82/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]
Loading weights:  27%|██▋       | 83/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  
Loading weights:  27%|██▋       | 83/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.mlp.up_proj.weight]
Loading weights:  27%|██▋       | 84/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]
Loading weights:  27%|██▋       | 84/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]
Loading weights:  27%|██▋       | 85/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.k_norm.weight]        
Loading weights:  27%|██▋       | 85/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.k_norm.weight]
Loading weights:  28%|██▊       | 86/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]
Loading weights:  28%|██▊       | 86/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]
Loading weights:  28%|██▊       | 87/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]
Loading weights:  28%|██▊       | 87/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]
Loading weights:  28%|██▊       | 88/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.q_norm.weight]
Loading weights:  28%|██▊       | 88/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.q_norm.weight]
Loading weights:  29%|██▊       | 89/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]
Loading weights:  29%|██▊       | 89/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]
Loading weights:  29%|██▉       | 90/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]
Loading weights:  29%|██▉       | 90/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]
Loading weights:  29%|██▉       | 91/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.8.input_layernorm.weight] 
Loading weights:  29%|██▉       | 91/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.8.input_layernorm.weight]
Loading weights:  30%|██▉       | 92/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  
Loading weights:  30%|██▉       | 92/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.8.mlp.down_proj.weight]
Loading weights:  30%|██▉       | 93/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]
Loading weights:  30%|██▉       | 93/311 [00:01<00:01, 121.19it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]
Loading weights:  30%|███       | 94/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]
Loading weights:  30%|███       | 94/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  
Loading weights:  30%|███       | 94/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.mlp.up_proj.weight]
Loading weights:  31%|███       | 95/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]
Loading weights:  31%|███       | 95/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]
Loading weights:  31%|███       | 96/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.k_norm.weight]        
Loading weights:  31%|███       | 96/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.k_norm.weight]
Loading weights:  31%|███       | 97/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]
Loading weights:  31%|███       | 97/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]
Loading weights:  32%|███▏      | 98/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]
Loading weights:  32%|███▏      | 98/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]
Loading weights:  32%|███▏      | 99/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.q_norm.weight]
Loading weights:  32%|███▏      | 99/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.q_norm.weight]
Loading weights:  32%|███▏      | 100/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]
Loading weights:  32%|███▏      | 100/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]
Loading weights:  32%|███▏      | 101/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]
Loading weights:  32%|███▏      | 101/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]
Loading weights:  33%|███▎      | 102/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.input_layernorm.weight] 
Loading weights:  33%|███▎      | 102/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.input_layernorm.weight]
Loading weights:  33%|███▎      | 103/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  
Loading weights:  33%|███▎      | 103/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.mlp.down_proj.weight]
Loading weights:  33%|███▎      | 104/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]
Loading weights:  33%|███▎      | 104/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]
Loading weights:  34%|███▍      | 105/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  
Loading weights:  34%|███▍      | 105/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.mlp.up_proj.weight]
Loading weights:  34%|███▍      | 106/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]
Loading weights:  34%|███▍      | 106/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]
Loading weights:  34%|███▍      | 107/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.k_norm.weight]        
Loading weights:  34%|███▍      | 107/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.k_norm.weight]
Loading weights:  35%|███▍      | 108/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]
Loading weights:  35%|███▍      | 108/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]
Loading weights:  35%|███▌      | 109/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]
Loading weights:  35%|███▌      | 109/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]
Loading weights:  35%|███▌      | 110/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.q_norm.weight]
Loading weights:  35%|███▌      | 110/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.q_norm.weight]
Loading weights:  36%|███▌      | 111/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]
Loading weights:  36%|███▌      | 111/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]
Loading weights:  36%|███▌      | 112/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]
Loading weights:  36%|███▌      | 112/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]
Loading weights:  36%|███▋      | 113/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.10.input_layernorm.weight]
Loading weights:  36%|███▋      | 113/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.10.input_layernorm.weight]
Loading weights:  37%|███▋      | 114/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  
Loading weights:  37%|███▋      | 114/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.10.mlp.down_proj.weight]
Loading weights:  37%|███▋      | 115/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]
Loading weights:  37%|███▋      | 115/311 [00:01<00:01, 142.25it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]
Loading weights:  37%|███▋      | 116/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]
Loading weights:  37%|███▋      | 116/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  
Loading weights:  37%|███▋      | 116/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.mlp.up_proj.weight]
Loading weights:  38%|███▊      | 117/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]
Loading weights:  38%|███▊      | 117/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]
Loading weights:  38%|███▊      | 118/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.k_norm.weight]        
Loading weights:  38%|███▊      | 118/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.k_norm.weight]
Loading weights:  38%|███▊      | 119/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]
Loading weights:  38%|███▊      | 119/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]
Loading weights:  39%|███▊      | 120/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]
Loading weights:  39%|███▊      | 120/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]
Loading weights:  39%|███▉      | 121/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.q_norm.weight]
Loading weights:  39%|███▉      | 121/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.q_norm.weight]
Loading weights:  39%|███▉      | 122/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]
Loading weights:  39%|███▉      | 122/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]
Loading weights:  40%|███▉      | 123/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]
Loading weights:  40%|███▉      | 123/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]
Loading weights:  40%|███▉      | 124/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.input_layernorm.weight] 
Loading weights:  40%|███▉      | 124/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.input_layernorm.weight]
Loading weights:  40%|████      | 125/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  
Loading weights:  40%|████      | 125/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.mlp.down_proj.weight]
Loading weights:  41%|████      | 126/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]
Loading weights:  41%|████      | 126/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]
Loading weights:  41%|████      | 127/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  
Loading weights:  41%|████      | 127/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.mlp.up_proj.weight]
Loading weights:  41%|████      | 128/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]
Loading weights:  41%|████      | 128/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]
Loading weights:  41%|████▏     | 129/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.k_norm.weight]        
Loading weights:  41%|████▏     | 129/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.k_norm.weight]
Loading weights:  42%|████▏     | 130/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]
Loading weights:  42%|████▏     | 130/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]
Loading weights:  42%|████▏     | 131/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]
Loading weights:  42%|████▏     | 131/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]
Loading weights:  42%|████▏     | 132/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.q_norm.weight]
Loading weights:  42%|████▏     | 132/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.q_norm.weight]
Loading weights:  43%|████▎     | 133/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]
Loading weights:  43%|████▎     | 133/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]
Loading weights:  43%|████▎     | 134/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]
Loading weights:  43%|████▎     | 134/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]
Loading weights:  43%|████▎     | 135/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.12.input_layernorm.weight] 
Loading weights:  43%|████▎     | 135/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.12.input_layernorm.weight]
Loading weights:  44%|████▎     | 136/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  
Loading weights:  44%|████▎     | 136/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.12.mlp.down_proj.weight]
Loading weights:  44%|████▍     | 137/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]
Loading weights:  44%|████▍     | 137/311 [00:01<00:01, 158.02it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]
Loading weights:  44%|████▍     | 138/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]
Loading weights:  44%|████▍     | 138/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  
Loading weights:  44%|████▍     | 138/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.mlp.up_proj.weight]
Loading weights:  45%|████▍     | 139/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]
Loading weights:  45%|████▍     | 139/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]
Loading weights:  45%|████▌     | 140/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.k_norm.weight]        
Loading weights:  45%|████▌     | 140/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.k_norm.weight]
Loading weights:  45%|████▌     | 141/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]
Loading weights:  45%|████▌     | 141/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]
Loading weights:  46%|████▌     | 142/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]
Loading weights:  46%|████▌     | 142/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]
Loading weights:  46%|████▌     | 143/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.q_norm.weight]
Loading weights:  46%|████▌     | 143/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.q_norm.weight]
Loading weights:  46%|████▋     | 144/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]
Loading weights:  46%|████▋     | 144/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]
Loading weights:  47%|████▋     | 145/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]
Loading weights:  47%|████▋     | 145/311 [00:01<00:01, 165.22it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]
Loading weights:  47%|████▋     | 146/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.input_layernorm.weight] 
Loading weights:  47%|████▋     | 146/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.input_layernorm.weight]
Loading weights:  47%|████▋     | 147/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  
Loading weights:  47%|████▋     | 147/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.mlp.down_proj.weight]
Loading weights:  48%|████▊     | 148/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]
Loading weights:  48%|████▊     | 148/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]
Loading weights:  48%|████▊     | 149/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  
Loading weights:  48%|████▊     | 149/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.mlp.up_proj.weight]
Loading weights:  48%|████▊     | 150/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]
Loading weights:  48%|████▊     | 150/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]
Loading weights:  49%|████▊     | 151/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.k_norm.weight]        
Loading weights:  49%|████▊     | 151/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.k_norm.weight]
Loading weights:  49%|████▉     | 152/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]
Loading weights:  49%|████▉     | 152/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]
Loading weights:  49%|████▉     | 153/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]
Loading weights:  49%|████▉     | 153/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]
Loading weights:  50%|████▉     | 154/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.q_norm.weight]
Loading weights:  50%|████▉     | 154/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.q_norm.weight]
Loading weights:  50%|████▉     | 155/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]
Loading weights:  50%|████▉     | 155/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]
Loading weights:  50%|█████     | 156/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]
Loading weights:  50%|█████     | 156/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]
Loading weights:  50%|█████     | 157/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.14.input_layernorm.weight] 
Loading weights:  50%|█████     | 157/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.14.input_layernorm.weight]
Loading weights:  51%|█████     | 158/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  
Loading weights:  51%|█████     | 158/311 [00:01<00:00, 165.22it/s, Materializing param=model.layers.14.mlp.down_proj.weight]
Loading weights:  51%|█████     | 159/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.mlp.down_proj.weight]
Loading weights:  51%|█████     | 159/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]
Loading weights:  51%|█████     | 159/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]
Loading weights:  51%|█████▏    | 160/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  
Loading weights:  51%|█████▏    | 160/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.mlp.up_proj.weight]
Loading weights:  52%|█████▏    | 161/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]
Loading weights:  52%|█████▏    | 161/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]
Loading weights:  52%|█████▏    | 162/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.k_norm.weight]        
Loading weights:  52%|█████▏    | 162/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.k_norm.weight]
Loading weights:  52%|█████▏    | 163/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]
Loading weights:  52%|█████▏    | 163/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]
Loading weights:  53%|█████▎    | 164/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]
Loading weights:  53%|█████▎    | 164/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]
Loading weights:  53%|█████▎    | 165/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.q_norm.weight]
Loading weights:  53%|█████▎    | 165/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.q_norm.weight]
Loading weights:  53%|█████▎    | 166/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]
Loading weights:  53%|█████▎    | 166/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]
Loading weights:  54%|█████▎    | 167/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]
Loading weights:  54%|█████▎    | 167/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]
Loading weights:  54%|█████▍    | 168/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.input_layernorm.weight] 
Loading weights:  54%|█████▍    | 168/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.input_layernorm.weight]
Loading weights:  54%|█████▍    | 169/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  
Loading weights:  54%|█████▍    | 169/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.mlp.down_proj.weight]
Loading weights:  55%|█████▍    | 170/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]
Loading weights:  55%|█████▍    | 170/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]
Loading weights:  55%|█████▍    | 171/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  
Loading weights:  55%|█████▍    | 171/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.mlp.up_proj.weight]
Loading weights:  55%|█████▌    | 172/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]
Loading weights:  55%|█████▌    | 172/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]
Loading weights:  56%|█████▌    | 173/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.k_norm.weight]        
Loading weights:  56%|█████▌    | 173/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.k_norm.weight]
Loading weights:  56%|█████▌    | 174/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]
Loading weights:  56%|█████▌    | 174/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]
Loading weights:  56%|█████▋    | 175/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]
Loading weights:  56%|█████▋    | 175/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]
Loading weights:  57%|█████▋    | 176/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.q_norm.weight]
Loading weights:  57%|█████▋    | 176/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.q_norm.weight]
Loading weights:  57%|█████▋    | 177/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]
Loading weights:  57%|█████▋    | 177/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]
Loading weights:  57%|█████▋    | 178/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]
Loading weights:  57%|█████▋    | 178/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]
Loading weights:  58%|█████▊    | 179/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.16.input_layernorm.weight] 
Loading weights:  58%|█████▊    | 179/311 [00:01<00:00, 174.59it/s, Materializing param=model.layers.16.input_layernorm.weight]
Loading weights:  58%|█████▊    | 180/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.input_layernorm.weight]
Loading weights:  58%|█████▊    | 180/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  
Loading weights:  58%|█████▊    | 180/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.mlp.down_proj.weight]
Loading weights:  58%|█████▊    | 181/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]
Loading weights:  58%|█████▊    | 181/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]
Loading weights:  59%|█████▊    | 182/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  
Loading weights:  59%|█████▊    | 182/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.mlp.up_proj.weight]
Loading weights:  59%|█████▉    | 183/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]
Loading weights:  59%|█████▉    | 183/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]
Loading weights:  59%|█████▉    | 184/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.k_norm.weight]        
Loading weights:  59%|█████▉    | 184/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.k_norm.weight]
Loading weights:  59%|█████▉    | 185/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]
Loading weights:  59%|█████▉    | 185/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]
Loading weights:  60%|█████▉    | 186/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]
Loading weights:  60%|█████▉    | 186/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]
Loading weights:  60%|██████    | 187/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.q_norm.weight]
Loading weights:  60%|██████    | 187/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.q_norm.weight]
Loading weights:  60%|██████    | 188/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]
Loading weights:  60%|██████    | 188/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]
Loading weights:  61%|██████    | 189/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]
Loading weights:  61%|██████    | 189/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]
Loading weights:  61%|██████    | 190/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.input_layernorm.weight] 
Loading weights:  61%|██████    | 190/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.input_layernorm.weight]
Loading weights:  61%|██████▏   | 191/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  
Loading weights:  61%|██████▏   | 191/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.mlp.down_proj.weight]
Loading weights:  62%|██████▏   | 192/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]
Loading weights:  62%|██████▏   | 192/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]
Loading weights:  62%|██████▏   | 193/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  
Loading weights:  62%|██████▏   | 193/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.mlp.up_proj.weight]
Loading weights:  62%|██████▏   | 194/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]
Loading weights:  62%|██████▏   | 194/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]
Loading weights:  63%|██████▎   | 195/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.k_norm.weight]        
Loading weights:  63%|██████▎   | 195/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.k_norm.weight]
Loading weights:  63%|██████▎   | 196/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]
Loading weights:  63%|██████▎   | 196/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]
Loading weights:  63%|██████▎   | 197/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]
Loading weights:  63%|██████▎   | 197/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]
Loading weights:  64%|██████▎   | 198/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.q_norm.weight]
Loading weights:  64%|██████▎   | 198/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.q_norm.weight]
Loading weights:  64%|██████▍   | 199/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]
Loading weights:  64%|██████▍   | 199/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]
Loading weights:  64%|██████▍   | 200/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]
Loading weights:  64%|██████▍   | 200/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]
Loading weights:  65%|██████▍   | 201/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.18.input_layernorm.weight] 
Loading weights:  65%|██████▍   | 201/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.18.input_layernorm.weight]
Loading weights:  65%|██████▍   | 202/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  
Loading weights:  65%|██████▍   | 202/311 [00:01<00:00, 174.98it/s, Materializing param=model.layers.18.mlp.down_proj.weight]
Loading weights:  65%|██████▌   | 203/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.mlp.down_proj.weight]
Loading weights:  65%|██████▌   | 203/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]
Loading weights:  65%|██████▌   | 203/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]
Loading weights:  66%|██████▌   | 204/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  
Loading weights:  66%|██████▌   | 204/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.mlp.up_proj.weight]
Loading weights:  66%|██████▌   | 205/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]
Loading weights:  66%|██████▌   | 205/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]
Loading weights:  66%|██████▌   | 206/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.k_norm.weight]        
Loading weights:  66%|██████▌   | 206/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.k_norm.weight]
Loading weights:  67%|██████▋   | 207/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]
Loading weights:  67%|██████▋   | 207/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]
Loading weights:  67%|██████▋   | 208/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]
Loading weights:  67%|██████▋   | 208/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]
Loading weights:  67%|██████▋   | 209/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.q_norm.weight]
Loading weights:  67%|██████▋   | 209/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.q_norm.weight]
Loading weights:  68%|██████▊   | 210/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]
Loading weights:  68%|██████▊   | 210/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]
Loading weights:  68%|██████▊   | 211/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]
Loading weights:  68%|██████▊   | 211/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]
Loading weights:  68%|██████▊   | 212/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.input_layernorm.weight] 
Loading weights:  68%|██████▊   | 212/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.input_layernorm.weight]
Loading weights:  68%|██████▊   | 213/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  
Loading weights:  68%|██████▊   | 213/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.mlp.down_proj.weight]
Loading weights:  69%|██████▉   | 214/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]
Loading weights:  69%|██████▉   | 214/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]
Loading weights:  69%|██████▉   | 215/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  
Loading weights:  69%|██████▉   | 215/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.mlp.up_proj.weight]
Loading weights:  69%|██████▉   | 216/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]
Loading weights:  69%|██████▉   | 216/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]
Loading weights:  70%|██████▉   | 217/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.k_norm.weight]        
Loading weights:  70%|██████▉   | 217/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.k_norm.weight]
Loading weights:  70%|███████   | 218/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]
Loading weights:  70%|███████   | 218/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]
Loading weights:  70%|███████   | 219/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]
Loading weights:  70%|███████   | 219/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]
Loading weights:  71%|███████   | 220/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.q_norm.weight]
Loading weights:  71%|███████   | 220/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.q_norm.weight]
Loading weights:  71%|███████   | 221/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]
Loading weights:  71%|███████   | 221/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]
Loading weights:  71%|███████▏  | 222/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]
Loading weights:  71%|███████▏  | 222/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]
Loading weights:  72%|███████▏  | 223/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.input_layernorm.weight] 
Loading weights:  72%|███████▏  | 223/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.input_layernorm.weight]
Loading weights:  72%|███████▏  | 224/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  
Loading weights:  72%|███████▏  | 224/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.mlp.down_proj.weight]
Loading weights:  72%|███████▏  | 225/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]
Loading weights:  72%|███████▏  | 225/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]
Loading weights:  73%|███████▎  | 226/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  
Loading weights:  73%|███████▎  | 226/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.mlp.up_proj.weight]
Loading weights:  73%|███████▎  | 227/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]
Loading weights:  73%|███████▎  | 227/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]
Loading weights:  73%|███████▎  | 228/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.k_norm.weight]        
Loading weights:  73%|███████▎  | 228/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.k_norm.weight]
Loading weights:  74%|███████▎  | 229/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]
Loading weights:  74%|███████▎  | 229/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]
Loading weights:  74%|███████▍  | 230/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]
Loading weights:  74%|███████▍  | 230/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]
Loading weights:  74%|███████▍  | 231/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.q_norm.weight]
Loading weights:  74%|███████▍  | 231/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.q_norm.weight]
Loading weights:  75%|███████▍  | 232/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]
Loading weights:  75%|███████▍  | 232/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]
Loading weights:  75%|███████▍  | 233/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]
Loading weights:  75%|███████▍  | 233/311 [00:01<00:00, 187.63it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]
Loading weights:  75%|███████▌  | 234/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]
Loading weights:  75%|███████▌  | 234/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.input_layernorm.weight] 
Loading weights:  75%|███████▌  | 234/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.input_layernorm.weight]
Loading weights:  76%|███████▌  | 235/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  
Loading weights:  76%|███████▌  | 235/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.mlp.down_proj.weight]
Loading weights:  76%|███████▌  | 236/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]
Loading weights:  76%|███████▌  | 236/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]
Loading weights:  76%|███████▌  | 237/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  
Loading weights:  76%|███████▌  | 237/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.mlp.up_proj.weight]
Loading weights:  77%|███████▋  | 238/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]
Loading weights:  77%|███████▋  | 238/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]
Loading weights:  77%|███████▋  | 239/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.k_norm.weight]        
Loading weights:  77%|███████▋  | 239/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.k_norm.weight]
Loading weights:  77%|███████▋  | 240/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]
Loading weights:  77%|███████▋  | 240/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]
Loading weights:  77%|███████▋  | 241/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]
Loading weights:  77%|███████▋  | 241/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]
Loading weights:  78%|███████▊  | 242/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.q_norm.weight]
Loading weights:  78%|███████▊  | 242/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.q_norm.weight]
Loading weights:  78%|███████▊  | 243/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]
Loading weights:  78%|███████▊  | 243/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]
Loading weights:  78%|███████▊  | 244/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]
Loading weights:  78%|███████▊  | 244/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]
Loading weights:  79%|███████▉  | 245/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.input_layernorm.weight] 
Loading weights:  79%|███████▉  | 245/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.input_layernorm.weight]
Loading weights:  79%|███████▉  | 246/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  
Loading weights:  79%|███████▉  | 246/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.mlp.down_proj.weight]
Loading weights:  79%|███████▉  | 247/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]
Loading weights:  79%|███████▉  | 247/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]
Loading weights:  80%|███████▉  | 248/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  
Loading weights:  80%|███████▉  | 248/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.mlp.up_proj.weight]
Loading weights:  80%|████████  | 249/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]
Loading weights:  80%|████████  | 249/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]
Loading weights:  80%|████████  | 250/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.k_norm.weight]        
Loading weights:  80%|████████  | 250/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.k_norm.weight]
Loading weights:  81%|████████  | 251/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]
Loading weights:  81%|████████  | 251/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]
Loading weights:  81%|████████  | 252/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]
Loading weights:  81%|████████  | 252/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]
Loading weights:  81%|████████▏ | 253/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.q_norm.weight]
Loading weights:  81%|████████▏ | 253/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.q_norm.weight]
Loading weights:  82%|████████▏ | 254/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]
Loading weights:  82%|████████▏ | 254/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]
Loading weights:  82%|████████▏ | 255/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]
Loading weights:  82%|████████▏ | 255/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]
Loading weights:  82%|████████▏ | 256/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.input_layernorm.weight] 
Loading weights:  82%|████████▏ | 256/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.input_layernorm.weight]
Loading weights:  83%|████████▎ | 257/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  
Loading weights:  83%|████████▎ | 257/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.mlp.down_proj.weight]
Loading weights:  83%|████████▎ | 258/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]
Loading weights:  83%|████████▎ | 258/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]
Loading weights:  83%|████████▎ | 259/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  
Loading weights:  83%|████████▎ | 259/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.mlp.up_proj.weight]
Loading weights:  84%|████████▎ | 260/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]
Loading weights:  84%|████████▎ | 260/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]
Loading weights:  84%|████████▍ | 261/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.k_norm.weight]        
Loading weights:  84%|████████▍ | 261/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.k_norm.weight]
Loading weights:  84%|████████▍ | 262/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]
Loading weights:  84%|████████▍ | 262/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]
Loading weights:  85%|████████▍ | 263/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]
Loading weights:  85%|████████▍ | 263/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]
Loading weights:  85%|████████▍ | 264/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.q_norm.weight]
Loading weights:  85%|████████▍ | 264/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.q_norm.weight]
Loading weights:  85%|████████▌ | 265/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]
Loading weights:  85%|████████▌ | 265/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]
Loading weights:  86%|████████▌ | 266/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]
Loading weights:  86%|████████▌ | 266/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]
Loading weights:  86%|████████▌ | 267/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.24.input_layernorm.weight] 
Loading weights:  86%|████████▌ | 267/311 [00:01<00:00, 220.11it/s, Materializing param=model.layers.24.input_layernorm.weight]
Loading weights:  86%|████████▌ | 268/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.input_layernorm.weight]
Loading weights:  86%|████████▌ | 268/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.mlp.down_proj.weight]  
Loading weights:  86%|████████▌ | 268/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.mlp.down_proj.weight]
Loading weights:  86%|████████▋ | 269/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]
Loading weights:  86%|████████▋ | 269/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]
Loading weights:  87%|████████▋ | 270/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.mlp.up_proj.weight]  
Loading weights:  87%|████████▋ | 270/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.mlp.up_proj.weight]
Loading weights:  87%|████████▋ | 271/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]
Loading weights:  87%|████████▋ | 271/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]
Loading weights:  87%|████████▋ | 272/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.k_norm.weight]        
Loading weights:  87%|████████▋ | 272/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.k_norm.weight]
Loading weights:  88%|████████▊ | 273/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]
Loading weights:  88%|████████▊ | 273/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]
Loading weights:  88%|████████▊ | 274/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]
Loading weights:  88%|████████▊ | 274/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]
Loading weights:  88%|████████▊ | 275/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.q_norm.weight]
Loading weights:  88%|████████▊ | 275/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.q_norm.weight]
Loading weights:  89%|████████▊ | 276/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]
Loading weights:  89%|████████▊ | 276/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]
Loading weights:  89%|████████▉ | 277/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]
Loading weights:  89%|████████▉ | 277/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]
Loading weights:  89%|████████▉ | 278/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.input_layernorm.weight] 
Loading weights:  89%|████████▉ | 278/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.input_layernorm.weight]
Loading weights:  90%|████████▉ | 279/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.mlp.down_proj.weight]  
Loading weights:  90%|████████▉ | 279/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.mlp.down_proj.weight]
Loading weights:  90%|█████████ | 280/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]
Loading weights:  90%|█████████ | 280/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]
Loading weights:  90%|█████████ | 281/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.mlp.up_proj.weight]  
Loading weights:  90%|█████████ | 281/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.mlp.up_proj.weight]
Loading weights:  91%|█████████ | 282/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]
Loading weights:  91%|█████████ | 282/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]
Loading weights:  91%|█████████ | 283/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.k_norm.weight]        
Loading weights:  91%|█████████ | 283/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.k_norm.weight]
Loading weights:  91%|█████████▏| 284/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]
Loading weights:  91%|█████████▏| 284/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]
Loading weights:  92%|█████████▏| 285/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]
Loading weights:  92%|█████████▏| 285/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]
Loading weights:  92%|█████████▏| 286/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.q_norm.weight]
Loading weights:  92%|█████████▏| 286/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.q_norm.weight]
Loading weights:  92%|█████████▏| 287/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]
Loading weights:  92%|█████████▏| 287/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]
Loading weights:  93%|█████████▎| 288/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]
Loading weights:  93%|█████████▎| 288/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]
Loading weights:  93%|█████████▎| 289/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.input_layernorm.weight] 
Loading weights:  93%|█████████▎| 289/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.input_layernorm.weight]
Loading weights:  93%|█████████▎| 290/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.mlp.down_proj.weight]  
Loading weights:  93%|█████████▎| 290/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.mlp.down_proj.weight]
Loading weights:  94%|█████████▎| 291/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]
Loading weights:  94%|█████████▎| 291/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]
Loading weights:  94%|█████████▍| 292/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.mlp.up_proj.weight]  
Loading weights:  94%|█████████▍| 292/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.mlp.up_proj.weight]
Loading weights:  94%|█████████▍| 293/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]
Loading weights:  94%|█████████▍| 293/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]
Loading weights:  95%|█████████▍| 294/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.k_norm.weight]        
Loading weights:  95%|█████████▍| 294/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.k_norm.weight]
Loading weights:  95%|█████████▍| 295/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]
Loading weights:  95%|█████████▍| 295/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]
Loading weights:  95%|█████████▌| 296/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]
Loading weights:  95%|█████████▌| 296/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]
Loading weights:  95%|█████████▌| 297/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.q_norm.weight]
Loading weights:  95%|█████████▌| 297/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.q_norm.weight]
Loading weights:  96%|█████████▌| 298/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]
Loading weights:  96%|█████████▌| 298/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]
Loading weights:  96%|█████████▌| 299/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]
Loading weights:  96%|█████████▌| 299/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]
Loading weights:  96%|█████████▋| 300/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.27.input_layernorm.weight] 
Loading weights:  96%|█████████▋| 300/311 [00:01<00:00, 250.48it/s, Materializing param=model.layers.27.input_layernorm.weight]
Loading weights:  97%|█████████▋| 301/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.input_layernorm.weight]
Loading weights:  97%|█████████▋| 301/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.mlp.down_proj.weight]  
Loading weights:  97%|█████████▋| 301/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.mlp.down_proj.weight]
Loading weights:  97%|█████████▋| 302/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]
Loading weights:  97%|█████████▋| 302/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]
Loading weights:  97%|█████████▋| 303/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.mlp.up_proj.weight]  
Loading weights:  97%|█████████▋| 303/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.mlp.up_proj.weight]
Loading weights:  98%|█████████▊| 304/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]
Loading weights:  98%|█████████▊| 304/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]
Loading weights:  98%|█████████▊| 305/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.k_norm.weight]        
Loading weights:  98%|█████████▊| 305/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.k_norm.weight]
Loading weights:  98%|█████████▊| 306/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]
Loading weights:  98%|█████████▊| 306/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]
Loading weights:  99%|█████████▊| 307/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]
Loading weights:  99%|█████████▊| 307/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]
Loading weights:  99%|█████████▉| 308/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.q_norm.weight]
Loading weights:  99%|█████████▉| 308/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.q_norm.weight]
Loading weights:  99%|█████████▉| 309/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]
Loading weights:  99%|█████████▉| 309/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]
Loading weights: 100%|█████████▉| 310/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]
Loading weights: 100%|█████████▉| 310/311 [00:01<00:00, 271.42it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]
Loading weights: 100%|██████████| 311/311 [00:01<00:00, 271.42it/s, Materializing param=model.norm.weight]                      
Loading weights: 100%|██████████| 311/311 [00:01<00:00, 271.42it/s, Materializing param=model.norm.weight]
Loading weights: 100%|██████████| 311/311 [00:01<00:00, 157.48it/s, Materializing param=model.norm.weight]
[WARNING|modeling_utils.py:2517] 2026-02-15 17:05:35,616 >> The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning
[INFO|configuration_utils.py:967] 2026-02-15 17:05:35,949 >> loading configuration file generation_config.json from cache at /Users/yunxuanhan/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/generation_config.json
[INFO|configuration_utils.py:1014] 2026-02-15 17:05:35,949 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

[WARNING|trainer.py:922] 2026-02-15 17:05:36,204 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:2778] 2026-02-15 17:05:36,205 >> Loading model from qwen3_lora/checkpoint-222.
[INFO|trainer.py:2383] 2026-02-15 17:05:36,726 >> ***** Running training *****
[INFO|trainer.py:2384] 2026-02-15 17:05:36,726 >>   Num examples = 591
[INFO|trainer.py:2385] 2026-02-15 17:05:36,726 >>   Num Epochs = 3
[INFO|trainer.py:2386] 2026-02-15 17:05:36,726 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2389] 2026-02-15 17:05:36,726 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:2390] 2026-02-15 17:05:36,726 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2391] 2026-02-15 17:05:36,726 >>   Total optimization steps = 222
[INFO|trainer.py:2392] 2026-02-15 17:05:36,728 >>   Number of trainable parameters = 5,046,272
[INFO|trainer.py:2414] 2026-02-15 17:05:36,728 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:2415] 2026-02-15 17:05:36,728 >>   Continuing training from epoch 3
[INFO|trainer.py:2416] 2026-02-15 17:05:36,728 >>   Continuing training from global step 222
[INFO|trainer.py:2418] 2026-02-15 17:05:36,728 >>   Will skip the first 3 epochs then the first 0 batches in the first epoch.

  0%|          | 0/222 [00:00<?, ?it/s][INFO|trainer.py:2657] 2026-02-15 17:05:36,730 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)



                                       

  0%|          | 0/222 [00:00<?, ?it/s]
  0%|          | 0/222 [00:00<?, ?it/s]
[INFO|trainer.py:4115] 2026-02-15 17:05:36,732 >> Saving model checkpoint to qwen3_lora
[INFO|configuration_utils.py:667] 2026-02-15 17:05:37,505 >> loading configuration file config.json from cache at /Users/yunxuanhan/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/config.json
[INFO|configuration_utils.py:739] 2026-02-15 17:05:37,506 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 28,
  "model_type": "qwen3",
  "num_attention_heads": 16,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pad_token_id": null,
  "rms_norm_eps": 1e-06,
  "rope_parameters": {
    "rope_theta": 1000000,
    "rope_type": "default"
  },
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "5.0.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:3327] 2026-02-15 17:05:37,690 >> chat template saved in qwen3_lora/chat_template.jinja
[INFO|tokenization_utils_base.py:2181] 2026-02-15 17:05:37,690 >> tokenizer config file saved in qwen3_lora/tokenizer_config.json
[INFO|modelcard.py:266] 2026-02-15 17:05:37,790 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

[INFO|2026-02-15 17:05:21] llamafactory.hparams.parser:144 >> Resuming training from qwen3_lora/checkpoint-222.
[INFO|2026-02-15 17:05:21] llamafactory.hparams.parser:144 >> Change `output_dir` or use `overwrite_output_dir` to avoid.
[INFO|2026-02-15 17:05:21] llamafactory.hparams.parser:459 >> Process rank: 0, world size: 1, device: mps, distributed training: False, compute dtype: torch.bfloat16
[INFO|2026-02-15 17:05:29] llamafactory.data.loader:144 >> Loading dataset identity.json...
[INFO|2026-02-15 17:05:30] llamafactory.data.loader:144 >> Loading dataset alpaca_en_demo.json...
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 6023, 151645, 198, 151644, 77091, 198, 9707, 0, 358, 1079, 444, 80001, 12, 18, 11, 458, 15235, 17847, 7881, 553, 444, 8747, 4835, 16937, 13, 2585, 646, 358, 7789, 498, 3351, 30, 151645, 198]
inputs:
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
hi<|im_end|>
<|im_start|>assistant
Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9707, 0, 358, 1079, 444, 80001, 12, 18, 11, 458, 15235, 17847, 7881, 553, 444, 8747, 4835, 16937, 13, 2585, 646, 358, 7789, 498, 3351, 30, 151645, 198]
labels:
Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|im_end|>

[INFO|2026-02-15 17:05:32] llamafactory.model.model_utils.kv_cache:144 >> KV cache is disabled during training.
[INFO|2026-02-15 17:05:35] llamafactory.model.model_utils.checkpointing:144 >> Gradient checkpointing enabled.
[INFO|2026-02-15 17:05:35] llamafactory.model.model_utils.attention:144 >> Using torch SDPA for faster training and inference.
[INFO|2026-02-15 17:05:35] llamafactory.model.adapter:144 >> Upcasting trainable params to float32.
[INFO|2026-02-15 17:05:35] llamafactory.model.adapter:144 >> Fine-tuning method: LoRA
[INFO|2026-02-15 17:05:35] llamafactory.model.model_utils.misc:144 >> Found linear modules: down_proj,q_proj,gate_proj,up_proj,v_proj,k_proj,o_proj
[INFO|2026-02-15 17:05:36] llamafactory.model.loader:144 >> trainable params: 5,046,272 || all params: 756,678,656 || trainable%: 0.6669
[INFO|2026-02-15 17:05:36] llamafactory.train.trainer_utils:144 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'train_runtime': '0.0034', 'train_samples_per_second': '5.222e+05', 'train_steps_per_second': '6.539e+04', 'train_loss': '0', 'epoch': '3'}
***** train metrics *****
  epoch                    =        3.0
  total_flos               =  1028464GF
  train_loss               =        0.0
  train_runtime            = 0:00:00.00
  train_samples_per_second = 522226.193
  train_steps_per_second   =  65388.728

