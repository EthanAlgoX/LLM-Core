#!/usr/bin/env python3
"""
Agent æ ¸å¿ƒåŸç†ï¼šReAct (Reason + Act) å¾ªç¯æ¼”ç¤ºã€‚
è¯¥è„šæœ¬æ¨¡æ‹Ÿäº† Agent å¦‚ä½•é€šè¿‡â€œæ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿâ€çš„é—­ç¯å®Œæˆä»»åŠ¡ã€‚
"""

import json
import time

def mock_search_tool(query: str) -> str:
    """æ¨¡æ‹Ÿæœç´¢å·¥å…·"""
    knowledge = {
        "åŒ—äº¬å¤©æ°”": "åŒ—äº¬ä»Šå¤©å¤šäº‘è½¬æ™´ï¼Œ25æ‘„æ°åº¦ã€‚",
        "LLM-Coreé¡¹ç›®": "è¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºå¤§æ¨¡å‹æ ¸å¿ƒåŸç†å­¦ä¹ çš„å¼€æºé¡¹ç›®ã€‚",
    }
    return knowledge.get(query, "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")

def mock_calculator_tool(expression: str) -> str:
    """æ¨¡æ‹Ÿè®¡ç®—å™¨å·¥å…·"""
    try:
        return str(eval(expression))
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {e}"

TOOLS = {
    "search": mock_search_tool,
    "calculator": mock_calculator_tool
}

def mock_llm_api(prompt: str) -> str:
    """æ¨¡æ‹Ÿ LLM çš„ ReAct æ­¥è¿›å“åº”"""
    # åœºæ™¯1ï¼šæŸ¥è¯¢å¤©æ°”
    if "æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”" in prompt and "Observation" not in prompt:
        return "Thought: ç”¨æˆ·æƒ³çŸ¥é“åŒ—äº¬çš„å¤©æ°”ï¼Œæˆ‘éœ€è¦ä½¿ç”¨æœç´¢å·¥å…·ã€‚\nAction: search(\"åŒ—äº¬å¤©æ°”\")"
    
    # åœºæ™¯2ï¼šå¾—åˆ°å¤©æ°”åé¦ˆåç»“æŸ
    if "Observation: åŒ—äº¬ä»Šå¤©å¤šäº‘è½¬æ™´ï¼Œ25æ‘„æ°åº¦ã€‚" in prompt:
        return "Thought: æˆ‘å·²ç»å¾—åˆ°äº†åŒ—äº¬çš„å¤©æ°”ä¿¡æ¯ã€‚\nFinal Answer: åŒ—äº¬ä»Šå¤©å¤šäº‘è½¬æ™´ï¼Œæ°”æ¸© 25 æ‘„æ°åº¦ã€‚"
    
    return "Thought: æˆ‘éœ€è¦æ›´å¤šä¿¡æ¯ã€‚\nFinal Answer: æ— æ³•å¤„ç†è¯¥è¯·æ±‚ã€‚"

def run_agent_loop(query: str, max_steps: int = 3):
    print(f"ğŸš€ ç”¨æˆ·è¯·æ±‚: {query}\n" + "="*40)
    
    context = f"Question: {query}\n"
    
    for step in range(max_steps):
        print(f"\n[Step {step + 1}]")
        
        # 1. LLM æ€è€ƒå¹¶è¾“å‡º Action
        llm_response = mock_llm_api(context)
        print(f"ğŸ¤– LLM å“åº”:\n{llm_response}")
        
        if "Final Answer:" in llm_response:
            print(f"\nâœ… æœ€ç»ˆç­”æ¡ˆ: {llm_response.split('Final Answer:')[1].strip()}")
            break
            
        # 2. è§£æ Action å¹¶è°ƒç”¨å·¥å…·
        if "Action:" in llm_response:
            action_line = llm_response.split("Action:")[1].strip()
            tool_name = action_line.split("(")[0]
            tool_input = action_line.split("(")[1].strip(")\"")
            
            print(f"ğŸ› ï¸ æ‰§è¡Œå·¥å…·: {tool_name}({tool_input})")
            observation = TOOLS[tool_name](tool_input)
            print(f"ğŸ‘ï¸ è§‚å¯Ÿç»“æœ: {observation}")
            
            # 3. å°†è§‚å¯Ÿç»“æœå–‚å›ä¸Šä¸‹æ–‡
            context += f"{llm_response}\nObservation: {observation}\n"
        
        time.sleep(0.5)

if __name__ == "__main__":
    run_agent_loop("è¯·å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”")
