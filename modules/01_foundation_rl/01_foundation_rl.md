# RL Basics 分类

> [!TIP]
> **一句话通俗理解**：从 MDP 到 GAE 的最小闭环，把“如何定义问题、如何估值、如何降方差”串成一条可复现学习路径。

## 定义与目标

- **定义**：RL 基础模块讲的是“智能体如何通过与环境交互学习策略”的最小知识闭环。
- **目标**：先建立“状态-动作-回报”的统一心智模型，再理解从价值估计到优势估计的演进。

## 适用场景与边界

- **适用场景**：用于强化学习入门、算法推导和 toy 环境复现。
- **不适用场景**：不适用于直接替代生产级策略系统（需结合业务约束与大规模评测）。
- **使用边界**：结论依赖环境定义、奖励设计与随机种子设置。

## 关键步骤

1. 从 `MDP` 建立问题建模框架（状态、动作、转移、奖励、折扣）。
2. 用 `TD Learning` 学习“边采样边更新”的价值估计方式。
3. 用 `Advantage` 理解“相对平均水平有多好”。
4. 用 `GAE` 在偏差与方差之间做工程可用的折中。

建议学习顺序：`mdp -> td_learning -> advantage -> gae`

## 关键公式

`V_pi(s) = E_{a~pi, s'~P}[r(s,a) + gamma * V_pi(s')]`

符号说明：
- `V_pi(s)`：策略 `pi` 在状态 `s` 下的价值。
- `gamma`：折扣因子，控制长期回报权重。
- `P`：环境转移概率。

## 关键步骤代码（纯文档示例）

```python
V = init_value_table()
for _ in range(num_iters):
    s = env.reset()
    done = False
    while not done:
        a = policy(s, V)
        s_next, r, done = env.step(a)
        V[s] = V[s] + alpha * (r + gamma * V[s_next] - V[s])  # TD 更新
        s = s_next
```

## 子模块导航

- `mdp`: MDP 建模与 Bellman 备份。
- `td_learning`: Q-Learning 与 SARSA。
- `advantage`: 优势函数估计与对比。
- `gae`: 广义优势估计与稳定训练。

## 工程实现要点

- 固定环境版本与随机种子，先确保结果可复现。
- 区分训练集与评估轨迹，避免数据泄漏。
- 同时记录平均回报、方差与收敛速度，不只看单点最优值。

## 常见错误与排查

- **症状**：回报长期不提升。  
  **原因**：学习率过高或探索不足导致策略陷入次优。  
  **解决**：降低学习率并提高探索强度（如 epsilon/entropy）。
- **症状**：不同机器复现结果差异大。  
  **原因**：环境版本、随机种子或预处理流程不一致。  
  **解决**：锁定依赖版本并统一 seed 与评测脚本。

## 与相近方法对比

| 方法 | 优点 | 局限 | 适用场景 |
| --- | --- | --- | --- |
| 本文主题方法 | 紧贴本节问题定义 | 依赖数据与实现质量 | 适合结构化评测与迭代优化 |
| 对比方法A | 上手成本更低 | 能力上限可能受限 | 快速原型与基线对照 |
| 对比方法B | 上限潜力更高 | 调参与资源成本更高 | 高要求生产或复杂任务场景 |

## 参考资料

- [Sutton & Barto《Reinforcement Learning》](http://incompleteideas.net/book/the-book.html)
- [David Silver RL Course](https://www.davidsilver.uk/teaching/)
