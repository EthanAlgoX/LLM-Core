# MDP（马尔可夫决策过程）

## 定位与分类

- **阶段**：强化学习理论基石（RL Foundation）。
- **类型**：序列决策数学模型。
- **作用**：它是所有强化学习问题的共同底座。无论是在线 PPO 还是离线 BCQ，其底层逻辑都可以抽象为一个 MDP 过程。

## 什么是 MDP？

MDP（Markov Decision Process，马尔可夫决策过程）是一个描述**智能体（Agent）**与**环境（Environment）**交互的数学模型。
其核心特性是“马尔可夫性”：**未来仅取决于现在，而与过去无关**。即只要知道了当前状态，我们就拥有了决定下一步动作所需的全部信息。

## 核心组成部分（五元组）

1. **状态空间 $S$ (States)**：智能体可能处在的所有情况（如网格坐标）。
2. **动作空间 $A$ (Actions)**：智能体在每个状态下可以采取的行为（如上下左右）。
3. **转移概率 $P(s'|s, a)$ (Transitions)**：在状态 $s$ 执行动作 $a$ 后，到达 $s'$ 的概率。
4. **奖励函数 $R(s, a, s')$ (Rewards)**：环境给出的反馈得分。
5. **折扣因子 $\gamma$ (Discount)**：对未来奖励的重视程度（0到1之间）。

## 核心数学公式

### 1. 贝尔曼方程 (Bellman Equation)

这是 MDP 的灵魂，它定义了当前价值与未来价值的关系：

$$V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) [R(s, a, s') + \gamma V^\pi(s')]$$

#### 如何理解贝尔曼方程？

贝尔曼方程的核心是**递归自洽性（Recursive Consistency）**。

- **拆解理解**：当前状态的价值 = 立即获得的奖励 + 经过折扣的未来状态价值。
- **直观理解**：如果你想知道在某个位置（状态）有多“好”，只需看下你现在能拿多少分，再加上你到了下一个位置后预期的总分。
- **这种“自我迭代”的循环**，使得我们可以通过不断的“备份（Backup）”操作，从终点向起点、或者在状态空间内反复平滑，最终让每个状态的价值都处于一个稳定的平衡点。

### 2. 最优值函数 (Optimal Value Function)

通过值迭代（Value Iteration）寻找最优分数的迭代式：

$$V^{\ast}(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) [R(s, a, s') + \gamma V^{\ast}(s')]$$

#### 最优值函数的作用是什么？

它是贝尔曼方程的**极限状态**，代表了在该环境中所能达到的“天花板”性能。

- **导航作用**：一旦我们求得了 $V^{\ast}(s)$，就相当于拥有了一张“上帝地图”。在任何时刻，只要选择那个能带你进入 $V^{\ast}$ 更高的下一个状态的动作，你就自动执行了**最优策略 $\pi^{\ast}$ **。
- **理论基石**：它是 Q-learning、PPO 等复杂算法的基础。大部分强化学习算法的本质，就是通过采样来逼近这个 $V^{\ast}$。

## 贝尔曼方程的作用总结 (面试必答)

1. **递归性**：将一长串复杂的奖励求和问题，转化为了“当前”与“下一步”的局部递推问题。
2. **桥梁作用**：它建立了状态、动作、奖励、未来价值之间的数学纽带。
3. **求解算法的依据**：所有的强化学习迭代算法（如值迭代、策略迭代、Q-learning）都是以贝尔曼方程的“自洽误差”作为优化驱动力的。

## 与相近方法区别

1. 相比 `TD Learning`：MDP 示例偏“有模型规划”，TD 偏“无模型学习”。
2. 相比 `Policy Gradient`：MDP 是问题定义，不限定具体求解算法。
3. 相比 `CQL/BCQ`：后者是离线 RL 具体算法，不是基础框架定义。

## 运行

```bash
cd /Users/yunxuanhan/Documents/workspace/ai/Finetune/post_train/rl_basics/mdp
source /opt/anaconda3/etc/profile.d/conda.sh
conda activate finetune
python code/mdp.py
```

## 输出结果

默认输出到 `output/mdp_metrics`，包含：

- `iteration_log.csv`
- `training_curves.png`
- `value_function.json`
- `policy.json`
- `summary.json`

## 目录文件说明（重点）

- `code/`：主流程代码，通常是可直接运行的单文件脚本。
- `data/`：示例数据、训练样本或数据索引配置。
- `models/`：训练完成后导出的最终模型权重（用于推理/部署）。
- `checkpoints/`：训练过程中的阶段性快照（含 step、优化器状态等），用于断点续训与回溯。
- `output/`：可视化图、指标表、训练日志与总结文件（如 `csv/png/json`）。
