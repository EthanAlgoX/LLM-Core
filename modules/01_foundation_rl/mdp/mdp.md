# MDP（马尔可夫决策过程）

> [!TIP]
> **一句话通俗理解**：MDP 五元组建模与 Bellman 备份方程

## 定位与分类

- **阶段**：强化学习理论基石（RL Foundation）。
- **类型**：序列决策数学模型。
- **作用**：它是所有强化学习问题的共同底座。无论是在线 PPO 还是离线 BCQ，其底层逻辑都可以抽象为一个 MDP 过程。

## 定义与目标

MDP（Markov Decision Process，马尔可夫决策过程）是一个描述**智能体（Agent）**与**环境（Environment）**交互的数学模型。
其核心特性是“马尔可夫性”：**未来仅取决于现在，而与过去无关**。即只要知道了当前状态，我们就拥有了决定下一步动作所需的全部信息。

## 核心组成部分（五元组）

1. **状态空间 $S$ (States)**：智能体可能处在的所有情况（如网格坐标）。
2. **动作空间 $A$ (Actions)**：智能体在每个状态下可以采取的行为（如上下左右）。
3. **转移概率 $P(s'|s, a)$ (Transitions)**：在状态 $s$ 执行动作 $a$ 后，到达 $s'$ 的概率。
4. **奖励函数 $R(s, a, s')$ (Rewards)**：环境给出的反馈得分。
5. **折扣因子 $\gamma$ (Discount)**：对未来奖励的重视程度（0到1之间）。

## 关键公式

### 1. 贝尔曼方程 (Bellman Equation)

这是 MDP 的灵魂，它定义了当前价值与未来价值的关系：

$$V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) [R(s, a, s') + \gamma V^\pi(s')]$$

#### 如何理解贝尔曼方程？

贝尔曼方程的核心是**递归自洽性（Recursive Consistency）**。

- **拆解理解**：当前状态的价值 = 立即获得的奖励 + 经过折扣的未来状态价值。
- **直观理解**：如果你想知道在某个位置（状态）有多“好”，只需看下你现在能拿多少分，再加上你到了下一个位置后预期的总分。
- **这种“自我迭代”的循环**，使得我们可以通过不断的“备份（Backup）”操作，从终点向起点、或者在状态空间内反复平滑，最终让每个状态的价值都处于一个稳定的平衡点。

### 2. 最优值函数 (Optimal Value Function)

通过值迭代（Value Iteration）寻找最优分数的迭代式：

$$V^{\ast}(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) [R(s, a, s') + \gamma V^{\ast}(s')]$$

#### 最优值函数的作用是什么？

它是贝尔曼方程的**极限状态**，代表了在该环境中所能达到的“天花板”性能。

- **导航作用**：一旦我们求得了 $V^{\ast}(s)$，就相当于拥有了一张“上帝地图”。在任何时刻，只要选择那个能带你进入 $V^{\ast}$ 更高的下一个状态的动作，你就自动执行了**最优策略 $\pi^{\ast}$**。
- **理论基石**：它是 Q-learning、PPO 等复杂算法的基础。大部分强化学习算法的本质，就是通过采样来逼近这个 $V^{\ast}$。

## 贝尔曼方程的作用总结 (技术解析重点)

1. **递归性**：将一长串复杂的奖励求和问题，转化为了“当前”与“下一步”的局部递推问题。
2. **桥梁作用**：它建立了状态、动作、奖励、未来价值之间的数学纽带。
3. **求解算法的依据**：所有的强化学习迭代算法（如值迭代、策略迭代、Q-learning）都是以贝尔曼方程的“自洽误差”作为优化驱动力的。

## 与相近方法区别

1. 相比 `TD Learning`：MDP 示例偏“有模型规划”，TD 偏“无模型学习”。
2. 相比 `Policy Gradient`：MDP 是问题定义，不限定具体求解算法。
3. 相比 `CQL/BCQ`：后者是离线 RL 具体算法，不是基础框架定义。

## 适用场景与边界

- **适用场景**：用于强化学习入门、算法推导和 toy 环境复现。
- **不适用场景**：不适用于直接替代生产级策略系统（需结合业务约束与大规模评测）。
- **使用边界**：结论依赖环境定义、奖励设计与随机种子设置。

## 关键步骤代码（纯文档示例）

```python
# 关键步骤代码（示意）
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```

## 输出结果

默认输出到 `output/mdp_metrics`，包含：

- `iteration_log.csv`
- `training_curves.png`
- `value_function.json`
- `policy.json`
- `summary.json`

## 目录文件说明（重点）

- 关键步骤代码：见“关键步骤代码（纯文档示例）”章节。
- `data/`：示例数据、训练样本或数据索引配置。
- `models/`：训练完成后导出的最终模型权重（用于推理/部署）。
- `checkpoints/`：训练过程中的阶段性快照（含 step、优化器状态等），用于断点续训与回溯。
- `output/`：可视化图、指标表、训练日志与总结文件（如 `csv/png/json`）。

## 工程实现要点

- 固定环境版本与随机种子，先确保结果可复现。
- 区分训练集与评估轨迹，避免数据泄漏。
- 同时记录平均回报、方差与收敛速度，不只看单点最优值。

## 常见错误与排查

- **症状**：回报长期不提升。  
  **原因**：学习率过高或探索不足导致策略陷入次优。  
  **解决**：降低学习率并提高探索强度（如 epsilon/entropy）。
- **症状**：不同机器复现结果差异大。  
  **原因**：环境版本、随机种子或预处理流程不一致。  
  **解决**：锁定依赖版本并统一 seed 与评测脚本。

## 参考资料

- [Sutton & Barto《Reinforcement Learning》](http://incompleteideas.net/book/the-book.html)
- [David Silver RL Course](https://www.davidsilver.uk/teaching/)

