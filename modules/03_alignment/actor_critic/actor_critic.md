# Actor-Critic

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šRL çš„åŸºç¡€ï¼šå¥½ç»“æœåŠ åˆ†ã€åç»“æœæ‰£åˆ†ï¼Œç”¨æ¢¯åº¦é©±åŠ¨ç­–ç•¥è¿›åŒ–

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šåè®­ç»ƒï¼ˆPost-trainingï¼‰ä¹‹ç­–ç•¥ä¼˜åŒ–åŸºç¡€ã€‚
- **ç±»å‹**ï¼šæ··åˆæ¶æ„ï¼ˆPolicy-based + Value-basedï¼‰ã€‚
- **ä½œç”¨**ï¼šå®ƒæ˜¯ PPO / RLHF çš„åº•å±‚èŒƒå¼ã€‚é€šè¿‡â€œè¡Œä¸ºè€…-åˆ¤å®˜â€åä½œï¼Œåœ¨æå‡æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæå¤§é™ä½äº†å­¦ä¹ è¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ï¼ˆæ–¹å·®ï¼‰ã€‚

## å®šä¹‰ä¸ç›®æ ‡

Actor-Critic æ˜¯ä¸€ç§å°†â€œç­–ç•¥æ¢¯åº¦â€ä¸â€œä»·å€¼è¯„ä¼°â€ç›¸ç»“åˆçš„ç»å…¸æ¨¡å‹æ¶æ„ï¼š

- **Actor (è¡Œä¸ºè€…)**ï¼šç­–ç•¥ç½‘ç»œã€‚è´Ÿè´£æ ¹æ®å½“å‰çš„æŒ‡ä»¤ï¼Œé¢„æµ‹å¹¶ç”Ÿæˆå…·ä½“çš„å›ç­”ï¼ˆActionï¼‰ã€‚
- **Critic (åˆ¤å®˜/è®°è´¦å‘˜)**ï¼šä»·å€¼ç½‘ç»œï¼ˆValue Headï¼‰ã€‚å®ƒä¸ç”Ÿäº§å†…å®¹ï¼Œè€Œæ˜¯è¯„ä¼°å½“å‰çŠ¶æ€çš„â€œä¼˜åŠ£â€ï¼Œå¹¶é¢„ä¼°æœªæ¥çš„æ€»å¥–åŠ±ã€‚

åœ¨ LLM è®­ç»ƒä¸­ï¼ŒCritic å°±åƒæ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼šè®¡ï¼Œæ—¶åˆ»ç›¯ç€ Actor çš„äº§å‡ºï¼Œåˆ¤æ–­å…¶æ˜¯å¦è¶…é¢„æœŸåœ°è·å¾—äº†é«˜åˆ†ã€‚

## é€‚ç”¨åœºæ™¯ä¸è¾¹ç•Œ

- **é€‚ç”¨åœºæ™¯**ï¼šç”¨äºæ„å»ºæŒ‡ä»¤è·Ÿéšã€åå¥½å¯¹é½ä¸å¥–åŠ±é©±åŠ¨ä¼˜åŒ–æµç¨‹ã€‚
- **ä¸é€‚ç”¨åœºæ™¯**ï¼šä¸é€‚ç”¨äºç¼ºå°‘é«˜è´¨é‡åå¥½æ•°æ®æˆ–è¯„æµ‹ä½“ç³»çš„ç›´æ¥è½åœ°ã€‚
- **ä½¿ç”¨è¾¹ç•Œ**ï¼šå¯¹é½æ”¶ç›Šå—æ•°æ®è´¨é‡ã€å¥–åŠ±å»ºæ¨¡ä¸ KL çº¦æŸç­–ç•¥å½±å“æ˜æ˜¾ã€‚

## å…³é”®æ­¥éª¤

1. **é‡‡æ · (Sampling)**ï¼šActor æ¥å—æŒ‡ä»¤ï¼Œç”Ÿæˆä¸€ç»„å¯¹è¯ã€‚
2. **æ‰“åˆ† (Reward Calculation)**ï¼šæ¨¡å‹è·å¾—ä¸€ä¸ªå¥–åŠ±åˆ†ï¼ˆæ¥è‡ª RM æ¨¡å‹ï¼‰ã€‚
3. **ä¼°å€¼ (Value Estimation)**ï¼šCritic å¯¹å½“å‰çš„å¯¹è¯çŠ¶æ€ç»™å‡ºä¸€ä¸ªâ€œé¢„ä¼°åˆ†â€ã€‚
4. **è®¡ç®—ä¼˜åŠ¿ (Advantage Computation)**ï¼šè®¡ç®—çœŸå®å¾—åˆ†æ¯” Critic é¢„ä¼°çš„å¾—åˆ†é«˜å‡ºå¤šå°‘ï¼ˆ $\mathrm{Reward} - \mathrm{Value}$ ï¼‰ã€‚
5. **åŒå‘æ›´æ–° (Update)**ï¼š
   - **æ›´æ–° Actor**ï¼šå¦‚æœä¼˜åŠ¿ä¸ºæ­£ï¼Œå¢åŠ è¯¥ç”Ÿæˆè¡Œä¸ºå‡ºç°çš„æ¦‚ç‡ã€‚
   - **æ›´æ–° Critic**ï¼šå‡å°å…¶é¢„ä¼°åˆ†ä¸çœŸå®åˆ†æ•°ä¹‹é—´çš„è¯¯å·®ï¼Œä½¿å…¶é¢„æµ‹æ›´å‡†ã€‚

## å…³é”®å…¬å¼

### 1. ä¼˜åŠ¿ä¼°è®¡ (Advantage)

$$\hat{A}_t = \mathrm{Reward}_t - V_\phi(s_t)$$

- å¦‚æœ $\hat{A}_t > 0$ ï¼Œè¯´æ˜ Actor çš„è¡¨ç°ä¼˜äºé¢„æœŸï¼Œåº”å½“è·å¾—æ­£åé¦ˆã€‚

### 2. Actor ç›®æ ‡ (ç­–ç•¥æ¢¯åº¦)

$$L_{actor} = - \log \pi_\theta(a|s) \cdot \hat{A}_t$$

- é€šè¿‡ä¼˜åŠ¿å‡½æ•°åŠ æƒï¼Œä½¿é«˜ Advantage çš„åŠ¨ä½œæ¦‚ç‡å˜å¤§ã€‚

### 3. Critic ç›®æ ‡ (ä»·å€¼å‡æ–¹è¯¯å·®)

$$L_{critic} = \frac{1}{2} (V_\phi(s_t) - G_t)^2$$

- $G_t$ ä¸ºçœŸå®ç´¯è®¡å¥–åŠ±ï¼ŒCritic é€šè¿‡å›å½’å­¦ä¹ å‡å°è¯¯å·®ã€‚

## ä¸ç›¸è¿‘æ–¹æ³•åŒºåˆ«

1. ç›¸æ¯” `Policy Gradient`ï¼šå¤šäº† Criticï¼Œé€šå¸¸æ›´ç¨³å®šã€æ›´é«˜æ ·æœ¬æ•ˆç‡ã€‚
2. ç›¸æ¯” `PPO`ï¼šActor-Critic æ˜¯ç»“æ„èŒƒå¼ï¼ŒPPO æ˜¯å…·ä½“ä¼˜åŒ–ç›®æ ‡/çº¦æŸç­–ç•¥ã€‚
3. ç›¸æ¯” `GAE`ï¼šGAE æ˜¯ä¼˜åŠ¿ä¼°è®¡æŠ€æœ¯ï¼Œå¯ä½œä¸º Actor-Critic çš„ç»„æˆéƒ¨åˆ†ã€‚

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜ï¼šActor-Critic å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

class ActorCritic(nn.Module):
    """Actor-Critic å…±äº«åº•å±‚ç‰¹å¾"""
    def __init__(self, state_dim, action_dim, hidden=128):
        super().__init__()
        # å…±äº«ç‰¹å¾æå–å±‚
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden),
            nn.ReLU(),
        )
        # Actor Head: è¾“å‡ºåŠ¨ä½œæ¦‚ç‡
        self.actor = nn.Sequential(
            nn.Linear(hidden, action_dim),
            nn.Softmax(dim=-1),
        )
        # Critic Head: è¾“å‡º V(s) çŠ¶æ€ä»·å€¼
        self.critic = nn.Linear(hidden, 1)

    def forward(self, x):
        features = self.shared(x)
        action_probs = self.actor(features)
        state_value = self.critic(features)
        return action_probs, state_value

# è®­ç»ƒå¾ªç¯
model = ActorCritic(state_dim=4, action_dim=2)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
gamma = 0.99

for episode in range(1000):
    state = env.reset()
    done = False

    while not done:
        state_tensor = torch.FloatTensor(state)
        probs, value = model(state_tensor)

        # Actor: é‡‡æ ·åŠ¨ä½œ
        dist = Categorical(probs)
        action = dist.sample()

        next_state, reward, done, _ = env.step(action.item())
        _, next_value = model(torch.FloatTensor(next_state))

        # Critic: è®¡ç®— TD ç›®æ ‡ä¸ Advantage
        td_target = reward + gamma * next_value * (1 - done)
        advantage = td_target - value              # A(s) = R + Î³V(s') - V(s)

        # åŒå‘æ›´æ–°
        actor_loss = -dist.log_prob(action) * advantage.detach()  # ç­–ç•¥æ¢¯åº¦
        critic_loss = advantage.pow(2)                             # ä»·å€¼å›å½’

        loss = actor_loss + 0.5 * critic_loss      # è”åˆæŸå¤±
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        state = next_state
```

### åœ¨ LLMï¼ˆPPOï¼‰ä¸­çš„å¯¹åº”

```python
# PPO ä¸­çš„ Actor-Critic æ¶æ„
from trl import AutoModelForCausalLMWithValueHead

# è‡ªåŠ¨ä¸º CausalLM åŠ è£… Value Headï¼ˆCriticï¼‰
model = AutoModelForCausalLMWithValueHead.from_pretrained("Qwen/Qwen2.5-7B")

# model.pretrained_model â†’ Actorï¼ˆç”Ÿæˆå›å¤ï¼‰
# model.v_head           â†’ Criticï¼ˆé¢„ä¼°ä»·å€¼ï¼‰
# è®­ç»ƒæ—¶ä¸¤è€…åŒæ­¥æ›´æ–°
```

---

## å…³é”®æ­¥éª¤ä»£ç ï¼ˆçº¯æ–‡æ¡£ç¤ºä¾‹ï¼‰

```python
# å…³é”®æ­¥éª¤ä»£ç ï¼ˆç¤ºæ„ï¼‰
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```

## å·¥ç¨‹å®ç°è¦ç‚¹

- ä¼˜å…ˆä¿è¯æ•°æ®è´¨é‡ä¸è¯„æµ‹ä¸€è‡´æ€§ï¼Œå†æ”¾å¤§è®­ç»ƒè§„æ¨¡ã€‚
- åœ¨çº¿/ç¦»çº¿å¯¹é½éœ€åˆ†åˆ«ç›‘æ§ç¨³å®šæ€§ã€å¥–åŠ±æ¼‚ç§»ä¸è¿‡ä¼˜åŒ–é£é™©ã€‚
- ä¿æŒå‚è€ƒæ¨¡å‹ä¸è®­ç»ƒæ¨¡å‹ç‰ˆæœ¬å¯è¿½è¸ªï¼Œä¾¿äºå›æº¯é—®é¢˜ã€‚

## å¸¸è§é”™è¯¯ä¸æ’æŸ¥

- **ç—‡çŠ¶**ï¼šå¥–åŠ±å‡é«˜ä½†äººå·¥ä½“éªŒä¸‹é™ã€‚  
  **åŸå› **ï¼šå¥–åŠ±é»‘å®¢æˆ–åå¥½æ¨¡å‹åå·®å¯¼è‡´ç›®æ ‡é”™ä½ã€‚  
  **è§£å†³**ï¼šå¼•å…¥äººå·¥æŠ½æ£€ä¸å¤šæŒ‡æ ‡çº¦æŸï¼Œé™åˆ¶å•ä¸€å¥–åŠ±é©±åŠ¨ã€‚
- **ç—‡çŠ¶**ï¼šè®­ç»ƒä¸ç¨³å®šæˆ–å‘æ•£ã€‚  
  **åŸå› **ï¼šå­¦ä¹ ç‡/KL ç³»æ•°/æ‰¹é‡é…ç½®ä¸åŒ¹é…ã€‚  
  **è§£å†³**ï¼šç¼©å°è¶…å‚æœç´¢èŒƒå›´å¹¶åˆ†é˜¶æ®µå¢å¤§è®­ç»ƒå¼ºåº¦ã€‚

## å‚è€ƒèµ„æ–™

- [InstructGPT](https://arxiv.org/abs/2203.02155)
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)

