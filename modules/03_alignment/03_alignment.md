# Alignment 分类

> [!TIP]
> **一句话通俗理解**：让模型从“会续写”变成“会按人类偏好回答”的后训练路线图。

## 定义与目标

- **定义**：Alignment 是通过监督数据、偏好数据与强化学习信号约束模型行为的后训练阶段。
- **目标**：提升模型的有用性、可控性与安全性，使其输出更符合用户意图与偏好。

## 适用场景与边界

- **适用场景**：用于构建指令跟随、偏好对齐与奖励驱动优化流程。
- **不适用场景**：不适用于缺少高质量偏好数据或评测体系的直接落地。
- **使用边界**：对齐收益受数据质量、奖励建模与 KL 约束策略影响明显。

## 关键步骤

1. `SFT`：用高质量指令数据建立基础行为。
2. `DPO/GRPO/PPO`：引入偏好与奖励信号优化回答质量。
3. `RLHF`：整合 SFT、奖励模型与策略优化，形成闭环。
4. 评估与回流：通过离线评估和人工反馈迭代数据与策略。

建议学习顺序：`sft -> dpo -> ppo/grpo -> rlhf`

## 关键公式

`J(theta) = E_{(x,y)~pi_theta}[r(x,y)] - beta * KL(pi_theta || pi_ref)`

符号说明：
- `r(x, y)`：奖励模型或偏好信号给出的评分。
- `pi_ref`：参考策略（约束模型别偏离太远）。
- `beta`：KL 约束强度。

## 关键步骤代码（纯文档示例）

```python
for batch in train_loader:
    samples = policy.generate(batch["prompt"])
    reward = reward_model.score(batch["prompt"], samples)
    kl = kl_divergence(policy, ref_policy, batch["prompt"], samples)
    loss = -(reward - beta * kl).mean()
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

## 子模块导航

- `sft`: 监督微调基础。
- `dpo`: 离线偏好优化。
- `grpo`: 组相对优势优化。
- `ppo`: 在线策略优化。
- `policy_gradient`: REINFORCE 基础。
- `actor_critic`: 策略-价值联合训练。
- `rlhf`: 奖励模型与策略优化闭环。

## 工程实现要点

- 优先保证数据质量与评测一致性，再放大训练规模。
- 在线/离线对齐需分别监控稳定性、奖励漂移与过优化风险。
- 保持参考模型与训练模型版本可追踪，便于回溯问题。

## 常见错误与排查

- **症状**：奖励升高但人工体验下降。  
  **原因**：奖励黑客或偏好模型偏差导致目标错位。  
  **解决**：引入人工抽检与多指标约束，限制单一奖励驱动。
- **症状**：训练不稳定或发散。  
  **原因**：学习率/KL 系数/批量配置不匹配。  
  **解决**：缩小超参搜索范围并分阶段增大训练强度。

## 与相近方法对比

| 方法 | 优点 | 局限 | 适用场景 |
| --- | --- | --- | --- |
| 本文主题方法 | 紧贴本节问题定义 | 依赖数据与实现质量 | 适合结构化评测与迭代优化 |
| 对比方法A | 上手成本更低 | 能力上限可能受限 | 快速原型与基线对照 |
| 对比方法B | 上限潜力更高 | 调参与资源成本更高 | 高要求生产或复杂任务场景 |

## 参考资料

- [InstructGPT](https://arxiv.org/abs/2203.02155)
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
