# æ•°æ®å·¥ç¨‹ä¸æ¨¡å‹è¯„ä¼° (Data & Evaluation)

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šæ•°æ®å·¥ç¨‹é€šè¿‡æ¸…æ´—ã€é…æ¯”ã€å»é‡å’Œè¯„æµ‹é—­ç¯å†³å®šå¯¹é½ä¸Šé™ï¼Œæ˜¯æ¯”æ¨¡å‹ç»“æ„æ›´å¸¸è§çš„æ•ˆæœç“¶é¢ˆã€‚

## æ•°æ®å¤„ç† (Data Processing)

é«˜è´¨é‡æ•°æ®æ˜¯ LLM èƒ½åŠ›çš„ä¸Šé™ã€‚

### 1. é¢„è®­ç»ƒä¸ SFT æ•°æ®æ¸…æ´—

- **å»é‡ (Deduplication)**ï¼šä½¿ç”¨ MinHash æˆ– LSH ç®—æ³•å‰”é™¤æµ·é‡ç½‘é¡µä¸­çš„é‡å¤å†…å®¹ã€‚
- **è¯­è¨€è¿‡æ»¤**ï¼šä½¿ç”¨ç‰¹å¾å“ˆå¸Œæˆ– FastText è¯†åˆ«è¯­ç§ã€‚
- **è´¨é‡è¯„åˆ†**ï¼šåˆ©ç”¨å¯å‘å¼è§„åˆ™ï¼ˆå¦‚ç¬¦å·å¯†åº¦ã€å›°æƒ‘åº¦ PPLï¼‰æˆ–å°æ¨¡å‹æ‰“åˆ†å‰”é™¤åƒåœ¾æ•°æ®ã€‚

### 2. æŒ‡ä»¤éµå¾ªæ•°æ®é‡‡æ ·

- **å¤šæ ·æ€§ (Diversity)**ï¼šé€šè¿‡ K-Means èšç±»ç¡®ä¿æŒ‡ä»¤è¦†ç›–æ•°å­¦ã€ä»£ç ã€åˆ›æ„å†™ä½œç­‰å¤šä¸ªç»´åº¦ã€‚
- **å¤æ‚åº¦é‡‡æ ·**ï¼šä¼˜å…ˆä¿ç•™é€»è¾‘é“¾æ¡ (CoT) å®Œæ•´çš„é«˜è´¨é‡æ ·æœ¬ã€‚

### 3. åˆæˆæ•°æ®ä¸ä»¿çœŸ (Synthetic & Simulation)

- **Adversarial User Generation**ï¼šåˆæˆå…·å¤‡æŒ‘æˆ˜æ€§ã€ç”šè‡³æ˜¯â€œæœ‰æ¯’â€çš„è¾¹ç¼˜æ¡ˆä¾‹ï¼ˆEdge Casesï¼‰ï¼Œç”¨äºæµ‹è¯• Agent çš„å®‰å…¨æ€§ä¸é²æ£’æ€§ã€‚
- **Multi-turn Interaction Synthesis**ï¼šåˆ©ç”¨ LLM æ¨¡æ‹Ÿå¤šè½®å¯¹è¯è½¨è¿¹ï¼Œè§£å†³å†·å¯åŠ¨æ—¶çœŸå®äº¤äº’æ•°æ®åŒ®ä¹çš„é—®é¢˜ã€‚
- **éšç§ä¿æŠ¤ (Privacy-preserving)**ï¼šåœ¨åˆæˆæ•°æ®ä¸­è‡ªåŠ¨å‰”é™¤æˆ–æ›¿æ¢æ•æ„Ÿ PII ä¿¡æ¯ï¼ˆä¸ªäººèº«ä»½ä¿¡æ¯ï¼‰ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„åˆè§„æ€§ã€‚

---

## æ¨¡å‹è¯„ä¼° (Model Evaluation)

å¦‚ä½•é‡åŒ–â€œå¤§æ¨¡å‹å˜èªæ˜äº†â€ï¼Ÿ

### 1. è‡ªåŠ¨åŒ–è¯„æµ‹ (Benchmarks)

- **é€‰æ‹©é¢˜ç±»**ï¼šMMLU, C-Eval, GSM8K (æ•°å­¦), HumanEval (ä»£ç )ã€‚
- **ç—›ç‚¹**ï¼šBenchmark æ±¡æŸ“é—®é¢˜ï¼ˆé¢˜ç›®å‡ºç°åœ¨è®­ç»ƒé›†ä¸­ï¼‰ã€‚

### 2. æ™ºèƒ½ä½“è¯„æµ‹ (Agent Evaluation)

- **ä»»åŠ¡æˆåŠŸç‡ (Success Rate)**ï¼šé’ˆå¯¹å…·ä½“æŒ‡ä»¤ï¼ˆå¦‚â€œè®¢ä¸€å¼ æœºç¥¨â€ï¼‰çš„ç«¯åˆ°ç«¯å®Œæˆæƒ…å†µã€‚
- **LLM-as-a-Judge**ï¼šåˆ©ç”¨å¼ºæ¨¡å‹ (GPT-4) ä½œä¸ºè£åˆ¤ã€‚å¼•å…¥ **æ³›åŒ–æ€§åˆ†æ**ï¼Œç¡®ä¿æ¨¡å‹ä¸æ˜¯åœ¨èƒŒè¯µç‰¹å®šçš„ Tool Calling åºåˆ—ã€‚
- **å¯¹æŠ—æ€§æµ‹è¯• (Adversarial Testing)**ï¼šé€šè¿‡æ¨¡æ‹Ÿå™¨å‘èµ·éé¢„æœŸæŒ‡ä»¤ï¼Œè¯„ä¼° Agent çš„æ‹’ç»æœåŠ¡ä¸é˜²å¾¡èƒ½åŠ›ã€‚

### 3. é•¿æ•ˆè¯„ä¼°å·¥å…·é“¾

- **Elo Rating**ï¼šç±»ä¼¼äºç«æŠ€æ¸¸æˆçš„æ’åç³»ç»Ÿï¼Œé€šè¿‡æ¨¡å‹åŒç›²å¯¹æˆ˜è·å–ç›¸å¯¹èƒœç‡ã€‚
- **æŒç»­è¯„æµ‹ (Continuous Eval)**ï¼šé›†æˆåˆ° CI/CD æµç¨‹ä¸­ï¼Œç¡®ä¿æ¯æ¬¡å¾®è°ƒï¼ˆSFT/DPOï¼‰ä¸ä¼šå¯¼è‡´æ—§èƒ½åŠ›çš„é€€åŒ–ï¼ˆRegressionï¼‰ã€‚

---

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜

### 1. MinHash æ–‡æœ¬å»é‡

```python
from datasketch import MinHash, MinHashLSH

def create_minhash(text, num_perm=128):
    """å°†æ–‡æœ¬è½¬ä¸º MinHash æŒ‡çº¹"""
    m = MinHash(num_perm=num_perm)
    for word in text.split():
        m.update(word.encode("utf-8"))
    return m

# å»ºç«‹ LSH ç´¢å¼•ï¼ˆé˜ˆå€¼ 0.8 = 80% ç›¸ä¼¼å³è®¤ä¸ºé‡å¤ï¼‰
lsh = MinHashLSH(threshold=0.8, num_perm=128)

unique_data = []
for i, item in enumerate(dataset):
    mh = create_minhash(item["text"])
    # æŸ¥è¯¢æ˜¯å¦æœ‰è¿‘ä¼¼é‡å¤
    if not lsh.query(mh):
        lsh.insert(f"doc_{i}", mh)
        unique_data.append(item)

print(f"å»é‡å‰: {len(dataset)} â†’ å»é‡å: {len(unique_data)}")
```

### 2. PPL è´¨é‡è¿‡æ»¤

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B")

def compute_ppl(text, max_length=512):
    """è®¡ç®—æ–‡æœ¬å›°æƒ‘åº¦ï¼ˆPPL è¶Šä½ = è´¨é‡è¶Šé«˜ï¼‰"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=max_length).to(model.device)
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
    return torch.exp(outputs.loss).item()

# è¿‡æ»¤é«˜ PPLï¼ˆä½è´¨é‡ï¼‰æ–‡æœ¬
filtered = [item for item in dataset if compute_ppl(item["text"]) < 50.0]
```

### 3. LLM-as-Judge è¯„æµ‹

```python
from openai import OpenAI

client = OpenAI()

def llm_judge(question, answer_a, answer_b):
    """ä½¿ç”¨ GPT-4 ä½œä¸ºè£åˆ¤ï¼Œå¯¹æ¯”ä¸¤ä¸ªå›ç­”"""
    prompt = f"""è¯·ä½œä¸ºå…¬æ­£çš„è£åˆ¤ï¼Œè¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ª AI åŠ©æ‰‹å¯¹ç”¨æˆ·é—®é¢˜çš„å›ç­”è´¨é‡ã€‚

é—®é¢˜ï¼š{question}
å›ç­” Aï¼š{answer_a}
å›ç­” Bï¼š{answer_b}

è¯·ä»ä»¥ä¸‹ç»´åº¦æ‰“åˆ†ï¼ˆ1-10ï¼‰ï¼šå‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€æ¸…æ™°åº¦ã€‚
è¾“å‡ºæ ¼å¼ï¼š{{"winner": "A" æˆ– "B", "reason": "ç®€è¦åŸå› "}}"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0,
    )
    return response.choices[0].message.content

# æ‰¹é‡è¯„æµ‹
results = []
for item in eval_dataset:
    judge_result = llm_judge(item["question"], item["model_a"], item["model_b"])
    results.append(judge_result)
```

### 4. è‡ªåŠ¨åŒ–è¯„æµ‹ (lm-evaluation-harness)

```bash
# å®‰è£…
pip install lm-eval

# è¯„æµ‹ MMLU
lm_eval --model hf \
    --model_args pretrained=Qwen/Qwen2.5-7B \
    --tasks mmlu \
    --batch_size 8

# è¯„æµ‹ GSM8Kï¼ˆæ•°å­¦æ¨ç†ï¼‰
lm_eval --model hf \
    --model_args pretrained=saves/qwen2.5-7b-sft-merged \
    --tasks gsm8k \
    --num_fewshot 5

# è¯„æµ‹ HumanEvalï¼ˆä»£ç ç”Ÿæˆï¼‰
lm_eval --model hf \
    --model_args pretrained=saves/qwen2.5-7b-sft-merged \
    --tasks humaneval \
    --batch_size 1
```

---
## å®šä¹‰ä¸ç›®æ ‡

- **å®šä¹‰**ï¼šæ•°æ®å·¥ç¨‹ä¸æ¨¡å‹è¯„ä¼° (Data & Evaluation) å±äºâ€œåè®­ç»ƒå¯¹é½æ¨¡å—ï¼Œèšç„¦ SFTã€åå¥½ä¼˜åŒ–ä¸ RLHF ç³»åˆ—æ–¹æ³•ã€‚â€èŒƒç•´ã€‚
- **ç›®æ ‡**ï¼šåœ¨èƒ½åŠ›ã€å¯æ§æ€§ä¸å®‰å…¨æ€§ä¹‹é—´å»ºç«‹å¯è¿­ä»£çš„å¯¹é½è®­ç»ƒé—­ç¯ã€‚
## é€‚ç”¨åœºæ™¯ä¸è¾¹ç•Œ

- **é€‚ç”¨åœºæ™¯**ï¼šç”¨äºæ„å»ºæŒ‡ä»¤è·Ÿéšã€åå¥½å¯¹é½ä¸å¥–åŠ±é©±åŠ¨ä¼˜åŒ–æµç¨‹ã€‚
- **ä¸é€‚ç”¨åœºæ™¯**ï¼šä¸é€‚ç”¨äºç¼ºå°‘é«˜è´¨é‡åå¥½æ•°æ®æˆ–è¯„æµ‹ä½“ç³»çš„ç›´æ¥è½åœ°ã€‚
- **ä½¿ç”¨è¾¹ç•Œ**ï¼šå¯¹é½æ”¶ç›Šå—æ•°æ®è´¨é‡ã€å¥–åŠ±å»ºæ¨¡ä¸ KL çº¦æŸç­–ç•¥å½±å“æ˜æ˜¾ã€‚

## å…³é”®æ­¥éª¤

1. æ„å»ºå¯¹é½æ•°æ®ä¸åå¥½ä¿¡å·ï¼ˆæŒ‡ä»¤æ•°æ®/åå¥½å¯¹/å¥–åŠ±æ¨¡å‹ï¼‰ã€‚
2. åœ¨çº¦æŸæ¡ä»¶ä¸‹ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿è¾“å‡ºæ›´ç¬¦åˆäººç±»åå¥½ã€‚
3. è”åˆæœ‰ç”¨æ€§ã€å®‰å…¨æ€§ä¸ç¨³å®šæ€§æŒ‡æ ‡è¿›è¡Œè¿­ä»£è¯„ä¼°ã€‚
## å…³é”®å…¬å¼ï¼ˆé€»è¾‘è¡¨è¾¾ï¼‰

`J(theta) = E[r(x, y)] - beta * KL(pi_theta || pi_ref)`

ç¬¦å·è¯´æ˜ï¼š
- `r(x, y)`ï¼šå¥–åŠ±æˆ–åå¥½è¯„åˆ†ã€‚
- `beta`ï¼šçº¦æŸå¼ºåº¦ç³»æ•°ã€‚
- `KL`ï¼šç­–ç•¥åç§»æƒ©ç½šé¡¹ã€‚
## å…³é”®æ­¥éª¤ä»£ç ï¼ˆçº¯æ–‡æ¡£ç¤ºä¾‹ï¼‰

```python
# å…³é”®æµç¨‹ç¤ºæ„ï¼ˆä¸å…·ä½“å·¥ç¨‹å®ç°è§£è€¦ï¼‰
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```

## å·¥ç¨‹å®ç°è¦ç‚¹

- ä¼˜å…ˆä¿è¯æ•°æ®è´¨é‡ä¸è¯„æµ‹ä¸€è‡´æ€§ï¼Œå†æ”¾å¤§è®­ç»ƒè§„æ¨¡ã€‚
- åœ¨çº¿/ç¦»çº¿å¯¹é½éœ€åˆ†åˆ«ç›‘æ§ç¨³å®šæ€§ã€å¥–åŠ±æ¼‚ç§»ä¸è¿‡ä¼˜åŒ–é£é™©ã€‚
- ä¿æŒå‚è€ƒæ¨¡å‹ä¸è®­ç»ƒæ¨¡å‹ç‰ˆæœ¬å¯è¿½è¸ªï¼Œä¾¿äºå›æº¯é—®é¢˜ã€‚

## å¸¸è§é”™è¯¯ä¸æ’æŸ¥

- **ç—‡çŠ¶**ï¼šå¥–åŠ±å‡é«˜ä½†äººå·¥ä½“éªŒä¸‹é™ã€‚  
  **åŸå› **ï¼šå¥–åŠ±é»‘å®¢æˆ–åå¥½æ¨¡å‹åå·®å¯¼è‡´ç›®æ ‡é”™ä½ã€‚  
  **è§£å†³**ï¼šå¼•å…¥äººå·¥æŠ½æ£€ä¸å¤šæŒ‡æ ‡çº¦æŸï¼Œé™åˆ¶å•ä¸€å¥–åŠ±é©±åŠ¨ã€‚
- **ç—‡çŠ¶**ï¼šè®­ç»ƒä¸ç¨³å®šæˆ–å‘æ•£ã€‚  
  **åŸå› **ï¼šå­¦ä¹ ç‡/KL ç³»æ•°/æ‰¹é‡é…ç½®ä¸åŒ¹é…ã€‚  
  **è§£å†³**ï¼šç¼©å°è¶…å‚æœç´¢èŒƒå›´å¹¶åˆ†é˜¶æ®µå¢å¤§è®­ç»ƒå¼ºåº¦ã€‚

## ä¸ç›¸è¿‘æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ä¼˜ç‚¹ | å±€é™ | é€‚ç”¨åœºæ™¯ |
| --- | --- | --- | --- |
| æœ¬æ–‡ä¸»é¢˜æ–¹æ³• | ç´§è´´æœ¬èŠ‚é—®é¢˜å®šä¹‰ | ä¾èµ–æ•°æ®ä¸å®ç°è´¨é‡ | é€‚åˆç»“æ„åŒ–è¯„æµ‹ä¸è¿­ä»£ä¼˜åŒ– |
| å¯¹æ¯”æ–¹æ³•A | ä¸Šæ‰‹æˆæœ¬æ›´ä½ | èƒ½åŠ›ä¸Šé™å¯èƒ½å—é™ | å¿«é€ŸåŸå‹ä¸åŸºçº¿å¯¹ç…§ |
| å¯¹æ¯”æ–¹æ³•B | ä¸Šé™æ½œåŠ›æ›´é«˜ | è°ƒå‚ä¸èµ„æºæˆæœ¬æ›´é«˜ | é«˜è¦æ±‚ç”Ÿäº§æˆ–å¤æ‚ä»»åŠ¡åœºæ™¯ |

## å‚è€ƒèµ„æ–™

- [InstructGPT](https://arxiv.org/abs/2203.02155)
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)

