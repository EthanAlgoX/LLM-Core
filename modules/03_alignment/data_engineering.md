# æ•°æ®å·¥ç¨‹ä¸æ¨¡å‹è¯„ä¼° (Data & Evaluation)

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šè®­ç»ƒæ•°æ®çš„è´¨é‡å†³å®šæ¨¡å‹ä¸Šé™ï¼Œç”¨ LLM æ¥ç»™ LLM æ‰“åˆ†åšç­›é€‰

## æ•°æ®å¤„ç† (Data Processing)

é«˜è´¨é‡æ•°æ®æ˜¯ LLM èƒ½åŠ›çš„ä¸Šé™ã€‚

### 1. é¢„è®­ç»ƒä¸ SFT æ•°æ®æ¸…æ´—

- **å»é‡ (Deduplication)**ï¼šä½¿ç”¨ MinHash æˆ– LSH ç®—æ³•å‰”é™¤æµ·é‡ç½‘é¡µä¸­çš„é‡å¤å†…å®¹ã€‚
- **è¯­è¨€è¿‡æ»¤**ï¼šä½¿ç”¨ç‰¹å¾å“ˆå¸Œæˆ– FastText è¯†åˆ«è¯­ç§ã€‚
- **è´¨é‡è¯„åˆ†**ï¼šåˆ©ç”¨å¯å‘å¼è§„åˆ™ï¼ˆå¦‚ç¬¦å·å¯†åº¦ã€å›°æƒ‘åº¦ PPLï¼‰æˆ–å°æ¨¡å‹æ‰“åˆ†å‰”é™¤åƒåœ¾æ•°æ®ã€‚

### 2. æŒ‡ä»¤éµå¾ªæ•°æ®é‡‡æ ·

- **å¤šæ ·æ€§ (Diversity)**ï¼šé€šè¿‡ K-Means èšç±»ç¡®ä¿æŒ‡ä»¤è¦†ç›–æ•°å­¦ã€ä»£ç ã€åˆ›æ„å†™ä½œç­‰å¤šä¸ªç»´åº¦ã€‚
- **å¤æ‚åº¦é‡‡æ ·**ï¼šä¼˜å…ˆä¿ç•™é€»è¾‘é“¾æ¡ (CoT) å®Œæ•´çš„é«˜è´¨é‡æ ·æœ¬ã€‚

### 3. åˆæˆæ•°æ®ä¸ä»¿çœŸ (Synthetic & Simulation)

- **Adversarial User Generation**ï¼šåˆæˆå…·å¤‡æŒ‘æˆ˜æ€§ã€ç”šè‡³æ˜¯â€œæœ‰æ¯’â€çš„è¾¹ç¼˜æ¡ˆä¾‹ï¼ˆEdge Casesï¼‰ï¼Œç”¨äºæµ‹è¯• Agent çš„å®‰å…¨æ€§ä¸é²æ£’æ€§ã€‚
- **Multi-turn Interaction Synthesis**ï¼šåˆ©ç”¨ LLM æ¨¡æ‹Ÿå¤šè½®å¯¹è¯è½¨è¿¹ï¼Œè§£å†³å†·å¯åŠ¨æ—¶çœŸå®äº¤äº’æ•°æ®åŒ®ä¹çš„é—®é¢˜ã€‚
- **éšç§ä¿æŠ¤ (Privacy-preserving)**ï¼šåœ¨åˆæˆæ•°æ®ä¸­è‡ªåŠ¨å‰”é™¤æˆ–æ›¿æ¢æ•æ„Ÿ PII ä¿¡æ¯ï¼ˆä¸ªäººèº«ä»½ä¿¡æ¯ï¼‰ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„åˆè§„æ€§ã€‚

---

## æ¨¡å‹è¯„ä¼° (Model Evaluation)

å¦‚ä½•é‡åŒ–â€œå¤§æ¨¡å‹å˜èªæ˜äº†â€ï¼Ÿ

### 1. è‡ªåŠ¨åŒ–è¯„æµ‹ (Benchmarks)

- **é€‰æ‹©é¢˜ç±»**ï¼šMMLU, C-Eval, GSM8K (æ•°å­¦), HumanEval (ä»£ç )ã€‚
- **ç—›ç‚¹**ï¼šBenchmark æ±¡æŸ“é—®é¢˜ï¼ˆé¢˜ç›®å‡ºç°åœ¨è®­ç»ƒé›†ä¸­ï¼‰ã€‚

### 2. æ™ºèƒ½ä½“è¯„æµ‹ (Agent Evaluation)

- **ä»»åŠ¡æˆåŠŸç‡ (Success Rate)**ï¼šé’ˆå¯¹å…·ä½“æŒ‡ä»¤ï¼ˆå¦‚â€œè®¢ä¸€å¼ æœºç¥¨â€ï¼‰çš„ç«¯åˆ°ç«¯å®Œæˆæƒ…å†µã€‚
- **LLM-as-a-Judge**ï¼šåˆ©ç”¨å¼ºæ¨¡å‹ (GPT-4) ä½œä¸ºè£åˆ¤ã€‚å¼•å…¥ **æ³›åŒ–æ€§åˆ†æ**ï¼Œç¡®ä¿æ¨¡å‹ä¸æ˜¯åœ¨èƒŒè¯µç‰¹å®šçš„ Tool Calling åºåˆ—ã€‚
- **å¯¹æŠ—æ€§æµ‹è¯• (Adversarial Testing)**ï¼šé€šè¿‡æ¨¡æ‹Ÿå™¨å‘èµ·éé¢„æœŸæŒ‡ä»¤ï¼Œè¯„ä¼° Agent çš„æ‹’ç»æœåŠ¡ä¸é˜²å¾¡èƒ½åŠ›ã€‚

### 3. é•¿æ•ˆè¯„ä¼°å·¥å…·é“¾

- **Elo Rating**ï¼šç±»ä¼¼äºç«æŠ€æ¸¸æˆçš„æ’åç³»ç»Ÿï¼Œé€šè¿‡æ¨¡å‹åŒç›²å¯¹æˆ˜è·å–ç›¸å¯¹èƒœç‡ã€‚
- **æŒç»­è¯„æµ‹ (Continuous Eval)**ï¼šé›†æˆåˆ° CI/CD æµç¨‹ä¸­ï¼Œç¡®ä¿æ¯æ¬¡å¾®è°ƒï¼ˆSFT/DPOï¼‰ä¸ä¼šå¯¼è‡´æ—§èƒ½åŠ›çš„é€€åŒ–ï¼ˆRegressionï¼‰ã€‚

---

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜

### 1. MinHash æ–‡æœ¬å»é‡

```python
from datasketch import MinHash, MinHashLSH

def create_minhash(text, num_perm=128):
    """å°†æ–‡æœ¬è½¬ä¸º MinHash æŒ‡çº¹"""
    m = MinHash(num_perm=num_perm)
    for word in text.split():
        m.update(word.encode("utf-8"))
    return m

# å»ºç«‹ LSH ç´¢å¼•ï¼ˆé˜ˆå€¼ 0.8 = 80% ç›¸ä¼¼å³è®¤ä¸ºé‡å¤ï¼‰
lsh = MinHashLSH(threshold=0.8, num_perm=128)

unique_data = []
for i, item in enumerate(dataset):
    mh = create_minhash(item["text"])
    # æŸ¥è¯¢æ˜¯å¦æœ‰è¿‘ä¼¼é‡å¤
    if not lsh.query(mh):
        lsh.insert(f"doc_{i}", mh)
        unique_data.append(item)

print(f"å»é‡å‰: {len(dataset)} â†’ å»é‡å: {len(unique_data)}")
```

### 2. PPL è´¨é‡è¿‡æ»¤

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B")

def compute_ppl(text, max_length=512):
    """è®¡ç®—æ–‡æœ¬å›°æƒ‘åº¦ï¼ˆPPL è¶Šä½ = è´¨é‡è¶Šé«˜ï¼‰"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=max_length).to(model.device)
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
    return torch.exp(outputs.loss).item()

# è¿‡æ»¤é«˜ PPLï¼ˆä½è´¨é‡ï¼‰æ–‡æœ¬
filtered = [item for item in dataset if compute_ppl(item["text"]) < 50.0]
```

### 3. LLM-as-Judge è¯„æµ‹

```python
from openai import OpenAI

client = OpenAI()

def llm_judge(question, answer_a, answer_b):
    """ä½¿ç”¨ GPT-4 ä½œä¸ºè£åˆ¤ï¼Œå¯¹æ¯”ä¸¤ä¸ªå›ç­”"""
    prompt = f"""è¯·ä½œä¸ºå…¬æ­£çš„è£åˆ¤ï¼Œè¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ª AI åŠ©æ‰‹å¯¹ç”¨æˆ·é—®é¢˜çš„å›ç­”è´¨é‡ã€‚

é—®é¢˜ï¼š{question}
å›ç­” Aï¼š{answer_a}
å›ç­” Bï¼š{answer_b}

è¯·ä»ä»¥ä¸‹ç»´åº¦æ‰“åˆ†ï¼ˆ1-10ï¼‰ï¼šå‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€æ¸…æ™°åº¦ã€‚
è¾“å‡ºæ ¼å¼ï¼š{{"winner": "A" æˆ– "B", "reason": "ç®€è¦åŸå› "}}"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0,
    )
    return response.choices[0].message.content

# æ‰¹é‡è¯„æµ‹
results = []
for item in eval_dataset:
    judge_result = llm_judge(item["question"], item["model_a"], item["model_b"])
    results.append(judge_result)
```

### 4. è‡ªåŠ¨åŒ–è¯„æµ‹ (lm-evaluation-harness)

```bash
# å®‰è£…
pip install lm-eval

# è¯„æµ‹ MMLU
lm_eval --model hf \
    --model_args pretrained=Qwen/Qwen2.5-7B \
    --tasks mmlu \
    --batch_size 8

# è¯„æµ‹ GSM8Kï¼ˆæ•°å­¦æ¨ç†ï¼‰
lm_eval --model hf \
    --model_args pretrained=saves/qwen2.5-7b-sft-merged \
    --tasks gsm8k \
    --num_fewshot 5

# è¯„æµ‹ HumanEvalï¼ˆä»£ç ç”Ÿæˆï¼‰
lm_eval --model hf \
    --model_args pretrained=saves/qwen2.5-7b-sft-merged \
    --tasks humaneval \
    --batch_size 1
```

---
## å®šä¹‰ä¸ç›®æ ‡

- **å®šä¹‰**ï¼šæœ¬èŠ‚ä¸»é¢˜ç”¨äºè§£é‡Šè¯¥æ¨¡å—çš„æ ¸å¿ƒæ¦‚å¿µä¸å®ç°æ€è·¯ã€‚
- **ç›®æ ‡**ï¼šå¸®åŠ©è¯»è€…å¿«é€Ÿå»ºç«‹é—®é¢˜æŠ½è±¡ã€æ–¹æ³•è·¯å¾„ä¸å·¥ç¨‹è½åœ°æ–¹å¼ã€‚
## å…³é”®æ­¥éª¤

1. æ˜ç¡®è¾“å…¥/è¾“å‡ºä¸ä»»åŠ¡è¾¹ç•Œã€‚
2. æŒ‰æ¨¡å—ä¸»æµç¨‹æ‰§è¡Œæ ¸å¿ƒç®—æ³•æˆ–ç³»ç»Ÿæ­¥éª¤ã€‚
3. è®°å½•æŒ‡æ ‡å¹¶åšå¯¹æ¯”åˆ†æï¼Œå½¢æˆå¯å¤ç”¨ç»“è®ºã€‚
## å…³é”®å…¬å¼ï¼ˆé€»è¾‘è¡¨è¾¾ï¼‰

\[
\text{Result} = \text{Core Method}(\text{Input}, \text{Config}, \text{Constraints})
\]

ç¬¦å·è¯´æ˜ï¼š
- \(\text{Input}\)ï¼šä»»åŠ¡è¾“å…¥ã€‚
- \(\text{Config}\)ï¼šè®­ç»ƒæˆ–æ¨ç†é…ç½®ã€‚
- \(\text{Constraints}\)ï¼šæ–¹æ³•çº¦æŸï¼ˆå¦‚èµ„æºã€ç¨³å®šæ€§æˆ–å®‰å…¨è¾¹ç•Œï¼‰ã€‚
## å…³é”®æ­¥éª¤ä»£ç ï¼ˆçº¯æ–‡æ¡£ç¤ºä¾‹ï¼‰

```python
# å…³é”®æµç¨‹ç¤ºæ„ï¼ˆä¸å…·ä½“å·¥ç¨‹å®ç°è§£è€¦ï¼‰
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```
