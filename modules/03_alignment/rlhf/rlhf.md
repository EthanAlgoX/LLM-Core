# RLHF (Reinforcement Learning from Human Feedback) äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ 

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šå¤§æ¨¡å‹å¯¹é½çš„â€œç»ˆææ–¹æ¡ˆâ€ã€‚
- **ç±»å‹**ï¼šå¤šé˜¶æ®µå¤åˆæ¶æ„ï¼ˆSFT + RM + RLï¼‰ã€‚
- **ä½œç”¨**ï¼šè§£å†³ SFT åªèƒ½â€œå­—é¢å¯¹é½â€çš„é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥äººç±»åå¥½ï¼Œè®©æ¨¡å‹åœ¨é€»è¾‘ã€ä»·å€¼è§‚å’Œå¤æ‚ä»»åŠ¡å¤„ç†ä¸ŠçœŸæ­£å…·å¤‡â€œçµæ„Ÿçš„è·ƒè¿â€ã€‚

## RLHF çš„ä¸‰å¤§å…³é”®æ­¥éª¤

1. **ç¬¬ä¸€é˜¶æ®µï¼šSFT (ç›‘ç£å¾®è°ƒ)**
   - **ç›®æ ‡**ï¼šå†·å¯åŠ¨ã€‚
   - **å†…å®¹**ï¼šåœ¨é«˜è´¨é‡çš„äººç±»å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œè®©æ¨¡å‹å­¦ä¼šå¦‚ä½•å¾—ä½“åœ°è¯´è¯ã€‚
   - **çŠ¶æ€**ï¼šè¿™æ˜¯åç»­é˜¶æ®µçš„åŸºçŸ³ï¼ˆActor çš„åˆå§‹çŠ¶æ€ï¼‰ã€‚

2. **ç¬¬äºŒé˜¶æ®µï¼šRM (å¥–åŠ±å»ºæ¨¡ - Reward Modeling)**
   - **ç›®æ ‡**ï¼šè®­ç»ƒä¸€ä¸ªâ€œç”µå­è£åˆ¤â€ã€‚
   - **å†…å®¹**ï¼šç»™åŒä¸€ä¸ªé—®é¢˜æä¾›ä¸¤ä¸ªå›ç­”ï¼Œè®©äººç±»æ ‡å‡ºå“ªä¸ªæ›´å¥½ï¼ˆRankï¼‰ï¼Œç„¶åè®­ç»ƒå¥–åŠ±æ¨¡å‹å»æ‹Ÿåˆè¿™ç§åå¥½ã€‚
   - **å…³é”®å…¬å¼ï¼ˆBradley-Terry æ¨¡å‹ï¼‰**ï¼š
     $$P(y_w \succ y_l | x) = \frac{\exp(r_\phi(x, y_w))}{\exp(r_\phi(x, y_w)) + \exp(r_\phi(x, y_l))}$$

     - é€šè¿‡æœ€å°åŒ–å…¶**è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNegative Log-Likelihoodï¼‰**æ¥å­¦ä¹ ã€‚

3. **ç¬¬ä¸‰é˜¶æ®µï¼šRL (å¼ºåŒ–å­¦ä¹ ä¼˜åŒ– - PPO/GRPO)**
   - **ç›®æ ‡**ï¼šç»ˆæå¯¹é½ã€‚
   - **å†…å®¹**ï¼šåˆ©ç”¨ç¬¬äºŒé˜¶æ®µè®­ç»ƒå¥½çš„ RM ç»™ Actor æ‰“åˆ†ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆPPO æˆ– GRPOï¼‰æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ã€‚
   - **å…³é”®å…ƒç´ **ï¼šRewardï¼ˆå¾—åˆ†ï¼‰ã€Criticï¼ˆé¢„ä¼°ï¼‰ã€KL Penaltyï¼ˆé˜²æ¨¡å‹ç»ƒåºŸçš„ç´§ç®å’’ï¼‰ã€‚

## æ ¸å¿ƒæ•°å­¦ç›®æ ‡ (The RLHF Objective)

RLHF çš„æœ€ç»ˆä¼˜åŒ–ç›®æ ‡æ˜¯ä¸¤è€…çš„å¹³è¡¡ï¼š

$$\max_{\pi_\theta} \mathbb{E}_{x \sim D, y \sim \pi_\theta(y|x)} [r_\phi(x, y) - \beta D_{KL}(\pi_\theta || \pi_{ref})]$$

- **ç¬¬ä¸€éƒ¨åˆ† $r_\phi(x, y)$**ï¼šè®©ç”Ÿæˆçš„å†…å®¹å¾—åˆ°å¥–åŠ±æ¨¡å‹å°½å¯èƒ½é«˜çš„æ‰“åˆ†ã€‚
- **ç¬¬äºŒéƒ¨åˆ† $\beta D_{KL}$**ï¼šæƒ©ç½šåç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œçš„è¡Œä¸ºï¼Œç¡®ä¿è¯­è¨€ä¾ç„¶é€šé¡ºã€ç¬¦åˆåŸºæœ¬åˆ†å¸ƒã€‚

## è¿›é˜¶å¯¹é½ï¼šAgentic-RL (æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ )

ä¼ ç»Ÿçš„ RLHF å…³æ³¨å›ç­”çš„â€œå³æ—¶åå¥½â€ï¼Œè€Œ **Agentic-RL** å…³æ³¨é•¿æœŸå†³ç­–çš„â€œä»»åŠ¡æˆåŠŸç‡â€ã€‚

### 1. è®­ç»ƒèŒƒå¼

- **ç¯å¢ƒé©±åŠ¨ (Env-driven)**ï¼šæ¨¡å‹ä¸ä»…åœ¨æ–‡æœ¬åŸŸå¯¹é½ï¼Œè¿˜åœ¨é«˜çœŸæ¨¡æ‹Ÿç¯å¢ƒï¼ˆå¦‚ä»£ç æ²™ç®±ã€æµè§ˆå™¨ç¯å¢ƒï¼‰ä¸­å­¦ä¹ ã€‚
- **å†·å¯åŠ¨ä¸æ¢ç´¢**ï¼šåˆ©ç”¨å¤§é‡é«˜è´¨é‡çš„åˆæˆè½¨è¿¹åˆå§‹åŒ–ï¼ˆSFTï¼‰ï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œ **æ¢ç´¢-åˆ©ç”¨æƒè¡¡ (Exploration-Exploitation Trade-off)**ã€‚
- **é•¿ç¨‹è§„åˆ’ (Long-term Planning)**ï¼šä¼˜åŒ– Reward æ—¶ä¸ä»…çœ‹å½“å‰ Stepï¼Œæ›´çœ‹æœ€ç»ˆ Task Successã€‚

### 2. ç”¨æˆ·æ¨¡æ‹Ÿå™¨ (User Simulator)

- **Persona-driven**ï¼šæ„å»ºå…·å¤‡ç‰¹å®šäººè®¾ã€ä»»åŠ¡ç›®æ ‡å’Œç­–ç•¥å¤šæ ·æ€§çš„ç”¨æˆ·æ¨¡å‹ã€‚
- **äº¤äº’ç”Ÿæˆ**ï¼šåˆ©ç”¨ç”¨æˆ·æ¨¡æ‹Ÿå™¨ç”Ÿæˆå¤§è§„æ¨¡ã€å¤šè½®æ¬¡çš„çœŸå®äº¤äº’æ•°æ®ï¼Œç”¨äºæ¼”ç»ƒ Agent çš„å†·å¯åŠ¨ç­–ç•¥ã€‚
- **Adversarial User Generation**ï¼šåˆæˆå…·å¤‡æŒ‘æˆ˜æ€§çš„è¾¹ç¼˜æ¡ˆä¾‹ï¼ˆEdge Casesï¼‰ä»¥æå‡ Agent çš„é²æ£’æ€§ã€‚

### 3. å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL)

- **æ ¸å¿ƒç®—æ³•**ï¼š
  - **MAPPO/MADDPG**ï¼šåœ¨å¤šæ™ºèƒ½ä½“äº¤äº’åœºæ™¯ä¸‹ä¼˜åŒ–é›†ç¾¤ç­–ç•¥ã€‚
  - **å…±è¯†ä¸åä½œ**ï¼šè®­ç»ƒ Agent åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹é€šè¿‡åšå¼ˆä¸é€šä¿¡è¾¾æˆä»»åŠ¡å…±è¯†ã€‚

## RLHF vs. DPO vs. Agentic-RL

| ç»´åº¦ | RLHF | DPO | Agentic-RL |
| :--- | :--- | :--- | :--- |
| **åé¦ˆæ¥æº** | é™æ€åå¥½æ ‡ç­¾ | é™æ€åå¥½å¯¹ | åŠ¨æ€ç¯å¢ƒ Reward / æ¨¡æ‹Ÿå™¨åé¦ˆ |
| **ä¼˜åŒ–ç›®æ ‡** | ç¬¦åˆäººè¨€ï¼ˆå³æ—¶ï¼‰ | ç¬¦åˆäººè¨€ï¼ˆå³æ—¶ï¼‰ | ä»»åŠ¡é—­ç¯ï¼ˆé•¿ç¨‹æˆåŠŸç‡ï¼‰ |
| **å·¥ç¨‹å¤æ‚åº¦** | é«˜ | ä½ | æé«˜ï¼ˆæ¶‰åŠç¯å¢ƒæ¨¡æ‹Ÿä¸å¤šæ™ºèƒ½ä½“åšå¼ˆï¼‰ |

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜ï¼šRLHF ä¸‰é˜¶æ®µæµæ°´çº¿

### LLaMA Factory ä¸€ç«™å¼ RLHF

```bash
# ========== Stage 1: SFT ===========
llamafactory-cli train stage1_sft.yaml

# ========== Stage 2: RM =============
llamafactory-cli train stage2_rm.yaml

# ========== Stage 3: PPO ============
llamafactory-cli train stage3_ppo.yaml
```

**Stage 1 - SFT YAML**ï¼š

```yaml
model_name_or_path: Qwen/Qwen2.5-7B
stage: sft
dataset: my_sft_data
template: qwen
finetuning_type: lora
lora_rank: 64
output_dir: saves/stage1_sft
```

**Stage 2 - Reward Model YAML**ï¼š

```yaml
model_name_or_path: Qwen/Qwen2.5-7B
adapter_name_or_path: saves/stage1_sft     # ä» SFT ç»§ç»­
stage: rm                                   # è®­ç»ƒå¥–åŠ±æ¨¡å‹
dataset: my_preference_pairs                # åå¥½å¯¹æ•°æ®
template: qwen
finetuning_type: lora
output_dir: saves/stage2_rm
```

**Stage 3 - PPO YAML**ï¼š

```yaml
model_name_or_path: Qwen/Qwen2.5-7B
adapter_name_or_path: saves/stage1_sft     # Actor åˆå§‹åŒ–è‡ª SFT
stage: ppo
reward_model: saves/stage2_rm              # æŒ‡å‘ RM
dataset: my_ppo_prompts                    # Prompt-only
template: qwen
finetuning_type: lora
ppo_epochs: 4
learning_rate: 1.0e-6
output_dir: saves/stage3_ppo
```

### TRL å®Œæ•´ RLHF Pipeline

```python
from trl import SFTTrainer, RewardTrainer, PPOTrainer, PPOConfig
from trl import AutoModelForCausalLMWithValueHead
from transformers import AutoModelForSequenceClassification

# ---- Stage 1: SFT ----
sft_trainer = SFTTrainer(
    model="Qwen/Qwen2.5-7B",
    train_dataset=sft_dataset,
    max_seq_length=2048,
)
sft_trainer.train()
sft_trainer.save_model("saves/sft_model")

# ---- Stage 2: Reward Model ----
rm_model = AutoModelForSequenceClassification.from_pretrained(
    "saves/sft_model", num_labels=1
)
rm_trainer = RewardTrainer(
    model=rm_model,
    train_dataset=preference_dataset,    # (chosen, rejected) pairs
)
rm_trainer.train()
rm_trainer.save_model("saves/rm_model")

# ---- Stage 3: PPO ----
ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained("saves/sft_model")
ref_model = AutoModelForCausalLMWithValueHead.from_pretrained("saves/sft_model")

ppo_config = PPOConfig(batch_size=4, learning_rate=1e-6, ppo_epochs=4)
ppo_trainer = PPOTrainer(ppo_config, ppo_model, ref_model, tokenizer)

for batch in prompt_dataloader:
    responses = ppo_trainer.generate(batch["input_ids"])
    rewards = reward_model.score(batch["input_ids"], responses)
    stats = ppo_trainer.step(batch["input_ids"], responses, rewards)
```

---

## åŸå§‹è„šæœ¬è¿è¡Œ

```bash
python code/rlhf.py --reward-model <ä½ çš„RMè·¯å¾„>
```

**æŒ‡æ ‡è§£è¯»**ï¼š

- `reward`ï¼šåº”éš Step ç¨³æ­¥ä¸Šå‡ã€‚
- `kl`ï¼šåº”ä¿æŒåœ¨ 1.0~5.0ï¼Œè¿‡é«˜è¯´æ˜æ¨¡å‹åœ¨èƒ¡è¯´å…«é“ã€‚
- `loss`ï¼šPPO æŸå¤±æ³¢åŠ¨è¾ƒå¤§ï¼Œé‡ç‚¹çœ‹ Reward è¶‹åŠ¿ã€‚
