# Policy Gradientï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šåè®­ç»ƒï¼ˆPost-trainingï¼‰ä¹‹ç­–ç•¥ä¼˜åŒ–åŸºç¡€ã€‚
- **ç±»å‹**ï¼šç›´æ¥ç­–ç•¥æœç´¢ï¼ˆPolicy-based RLï¼‰ã€‚
- **ä½œç”¨**ï¼šå®ƒæ˜¯å¼ºåŒ–å­¦ä¹ ä¸­æœ€ç›´è§‚çš„ä¸€ç±»ç®—æ³•ï¼Œç›´æ¥å¯¹ç­–ç•¥å‚æ•°è¿›è¡Œæ¢¯åº¦ä¸Šå‡ã€‚å®ƒæ˜¯ PPO å’Œ Actor-Critic ç­‰é«˜çº§ç®—æ³•çš„é¼»ç¥–ã€‚

## ä»€ä¹ˆæ˜¯ Policy Gradientï¼Ÿ

ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼‰æ˜¯ä¸€ç±»ç›´æ¥å¯¹ç­–ç•¥è¿›è¡Œå‚æ•°åŒ–çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚ä¸åŒäºå­¦ä¹ ä»·å€¼å‡½æ•°ï¼ˆQ-learningï¼‰ï¼Œå®ƒç›´æ¥é€šè¿‡ä¼˜åŒ–ç¥ç»ç½‘ç»œè¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒæ¥æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ã€‚å…¶æ ¸å¿ƒå“²å­¦æ˜¯ï¼š**â€œå¦‚æœä¸€ä¸ªè¡Œä¸ºå¸¦æ¥äº†å¥½ç»“æœï¼Œé‚£å°±å¢åŠ å®ƒå‡ºç°çš„æ¦‚ç‡ï¼›åä¹‹ï¼Œåˆ™é™ä½å®ƒã€‚â€**

## è®­ç»ƒçš„å…³é”®æ­¥éª¤

1. **é‡‡æ · (Trajectory Generation)**ï¼šè®©æ¨¡å‹ï¼ˆActorï¼‰æ ¹æ®å½“å‰æ¦‚ç‡ç”Ÿæˆä¸€æ®µå®Œæ•´çš„å¯¹è¯è½¨è¿¹ $\tau$ã€‚
2. **å›æŠ¥è®¡ç®— (Return Calculation)**ï¼šè®¡ç®—è¯¥è·¯å¾„ä¸Šè·å¾—çš„æ€»å¥–åŠ± $R(\tau)$ã€‚
3. **æ¢¯åº¦ä¼°è®¡ (Gradient Estimation)**ï¼šåˆ©ç”¨å¯¹æ•°å¾®åˆ†æŠ€å·§ï¼ˆLog-Derivative Trickï¼‰è®¡ç®—æ¢¯åº¦çš„ä¼°è®¡å€¼ã€‚
4. **ç­–ç•¥æ›´æ–° (Weight Update)**ï¼šæ²¿ç€æ¢¯åº¦æ–¹å‘æ›´æ–°æ¨¡å‹å‚æ•° $\theta$ã€‚
5. **è¿­ä»£ (Iteration)**ï¼šé‡‡æ ·æ–°æ•°æ®ï¼Œä¸æ–­å¾ªç¯ï¼Œä½¿æ¨¡å‹å‘é«˜å¥–åŠ±çš„æ–¹å‘åç§»ã€‚

## æ ¸å¿ƒæ•°å­¦å…¬å¼

### 1. ç›®æ ‡å‡½æ•° (Objective)

$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]$$

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ‰€æœ‰å¯èƒ½è½¨è¿¹çš„æœŸæœ›å¥–åŠ±ã€‚

### 2. ç­–ç•¥æ¢¯åº¦åŸºæœ¬å®šç† (Policy Gradient Theorem)

$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) G_t \right]$$

- **$\nabla_\theta \log \pi_\theta(a_t | s_t)$**ï¼šè¡¨ç¤ºå¦‚ä½•è°ƒæ•´å‚æ•°æ‰èƒ½è®©æŸä¸ªåŠ¨ä½œæ¦‚ç‡å˜å¤§ã€‚
- **$G_t$ (Return)**ï¼šè¯¥åŠ¨ä½œå¸¦æ¥çš„æ€»å›æŠ¥ã€‚å®ƒæ˜¯æ¢¯åº¦çš„æƒé‡ã€‚

### 3. Log-Derivative Trick (å¯¹æ•°å¾®åˆ†æŠ€å·§)

è¿™æ˜¯å®ç°å…¬å¼è½¬åŒ–çš„å…³é”®æ¡¥æ¢ï¼š

$$\nabla_\theta \pi_\theta = \pi_\theta \frac{\nabla_\theta \pi_\theta}{\pi_\theta} = \pi_\theta \nabla_\theta \log \pi_\theta$$

è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥ç›´æ¥é€šè¿‡é‡‡æ ·ï¼ˆç”±äºæœ‰ $\pi_\theta$ é¡¹ï¼‰æ¥ä¼°è®¡æœ¬æ¥çœ‹ä¼¼æ— æ³•è®¡ç®—çš„æœŸæœ›æ¢¯åº¦ã€‚

## ä¸ç›¸è¿‘æ–¹æ³•åŒºåˆ«

1. ç›¸æ¯” `Actor-Critic`ï¼šPolicy Gradient ä¸æ˜¾å¼å­¦ä¹  value criticï¼ˆæˆ–å¼±ä¾èµ–ï¼‰ã€‚
2. ç›¸æ¯” `PPO`ï¼šPolicy Gradient é€šå¸¸æ²¡æœ‰ clip çº¦æŸï¼Œæ›´æ–°ç¨³å®šæ€§æ›´ä¾èµ–è¶…å‚ã€‚
3. ç›¸æ¯” `RLHF`ï¼šè¿™é‡Œåªæ˜¯ä¼˜åŒ–ç®—æ³•è§†è§’ï¼Œä¸æ˜¯å®Œæ•´äººç±»åé¦ˆæµæ°´çº¿ã€‚

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜ï¼šREINFORCE ç®—æ³•å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

class PolicyNetwork(nn.Module):
    """ç®€å•ç­–ç•¥ç½‘ç»œï¼šè¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºåŠ¨ä½œæ¦‚ç‡"""
    def __init__(self, state_dim, action_dim, hidden=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, action_dim),
            nn.Softmax(dim=-1),
        )

    def forward(self, x):
        return self.net(x)

    def select_action(self, state):
        probs = self.forward(state)
        dist = Categorical(probs)
        action = dist.sample()
        return action, dist.log_prob(action)

# REINFORCE è®­ç»ƒå¾ªç¯
policy = PolicyNetwork(state_dim=4, action_dim=2)
optimizer = optim.Adam(policy.parameters(), lr=1e-3)

for episode in range(1000):
    log_probs, rewards = [], []
    state = env.reset()

    # é‡‡æ ·ä¸€æ¡å®Œæ•´è½¨è¿¹
    while not done:
        state_tensor = torch.FloatTensor(state)
        action, log_prob = policy.select_action(state_tensor)
        next_state, reward, done, _ = env.step(action.item())

        log_probs.append(log_prob)
        rewards.append(reward)
        state = next_state

    # è®¡ç®—æŠ˜æ‰£å›æŠ¥ G_t
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + 0.99 * G         # Î³ = 0.99
        returns.insert(0, G)
    returns = torch.tensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # Baseline: æ ‡å‡†åŒ–

    # ç­–ç•¥æ¢¯åº¦æ›´æ–°
    loss = -sum(lp * Gt for lp, Gt in zip(log_probs, returns))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### åœ¨ LLM ä¸­çš„å¯¹åº”

åœ¨ LLM å¾®è°ƒåœºæ™¯ä¸­ï¼ŒREINFORCE çš„æ€æƒ³ä½“ç°ä¸ºï¼š

```python
# ä¼ªä»£ç ï¼šLLM ç­–ç•¥æ¢¯åº¦
for prompt in prompts:
    response = model.generate(prompt)           # Actor é‡‡æ ·
    reward = reward_model(prompt, response)      # RM æ‰“åˆ†

    log_prob = model.log_prob(response | prompt) # è®¡ç®—å¯¹æ•°æ¦‚ç‡
    loss = -log_prob * reward                    # ç­–ç•¥æ¢¯åº¦
    loss.backward()
```

> **æ³¨æ„**ï¼šåŸå§‹ REINFORCE æ–¹å·®æå¤§ï¼Œå®é™… LLM è®­ç»ƒä¸­éƒ½ä½¿ç”¨ PPO/GRPO ç­‰å¸¦ Baseline/Clipping çš„æ”¹è¿›ç‰ˆæœ¬ã€‚

---

## åŸå§‹è„šæœ¬è¿è¡Œ

```bash
cd <YOUR_PROJECT_ROOT>/post_train/alignment/policy_gradient
conda activate finetune
python code/policy_gradient.py
```
