# PPO (Proximal Policy Optimization) å¼ºåŒ–å­¦ä¹ å¯¹é½

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šç»™ AI ä¸¤ä¸ªç­”æ¡ˆè®©å®ƒé€‰å¥½çš„ï¼Œé€šè¿‡"åå¥½æ‰“åˆ†"é©¯åŒ–å®ƒè¯´äººè¯

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šåè®­ç»ƒï¼ˆPost-trainingï¼‰ä¹‹ RLHF å¯¹é½é˜¶æ®µã€‚
- **ç±»å‹**ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆPolicy Gradient + KL Constraintï¼‰ã€‚

## å…³é”®å…¬å¼

### 1. å…³é”®å…¬å¼ï¼šPPO å‰ªåˆ‡ç›®æ ‡å‡½æ•° (Clipped Objective)

PPO çš„æ ¸å¿ƒåœ¨äºå¦‚ä½•åœ¨æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œé™åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦ï¼Œç¡®ä¿è®­ç»ƒç¨³å¥ã€‚å…¶æ ¸å¿ƒç›®æ ‡å‡½æ•°ä¸ºï¼š

$$L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta)\hat{A}_t, \mathrm{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t \right) \right]$$

**å…¬å¼æ‹†è§£ä¸ç†è§£ï¼š**

- **$r_t(\theta)$ (æ¦‚ç‡æ¯”ä¾‹)**ï¼šå½“å‰æ–°ç­–ç•¥ä¸æ—§ç­–ç•¥åœ¨ç‰¹å®šåŠ¨ä½œä¸Šçš„æ¦‚ç‡æ¯”å€¼ã€‚
- **$\hat{A}_t$ (Advantage/ä¼˜åŠ¿)** ï¼šç”± Critic è¾…åŠ©è®¡ç®—ã€‚å‘Šè¯‰æ¨¡å‹å½“å‰çš„åŠ¨ä½œæ¯”å¹³å‡æ°´å¹³å¥½å¤šå°‘ã€‚
- **$\mathrm{clip}(\dots, 1-\epsilon, 1+\epsilon)$**ï¼šå‰ªåˆ‡æœºåˆ¶ã€‚å¦‚æœæ–°æ—§ç­–ç•¥å·®å¼‚è¶…è¿‡ $\epsilon$ ï¼ˆé€šå¸¸ä¸º 0.1 æˆ– 0.2ï¼‰ï¼Œåˆ™å¼ºåˆ¶å°†æ¯”ä¾‹æˆªæ–­ã€‚
- **$\min$ å‡½æ•°**ï¼šå–ä¸¤è€…çš„æœ€å°å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªä¿å®ˆç­–ç•¥ï¼šå³ä½¿ Advantage éå¸¸å¤§ï¼Œæˆ‘ä»¬ä¹Ÿä¸å¸Œæœ›ä¸€æ¬¡æ€§æ›´æ–°å¤ªçŒ›ï¼›å¦‚æœæ›´æ–°æ–¹å‘é”™äº†ï¼Œ $\min$ ä¼šè®©æ¨¡å‹å¿«é€Ÿå›è°ƒã€‚

### 2. LLM ä¸­çš„ç‰¹æ®Šçº¦æŸï¼šKL æƒ©ç½š

åœ¨ LLM çš„ PPO æµç¨‹ä¸­ï¼Œä¸ºäº†é˜²æ­¢æ¨¡å‹ä¸ºäº†åˆ·åˆ†è€Œå˜æˆâ€œå¤è¯»æœºâ€æˆ–å¤±å»é€»è¾‘ï¼Œä¼šåœ¨å¥–åŠ±å‡½æ•°ä¸­åŠ å…¥ KL æ•£åº¦é¡¹ï¼š

$$\mathrm{Reward}_{total} = \mathrm{Reward}_{RM} - \beta \cdot D_{KL}(\pi_\theta || \pi_{ref})$$

- **æ„ä¹‰**ï¼šåœ¨è¿½æ±‚é«˜åˆ†ï¼ˆRewardï¼‰çš„åŒæ—¶ï¼Œæƒ©ç½šé‚£äº›åç¦»åŸå§‹ SFT æ¨¡å‹ï¼ˆReferenceï¼‰å¤ªè¿œçš„è¡Œä¸ºã€‚å®ƒå°±åƒä¸€æ¡â€œé£ç­çº¿â€ï¼Œæ‹‰ä½æ¨¡å‹ä¸è®©å…¶äº§ç”Ÿä¸¥é‡çš„ç­–ç•¥å´©åã€‚

## æ ¸å¿ƒæ¶æ„ï¼šå››å¤§è§’è‰²

PPO è®­ç»ƒèƒŒåæ¶‰åŠå››ä¸ªå…³é”®æ¨¡å‹çš„åä½œï¼š

| è§’è‰² | æ¨¡å‹åç§° | èŒè´£æè¿° | çŠ¶æ€ |
| :--- | :--- | :--- | :--- |
| **Actor** | ç­–ç•¥æ¨¡å‹ (Policy) | æ ¸å¿ƒä¼˜åŒ–å¯¹è±¡ã€‚è´Ÿè´£æ ¹æ®æŒ‡ä»¤ç”Ÿæˆå›å¤ï¼Œç›®æ ‡æ˜¯è·å¾—æœ€é«˜å¥–åŠ±ã€‚ | **åŠ¨æ€æ›´æ–°** |
| **Reference** | å‚è€ƒæ¨¡å‹ (Ref) | å†»ç»“çš„ SFT åŸå‹ã€‚è®¡ç®— KL æ•£åº¦ï¼Œé˜²æ­¢ Actor ä¸ºäº†åˆ·åˆ†è€Œå¯¼è‡´è¯­è¨€é€»è¾‘å´©åã€‚ | **å®Œå…¨å†»ç»“** |
| **Reward** | å¥–åŠ±æ¨¡å‹ (RM) | è£åˆ¤ã€‚æ ¹æ®å­¦åˆ°çš„äººç±»ä»·å€¼è§‚åå¥½ï¼Œä¸º Actor çš„ç”Ÿæˆç»“æœæ‰“åˆ†ã€‚ | **å®Œå…¨å†»ç»“** |
| **Critic** | ä»·å€¼æ¨¡å‹ (Value) | ä¼šè®¡ã€‚é¢„æµ‹å½“å‰çŠ¶æ€çš„æœŸæœ›å¾—åˆ†ï¼Œè¾…åŠ©è®¡ç®—â€œè¶…é¢å›æŠ¥ï¼ˆAdvantageï¼‰â€ã€‚ | **åŠ¨æ€æ›´æ–°** |

## æ•°æ®é€»è¾‘ï¼šPrompt-only

åœ¨ PPO é˜¶æ®µï¼Œæ•°æ®é›†çš„ä½¿ç”¨é€»è¾‘ä¸ SFT æœ‰æœ¬è´¨åŒºåˆ«ï¼š

- **Instruction (é—®é¢˜) æ˜¯æ ¸å¿ƒ**ï¼šå®ƒæ˜¯äº§ç”Ÿè¡Œä¸ºçš„â€œå¯åŠ¨å™¨ï¼ˆPromptï¼‰â€ã€‚PPO åªéœ€è¦æŒ‡ä»¤æ¥é©±åŠ¨æ¨¡å‹åœ¨çº¿ç”Ÿæˆç­”æ¡ˆã€‚
- **Output (æ ‡ç­¾) æ˜¯å¼±ä¾èµ–**ï¼šåœ¨ PPO å¾ªç¯ä¸­ï¼Œæ¨¡å‹ä¸çœ‹æ•°æ®é›†é‡Œçš„ Output å­¦ä¹ ï¼Œè€Œæ˜¯çœ‹å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰çš„å®æ—¶æ‰“åˆ†ã€‚Output åœ¨æ­¤å¤„å¤šä½œä¸ºæ ¼å¼å…¼å®¹æˆ–è¯„ä¼°å‚è€ƒã€‚
- **æœ¬è´¨è½¬å‘**ï¼šä» SFT çš„â€œèƒŒä¹¦å¼å­¦ä¹ ï¼ˆæ ‡ç­¾é©±åŠ¨ï¼‰â€è½¬å‘äº† PPO çš„â€œå®è·µå¼å­¦ä¹ ï¼ˆåé¦ˆé©±åŠ¨ï¼‰â€ã€‚

## æ·±åº¦è¾¨æï¼šReward vs. Critic

è¿™æ˜¯ç†è§£ PPO ç®—æ³•ç¨³å®šæ€§çš„å…³é”®ï¼š

- **Reward Model (è£åˆ¤)**ï¼šå®šæ€§ã€‚å†³å®šäº†â€œä»€ä¹ˆæ˜¯å¥½â€ã€‚å®ƒè¾“å‡ºçš„æ˜¯**å®é™…å¾—åˆ†**ã€‚
- **Critic Model (ä¼šè®¡)**ï¼šå®šé‡ã€‚å†³å®šäº†â€œå¥½å‡ºå¤šå°‘â€ã€‚å®ƒè¾“å‡ºçš„æ˜¯**æœŸæœ›å¾—åˆ†**ã€‚
- **Advantage (ä¼˜åŠ¿/è¶…é¢å›æŠ¥)**ï¼šç”± `å®é™…å¾—åˆ† - æœŸæœ›å¾—åˆ†` è®¡ç®—å¾—å‡ºã€‚
  - è‹¥ `Advantage > 0`ï¼šè¡¨ç¤ºè¿™æ¬¡è¡¨ç°è¶…å‡ºäº†å¹³å‡æ°´å¹³ï¼Œå¼ºåŒ–è¯¥è¡Œä¸ºã€‚
  - è‹¥ `Advantage < 0`ï¼šè¡¨ç¤ºè¿™æ¬¡è¡¨ç°ä½äºå¹³å‡æ°´å¹³ï¼Œå³ä½¿åˆ†æ•°å¾ˆé«˜ä¹Ÿè¦åæ€ã€‚
  > **æ„ä¹‰**ï¼šCritic æŠ¹å¹³äº†é¢˜ç›®éš¾åº¦å·®å¼‚ï¼Œè®©æ¨¡å‹åªå…³æ³¨çº¯ç²¹çš„æŠ€æœ¯è¿›æ­¥ã€‚

## å…³é”®è®­ç»ƒé…ç½®

| å‚æ•° | å…¸å‹å€¼ | åŸç†è§£è¯» |
| :--- | :--- | :--- |
| `learning_rate` | `1e-6` | æä½å­¦ä¹ ç‡ï¼Œç¡®ä¿å¼ºåŒ–å­¦ä¹ è¿™ç§æä¸ç¨³å®šçš„è¿‡ç¨‹èƒ½å¹³ç¨³æ”¶æ•›ã€‚ |
| `ppo_epochs` | `4` | å¯¹åŒä¸€æ‰¹é‡‡æ ·æ•°æ®é‡å¤å­¦ä¹ çš„æ¬¡æ•°ï¼Œæé«˜æ ·æœ¬åˆ©ç”¨ç‡ã€‚ |
| `ppo_target` | `6.0` | KL ç›®æ ‡å€¼ã€‚åŠ¨æ€è°ƒæ•´æƒ©ç½šå¼ºåº¦ï¼Œç¡®ä¿æ¨¡å‹ä¸è„±ç¦»äººç±»è¯­è¨€åˆ†å¸ƒã€‚ |
| `ppo_buffer_size` | `1` | ç»éªŒå›æ”¾æ± å¤§å°ï¼Œåœ¨èµ„æºå—é™æ—¶æ§åˆ¶å•æ¬¡æ›´æ–°çš„æ•°æ®é‡ã€‚ |

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜ï¼šPPO/RLHF è®­ç»ƒ

### æ–¹å¼ä¸€ï¼šLLaMA Factory

**æ•°æ®æ ¼å¼**ï¼ˆPPO ä»…éœ€ Promptï¼Œä¸éœ€è¦æ ‡å‡†ç­”æ¡ˆï¼‰ï¼š

```json
[
  {"instruction": "è¯·å†™ä¸€é¦–å…³äºç§‹å¤©çš„äº”è¨€ç»å¥ã€‚", "input": ""},
  {"instruction": "å¦‚ä½•ä¼˜åŒ– Python ä¸­çš„å†…å­˜ä½¿ç”¨ï¼Ÿ", "input": ""}
]
```

**è®­ç»ƒé…ç½® YAMLï¼ˆéœ€å…ˆè®­ç»ƒ Reward Modelï¼‰**ï¼š

```yaml
### Step 1: è®­ç»ƒå¥–åŠ±æ¨¡å‹ (reward_model.yaml)
model_name_or_path: Qwen/Qwen2.5-7B
stage: rm                               # å¥–åŠ±æ¨¡å‹è®­ç»ƒ
finetuning_type: lora
dataset: my_preference_data              # åå¥½å¯¹æ•°æ®ï¼ˆåŒ DPO æ ¼å¼ï¼‰
template: qwen
output_dir: saves/qwen2.5-7b/lora/reward
```

```yaml
### Step 2: PPO è®­ç»ƒ (ppo_train.yaml)
model_name_or_path: Qwen/Qwen2.5-7B
stage: ppo                               # å…³é”®ï¼šè®¾ä¸º ppo
do_train: true
finetuning_type: lora
reward_model: saves/qwen2.5-7b/lora/reward   # æŒ‡å‘ RM checkpoint

### PPO è¶…å‚
ppo_epochs: 4                            # æ¯æ‰¹æ•°æ®é‡å¤è®­ç»ƒæ¬¡æ•°
ppo_target: 6.0                          # KL ç›®æ ‡å€¼

### LoRA
lora_rank: 64
lora_target: all

### æ•°æ®ï¼ˆPrompt-onlyï¼‰
dataset: my_ppo_prompts
template: qwen

### è®­ç»ƒ
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-6                    # æä½å­¦ä¹ ç‡ï¼Œé˜²æ­¢ RL å‘æ•£
bf16: true
output_dir: saves/qwen2.5-7b/lora/ppo
```

```python
# å…³é”®æ­¥éª¤ä»£ç ï¼ˆç¤ºæ„ï¼‰
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```

**å¯è§†åŒ–**ï¼šé»˜è®¤è¾“å‡ºè‡³ `output/ppo_metrics`ã€‚å»ºè®®é˜…è¯»é¡ºåºï¼š`summary.json` â†’ `training_curves.png` â†’ `training_metrics.csv`ã€‚

---
## å®šä¹‰ä¸ç›®æ ‡

- **å®šä¹‰**ï¼šæœ¬èŠ‚ä¸»é¢˜ç”¨äºè§£é‡Šè¯¥æ¨¡å—çš„æ ¸å¿ƒæ¦‚å¿µä¸å®ç°æ€è·¯ã€‚
- **ç›®æ ‡**ï¼šå¸®åŠ©è¯»è€…å¿«é€Ÿå»ºç«‹é—®é¢˜æŠ½è±¡ã€æ–¹æ³•è·¯å¾„ä¸å·¥ç¨‹è½åœ°æ–¹å¼ã€‚
## å…³é”®æ­¥éª¤

1. æ˜ç¡®è¾“å…¥/è¾“å‡ºä¸ä»»åŠ¡è¾¹ç•Œã€‚
2. æŒ‰æ¨¡å—ä¸»æµç¨‹æ‰§è¡Œæ ¸å¿ƒç®—æ³•æˆ–ç³»ç»Ÿæ­¥éª¤ã€‚
3. è®°å½•æŒ‡æ ‡å¹¶åšå¯¹æ¯”åˆ†æï¼Œå½¢æˆå¯å¤ç”¨ç»“è®ºã€‚
## å…³é”®æ­¥éª¤ä»£ç ï¼ˆçº¯æ–‡æ¡£ç¤ºä¾‹ï¼‰

```python
# å…³é”®æµç¨‹ç¤ºæ„ï¼ˆä¸å…·ä½“å·¥ç¨‹å®ç°è§£è€¦ï¼‰
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```
