# GRPO (Group Relative Policy Optimization) ç»„å†…ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šåè®­ç»ƒï¼ˆPost-trainingï¼‰ä¹‹å¯¹é½/æ¨ç†å¢å¼ºé˜¶æ®µã€‚
- **ç±»å‹**ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆæ•°å­¦é€»è¾‘æ¨ç†å¢å¼ºï¼‰ã€‚
- **ä½œç”¨**ï¼šç”± DeepSeek æå‡ºï¼Œé€šè¿‡å–æ¶ˆ Critic æ¨¡å‹å¹¶é‡‡ç”¨ç»„å†…ç›¸å¯¹åˆ†æ•°ï¼ˆGroup Relativeï¼‰ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å¼€é”€ï¼Œå¹¶æå‡æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­çš„çˆ†å‘åŠ›ã€‚

## æ ¸å¿ƒæ¶æ„ï¼šåŒ–ç¹ä¸ºç®€

ç›¸æ¯” PPO çš„â€œå››è§’å¹³è¡¡â€ï¼ŒGRPO é‡‡ç”¨äº†æ›´è½»é‡åŒ–çš„â€œä¸‰è§’ç»“æ„â€ï¼š

| è§’è‰² | æ˜¯å¦å­˜åœ¨ | èŒè´£æè¿° | çŠ¶æ€ |
| :--- | :--- | :--- | :--- |
| **Actor** | æ˜¯ | æ ¸å¿ƒä¼˜åŒ–å¯¹è±¡ã€‚è´Ÿè´£æ ¹æ®æŒ‡ä»¤ç”Ÿæˆå›å¤ã€‚ | **åŠ¨æ€æ›´æ–°** |
| **Reference** | æ˜¯ | å†»ç»“çš„åŸå‹ã€‚è®¡ç®— KL æ•£åº¦ï¼Œé˜²æ­¢ç­–ç•¥å´©æºƒã€‚ | **å®Œå…¨å†»ç»“** |
| **Reward** | æ˜¯ | è£åˆ¤ã€‚å¯ä»¥æ˜¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¡¬æ€§è§„åˆ™ï¼ˆå¦‚ç¼–è¯‘å™¨ï¼‰ã€‚ | **å®Œå…¨å†»ç»“** |
| **Critic** | **å¦** | **å–æ¶ˆã€‚** ä¸å†é¢„æµ‹æœŸæœ›å¾—åˆ†ï¼Œç”±ç»„å†…å¹³å‡åˆ†æ›¿ä»£å…¶åŠŸèƒ½ã€‚ | **N/A** |

> **ä¼˜åŠ¿**ï¼šå–æ¶ˆ Critic æ¨¡å‹å¯èŠ‚çœçº¦ 50% çš„æ¨¡å‹æƒé‡æ˜¾å­˜ï¼Œæ”¯æŒæ›´å¤§è§„æ¨¡çš„å¹¶è¡Œé‡‡æ ·ã€‚

## æ ¸å¿ƒé€»è¾‘ï¼šç»„å†…å¯¹æ¯” (Group Relative)

è¿™æ˜¯ GRPO åå­—çš„ç”±æ¥ã€‚å®ƒä¸å†çœ‹â€œå†å²å¹³å‡åˆ†ï¼ˆCriticï¼‰â€ï¼Œè€Œæ˜¯çœ‹â€œåŒä¾ªè¡¨ç°â€ï¼š

1. **ç»„å†…é‡‡æ ·**ï¼šå¯¹äºåŒä¸€ä¸ªé—®é¢˜ï¼ŒActor ä¸€æ¬¡æ€§ç”Ÿæˆä¸€ç»„å›ç­”ï¼ˆé‡‡æ ·æ•°ç”± `num_generations` æ§åˆ¶ï¼Œå¦‚ä¸€ç»„ 8 ä¸ªï¼‰ã€‚
2. **è®¡ç®—ä¼˜åŠ¿ (Advantage)**ï¼š
   - ç®—å‡ºè¿™ç»„å›ç­”çš„å¹³å‡åˆ†ï¼ˆMeanï¼‰å’Œæ ‡å‡†å·®ï¼ˆStdï¼‰ã€‚
   - **Advantage å…¬å¼**ï¼š $A_i = \frac{Reward_i - \mathrm{Mean}(Rewards)}{\mathrm{Std}(Rewards)}$
3. **åŸç†**ï¼šåªè¦ä½ çš„å›ç­”æ¯”åŒç»„çš„å…¶ä»–â€œå…„å¼Ÿâ€å¥½ï¼Œä½ å°±è·å¾—æ­£å‘æ¿€åŠ±ã€‚è¿™ç§æ¨ªå‘å¯¹æ¯”å¤©ç„¶æŠ¹å¹³äº†é¢˜ç›®éš¾åº¦çš„å¹²æ‰°ã€‚

## æ ¸å¿ƒåŸç†ä¸æ•°å­¦å…¬å¼

### 1. ç»„å†…ä¼˜åŠ¿å‡½æ•° (Group Relative Advantage)

è¿™æ˜¯ GRPO çš„æ ¸å¿ƒæ•°å­¦åˆ›æ–°ã€‚å¯¹äºé’ˆå¯¹åŒä¸€ä¸ª Prompt ç”Ÿæˆçš„ä¸€ç»„å›ç­” $\{o_1, o_2, \dots, o_G\}$ï¼Œæ¯ä¸ªå›ç­”çš„ä¼˜åŠ¿ $A_i$ è®¡ç®—å¦‚ä¸‹ï¼š

$$A_i = \frac{r_i - \mathrm{mean}(r_1, r_2, \dots, r_G)}{\mathrm{std}(r_1, r_2, \dots, r_G)}$$

- **$r_i$**ï¼šç¬¬ $i$ ä¸ªå›ç­”è·å¾—çš„æ˜¾å¼å¥–åŠ±åˆ†æ•°ã€‚
- **$\mathrm{mean}$ ä¸ $\mathrm{std}$**ï¼šè¿™ç»„å›ç­”å¥–åŠ±åˆ†çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- **ç›´è§‰ç†è§£**ï¼šè¿™æ˜¯ä¸€ç§**å½’ä¸€åŒ–**æ“ä½œã€‚å®ƒå°†ç»å¯¹åˆ†æ•°è½¬åŒ–ä¸ºäº†â€œåœ¨è¯¥ç»„ä¸­çš„è¡¨ç°æ’åâ€ã€‚

### 2. ç›®æ ‡ä¼˜åŒ–å‡½æ•° (Objective Function)

GRPO æ²¿ç”¨äº† PPO çš„å‰ªåˆ‡ï¼ˆClippedï¼‰æ€æƒ³ï¼Œä½†åœ¨è®¡ç®—æœŸæœ›æ—¶æ˜¯åœ¨ç»„å†…è¿›è¡Œçš„ï¼š

$$J_{GRPO}(\theta) = \mathbb{E} \left[ q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}} \right] \left( \frac{1}{G} \sum_{i=1}^G L_i^{CLIP}(\theta) - \beta D_{KL}(\pi_\theta || \pi_{ref}) \right)$$

- **$\frac{1}{G} \sum$**ï¼šå¯¹æ•´ç»„å›ç­”çš„æŸå¤±è¿›è¡Œå¹³å‡ã€‚
- **KL æ•£åº¦çº¦æŸ**ï¼šåŒæ ·ä¿ç•™äº† KL æƒ©ç½šï¼Œé˜²æ­¢æ¨¡å‹ä¸ºäº†èµ¢å¾—ç»„å†…ç«äº‰è€Œå†™å‡ºä¹±ç ã€‚

### åœºæ™¯åˆ†æï¼šç»„å†…å¯¹æ¯”å¦‚ä½•å¥æ•ˆï¼Ÿ

- **é¢˜ç›®æéš¾æ—¶**ï¼š
    å‡è®¾ç”±äºé¢˜ç›®å¤ªéš¾ï¼Œå…¨ç»„ 8 ä¸ªå›ç­”çš„ç»å¯¹å¾—åˆ†éƒ½å¾ˆä½ï¼ˆå¹³å‡åˆ†ä»… 10 åˆ†ï¼‰ã€‚
  - **A å›ç­”**ï¼šå¾—äº† 12 åˆ†ã€‚è™½ç„¶ç»å¯¹åˆ†ä½ï¼Œä½†åœ¨ç»„å†…æ˜¯â€œä¼˜ç­‰ç”Ÿâ€ï¼Œ $Advantage > 0$ï¼Œæ¨¡å‹ä¼šå­¦ä¹ å¥–åŠ±è¿™ç§è¡Œä¸ºã€‚
- **é¢˜ç›®æç®€å•æ—¶**ï¼š
    å‡è®¾ç”±äºé¢˜ç›®å¤ªæ˜“ï¼Œå…¨ç»„å¹³å‡åˆ†é«˜è¾¾ 95 åˆ†ã€‚
  - **B å›ç­”**ï¼šå¾—äº† 90 åˆ†ã€‚è™½ç„¶ç»å¯¹åˆ†å¾ˆé«˜ï¼Œä½†åœ¨ç»„å†…æ˜¯â€œå·®ç”Ÿâ€ï¼Œ $Advantage < 0$ï¼Œæ¨¡å‹åè€Œä¼šåæ€è¿™ç§è¡Œä¸ºã€‚

> **ç»“è®º**ï¼šGRPO è®©æ¨¡å‹ä¸å†çº ç»“äºåˆ†æ•°çš„â€œç»å¯¹å€¼â€ï¼Œè€Œæ˜¯ä¸“æ³¨äº**â€œå¦‚ä½•åšå¾—æ¯”åŒç±»æ›´å¥½â€**ã€‚

## GRPO vs. PPO æ·±åº¦å¯¹æ¯”

| ç‰¹æ€§ | PPO (ç»å…¸) | GRPO (æ–°å‹) |
| :--- | :--- | :--- |
| **åŸºå‡†æ¥æº** | **çºµå‘å¯¹æ¯”**ï¼šé  Critic ç¥ç»ç½‘ç»œé¢„æµ‹ã€‚ | **æ¨ªå‘å¯¹æ¯”**ï¼šé ç»Ÿè®¡å­¦ç»„å†…å¹³å‡å€¼ã€‚ |
| **æ˜¾å­˜å‹åŠ›** | é«˜ï¼ˆéœ€è¦ç»´æŠ¤å·¨å¤§çš„ Critic ç½‘ç»œï¼‰ã€‚ | ä½ï¼ˆå–æ¶ˆ Criticï¼Œçœæ˜¾å­˜ï¼‰ã€‚ |
| **ç¨³å®šæ€§** | ä¾èµ– Critic çš„æ‹Ÿåˆè´¨é‡ã€‚ | ä¾èµ–ç»„å†…é‡‡æ ·æ•°é‡ (num_generations)ã€‚ |
| **æœ€ä½³åœºæ™¯** | å¯¹è¯å¯¹é½ã€é€šç”¨åå¥½å­¦ä¹ ã€‚ | **é€»è¾‘æ¨ç†ã€æ•°å­¦éš¾é¢˜ã€æ·±åº¦æ€ç´¢ (CoT)**ã€‚ |

## å…³é”®è®­ç»ƒé…ç½®

| å‚æ•° | è„šæœ¬é”®å€¼ | åŸç†è§£è¯» |
| :--- | :--- | :--- |
| `num_generations` | `2` (Demo) / `8~16` (ç”Ÿäº§) | æ¯ç»„é‡‡æ ·ä¸ªæ•°ã€‚è¶Šå¤§ï¼Œç»„å†…ç»Ÿè®¡å‡ºçš„å¹³å‡å€¼è¶Šå‡†ï¼Œè®­ç»ƒè¶Šç¨³ã€‚ |
| `scale_rewards` | `"group"` | å¼€å¯ç»„å†…æ ‡å‡†åŒ–æ¨¡å¼ã€‚è¿™æ˜¯ GRPO çš„æ ¸å¿ƒå¼€å…³ã€‚ |
| `learning_rate` | `5e-7` | æä½çš„å­¦ä¹ ç‡ï¼Œé˜²æ­¢ç­–ç•¥æ¢¯åº¦åœ¨é‡‡æ ·ä¸è¶³æ—¶äº§ç”ŸæŠ–åŠ¨ã€‚ |

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜ï¼šGRPO è®­ç»ƒ

### æ–¹å¼ä¸€ï¼šLLaMA Factory

**æ•°æ®æ ¼å¼**ï¼ˆä¸ PPO ç±»ä¼¼ï¼ŒPrompt-only + å¯éªŒè¯å¥–åŠ±ï¼‰ï¼š

```json
[
  {"instruction": "è®¡ç®— (3 + 5) Ã— 2 = ?", "input": "", "output": "16"},
  {"instruction": "æ±‚è§£æ–¹ç¨‹ 2x + 3 = 11", "input": "", "output": "x = 4"}
]
```

**è®­ç»ƒé…ç½® YAML**ï¼š

```yaml
### GRPO è®­ç»ƒé…ç½®
model_name_or_path: Qwen/Qwen2.5-7B
stage: grpo                             # å…³é”®ï¼šè®¾ä¸º grpoï¼ˆè€Œé ppoï¼‰
do_train: true
finetuning_type: lora

### GRPO ç‰¹æœ‰å‚æ•°
num_generations: 8                      # æ¯é¢˜é‡‡æ · G ä¸ªç­”æ¡ˆï¼ˆæ ¸å¿ƒè¶…å‚ï¼‰
pref_beta: 0.04                         # KL çº¦æŸå¼ºåº¦

### å¥–åŠ±é…ç½®ï¼ˆå¯éªŒè¯å¥–åŠ±ï¼Œæ— éœ€ RMï¼‰
reward_funcs: accuracy,format           # å†…ç½®å¥–åŠ±å‡½æ•°ï¼šå‡†ç¡®ç‡ + æ ¼å¼æ£€æŸ¥

### LoRA
lora_rank: 64
lora_target: all

### æ•°æ®
dataset: my_math_data
template: qwen
cutoff_len: 4096                        # æ¨ç†ä»»åŠ¡éœ€è¦æ›´é•¿ä¸Šä¸‹æ–‡

### è®­ç»ƒ
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
learning_rate: 5.0e-7                   # æä½å­¦ä¹ ç‡ï¼ŒGRPO å¯¹æ¢¯åº¦æ›´æ•æ„Ÿ
num_train_epochs: 1
bf16: true
output_dir: saves/qwen2.5-7b/lora/grpo
```

```bash
llamafactory-cli train grpo_config.yaml
```

> **æ˜¾å­˜ä¼°ç®—**ï¼šGRPO æ— éœ€ Criticï¼Œä½† `num_generations=8` æ„å‘³ç€æ¯æ­¥ç”Ÿæˆ 8 æ¡å›å¤ã€‚7B + LoRA + 8 é‡‡æ · â‰ˆ **40~60GB VRAM**ï¼ˆå»ºè®®å¤šå¡æˆ– ZeRO-3ï¼‰ã€‚

### æ–¹å¼äºŒï¼šTRL åº“ + è‡ªå®šä¹‰å¥–åŠ±

```python
from trl import GRPOTrainer, GRPOConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
import re

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B")

# 2. å®šä¹‰å¯éªŒè¯å¥–åŠ±å‡½æ•°
def accuracy_reward(completions, references, **kwargs):
    """æå–ç­”æ¡ˆå¹¶ä¸æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”"""
    rewards = []
    for completion, ref in zip(completions, references):
        # æå– <answer>...</answer> ä¸­çš„å†…å®¹
        match = re.search(r"<answer>(.*?)</answer>", completion)
        predicted = match.group(1).strip() if match else ""
        rewards.append(1.0 if predicted == ref else 0.0)
    return rewards

def format_reward(completions, **kwargs):
    """æ£€æŸ¥è¾“å‡ºæ ¼å¼æ˜¯å¦åŒ…å« think + answer æ ‡ç­¾"""
    rewards = []
    for completion in completions:
        has_think = "<think>" in completion and "</think>" in completion
        has_answer = "<answer>" in completion and "</answer>" in completion
        rewards.append(1.0 if has_think and has_answer else 0.0)
    return rewards

# 3. GRPO é…ç½®
training_args = GRPOConfig(
    output_dir="saves/grpo",
    num_generations=8,                   # æ¯é¢˜ç”Ÿæˆ G ä¸ªå€™é€‰
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=5e-7,
    bf16=True,
)

# 4. å¯åŠ¨ GRPO è®­ç»ƒ
trainer = GRPOTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    reward_funcs=[accuracy_reward, format_reward],  # å¤šå¥–åŠ±å‡½æ•°ç»„åˆ
)
trainer.train()
```

---

## åŸå§‹è„šæœ¬è¿è¡Œ

```bash
python code/grpo_demo.py
```

**å¯è§†åŒ–**ï¼šé»˜è®¤è¾“å‡ºè‡³ `output/grpo_metrics`ã€‚å…³æ³¨ `reward`ï¼ˆæ€»åˆ†ï¼‰ä¸ `reward_std`ï¼ˆç»„å†…å·®å¼‚ï¼‰çš„å˜åŒ–è¶‹åŠ¿ã€‚
