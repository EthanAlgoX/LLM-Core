# SFT (Supervised Fine-Tuning) 监督微调

> [!TIP]
> **一句话通俗理解**：拿人类写的高质量问答对，手把手教模型"怎么说话"

## 定位与分类

- **阶段**：后训练（Post-training）之对齐起点。
- **类型**：监督学习（Supervised Learning）。
- **作用**：将预训练基座模型（Base Model）转化为能够遵循人类指令（Instruction Following）的对话模型（Chat Model）。它是 RLHF 流程的物理基础。

## 模型训练的关键步骤

SFT 处理流程遵循以下核心步骤：

1. **数据分词 (Tokenization)**：将指令（Instruction）与回答（Output）拼接，并转换为模型可读的 Token IDs。
2. **掩码处理 (Label Masking)**：在计算损失时，通常将指令部分的标签置为 `-100`（忽略），确保模型仅学习如何生成回答，而不去学如何复述指令。
3. **前向传播 (Forward Pass)**：模型根据 Prompt 预测下一个字符（Token）的概率分布。
4. **损失计算 (Loss Calculation)**：使用**交叉熵（Cross-Entropy）**对比预测值与标准答案。
5. **反向传播与优化 (Backprop & Update)**：根据梯度更新模型权重（或 LoRA 权重）。

## 核心原理与损失函数

### 1. 关键公式：交叉熵损失 (Cross-Entropy Loss)

SFT 的本质是**最大似然估计（MLE）**，其核心数学目标是最小化回答序列的负对数似然：

$$L(\theta) = - \sum_{i=1}^{T} \log P_\theta(y_i | x, y_{1}, \dots, y_{i-1})$$

**公式拆解与理解：**

- **$x$ (Input)**：输入的指令内容（Prompt）。
- **$y_i$ (Target)**：标准答案中第 $i$ 个位置的词（Token）。
- **$P_\theta(\dots)$**：模型根据当前参数 $\theta$，在已知指令和前序文字的前提下，预测出正确下一个词的“概率”。
- **$\log$ 与负号**：将概率转化为损失值。概率越大（预测越准）， $\log$ 越接近 0，损失值越小。

### 2. 深度解读：如何直观理解这个过程？

- **逐词对齐 (Token-level Alignment)**：模型在每一个步长上都在尝试预测“下一个词”。它在学习标准答案中词与词之间的统计规律。
- **Teacher Forcing (强制纠偏)**：这是 SFT 的关键特征。在训练前向传播时，无论模型预测出的上一个词是否正确，模型在计算当前词时输入的永远是**真实答案**中的前文。就像老师牵着手写字，错了一笔立即拉回。
- **概率最大化**：公式的终极目的是让模型在看到特定指令时，能够以“最大概率”吐出数据集里的标准字句。

### 3. 与 PPO/GRPO 的本质区别

| 特性 | SFT (监督微调) | RL (PPO/GRPO) |
| :--- | :--- | :--- |
| **学习源** | **静态标签**（Output 字对字模仿）。 | **动态反馈**（Reward 打分驱动）。 |
| **灵活性** | 低。模型被限制在模仿数据集。 | 高。模型可以探索数据集之外更好的解。 |
| **稳定性** | 极高。最简单的梯度下降。 | 低。容易发散，需要复杂的超参控制。 |

## 关键配置解读

| 参数 | 建议值 | 原理解读 |
| :--- | :--- | :--- |
| `learning_rate` | `1e-4` 或 `5e-5` | 相比 RL，SFT 使用较高的学习率以快速学习任务模式。 |
| `cutoff_len` | `1024` | 决定了模型单次能处理的问题+答案的总长度。 |
| `lora_target` | `all` | 为所有线性层添加低秩适配器，可以在提升效果的同时极大节省显存。 |

## 🛠️ 工程实战：使用 LLaMA Factory 进行 SFT

[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory) 是目前最流行的开源微调框架，支持 100+ 模型、LoRA/QLoRA/全量微调、WebUI 可视化训练。

### Step 1: 环境准备

```python
# 关键步骤代码（示意）
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```

---
## 定义与目标

- **定义**：SFT (Supervised Fine-Tuning) 监督微调 属于“后训练对齐模块，聚焦 SFT、偏好优化与 RLHF 系列方法。”范畴。
- **目标**：在能力、可控性与安全性之间建立可迭代的对齐训练闭环。
## 适用场景与边界

- **适用场景**：用于构建指令跟随、偏好对齐与奖励驱动优化流程。
- **不适用场景**：不适用于缺少高质量偏好数据或评测体系的直接落地。
- **使用边界**：对齐收益受数据质量、奖励建模与 KL 约束策略影响明显。

## 关键步骤

1. 构建对齐数据与偏好信号（指令数据/偏好对/奖励模型）。
2. 在约束条件下优化策略，使输出更符合人类偏好。
3. 联合有用性、安全性与稳定性指标进行迭代评估。
## 关键公式（逻辑表达）

`J(theta) = E[r(x, y)] - beta * KL(pi_theta || pi_ref)`

符号说明：
- `r(x, y)`：奖励或偏好评分。
- `beta`：约束强度系数。
- `KL`：策略偏移惩罚项。
## 关键步骤代码（纯文档示例）

```python
# 关键流程示意（与具体工程实现解耦）
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```

## 工程实现要点

- 优先保证数据质量与评测一致性，再放大训练规模。
- 在线/离线对齐需分别监控稳定性、奖励漂移与过优化风险。
- 保持参考模型与训练模型版本可追踪，便于回溯问题。

## 常见错误与排查

- **症状**：奖励升高但人工体验下降。  
  **原因**：奖励黑客或偏好模型偏差导致目标错位。  
  **解决**：引入人工抽检与多指标约束，限制单一奖励驱动。
- **症状**：训练不稳定或发散。  
  **原因**：学习率/KL 系数/批量配置不匹配。  
  **解决**：缩小超参搜索范围并分阶段增大训练强度。

## 与相近方法对比

| 方法 | 优点 | 局限 | 适用场景 |
| --- | --- | --- | --- |
| 本文主题方法 | 紧贴本节问题定义 | 依赖数据与实现质量 | 适合结构化评测与迭代优化 |
| 对比方法A | 上手成本更低 | 能力上限可能受限 | 快速原型与基线对照 |
| 对比方法B | 上限潜力更高 | 调参与资源成本更高 | 高要求生产或复杂任务场景 |

## 参考资料

- [InstructGPT](https://arxiv.org/abs/2203.02155)
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)

