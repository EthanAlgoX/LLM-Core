# DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šç»™ AI ä¸¤ä¸ªç­”æ¡ˆè®©å®ƒé€‰å¥½çš„ï¼Œé€šè¿‡"åå¥½æ‰“åˆ†"é©¯åŒ–å®ƒè¯´äººè¯

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šåè®­ç»ƒï¼ˆPost-trainingï¼‰ä¹‹åå¥½å¯¹é½ã€‚
- **ç±»å‹**ï¼šç›´æ¥åå¥½å­¦ä¹ ï¼ˆDirect Preference Learningï¼‰ã€‚
- **ä½œç”¨**ï¼šå–ä»£å¤æ‚çš„â€œå¥–åŠ±æ¨¡å‹ + PPOâ€æµç¨‹ï¼Œç›´æ¥é€šè¿‡å¯¹æ¯”â€œå¥½å›ç­”â€ä¸â€œåå›ç­”â€ï¼Œå°†äººç±»çš„åå¥½æ³¨å…¥æ¨¡å‹ä¸­ã€‚

## ä»€ä¹ˆæ˜¯ DPOï¼Ÿ

DPOï¼ˆDirect Preference Optimizationï¼‰æ˜¯ç”±æ–¯å¦ç¦å¤§å­¦æå‡ºçš„ä¸€ç§ç®€åŒ–ç‰ˆå¯¹é½ç®—æ³•ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**ä¸å†è®­ç»ƒä¸€ä¸ªè£åˆ¤ï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰ï¼Œè€Œæ˜¯ç›´æ¥è®©æ¨¡å‹åœ¨â€œå¥½åå¯¹â€ä¸­å­¦ä¹ ã€‚**
å®ƒåœ¨æ•°å­¦ä¸Šè¯æ˜äº†ï¼Œé€šè¿‡å¯¹æ•°æ¯”ä¾‹ï¼ˆLog-Ratioï¼‰çš„ä¼˜åŒ–ï¼Œå¯ä»¥è¾¾åˆ°ä¸ä¼ ç»Ÿ RLHF ç›¸åŒçš„å¯¹é½æ•ˆæœï¼Œä½†å·¥ç¨‹å®ç°éš¾åº¦é™ä½äº† 90%ã€‚

## DPO è®­ç»ƒçš„å…³é”®æ­¥éª¤

1. **æ„å»ºåå¥½å¯¹ (Preference Pairs)**ï¼šå‡†å¤‡æ•°æ®ï¼Œæ ¼å¼ä¸º `(Prompt, Chosen_Answer, Rejected_Answer)`ã€‚
2. **åŠ è½½åŒæ¨¡å‹**ï¼š
   - **Policy Model (å¾…è®­æ¨¡å‹)**ï¼šæˆ‘ä»¬è¦ä¼˜åŒ–çš„ Actorã€‚
   - **Reference Model (å‚è€ƒæ¨¡å‹)**ï¼šé€šå¸¸æ˜¯ SFT åçš„å†»ç»“æ¨¡å‹ï¼Œä½œä¸ºåŠ¨æ€åŸºå‡†ã€‚
3. **è®¡ç®— Log-Prob**ï¼šå¾…è®­æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹åˆ†åˆ«å¯¹ Chosen å’Œ Rejected ç­”æ¡ˆè®¡ç®—é¢„æµ‹æ¦‚ç‡çš„å¯¹æ•°ï¼ˆLog-Probabilityï¼‰ã€‚
4. **è®¡ç®—å¯¹æ•°æ¯”ä¾‹å·®è· (Log-Ratio Gap)**ï¼šè®¡ç®—å¾…è®­æ¨¡å‹ç›¸å¯¹äºå‚è€ƒæ¨¡å‹ï¼Œåœ¨ Chosen ä¸Šçš„è¿›æ­¥æ˜¯å¦æ¯”åœ¨ Rejected ä¸Šçš„è¿›æ­¥æ›´å¤§ã€‚
5. **åå¥½æ›´æ–° (Optimization)**ï¼šé€šè¿‡ Sigmoid æ¿€æ´»å‡½æ•°å’Œæ¢¯åº¦ä¸‹é™ï¼Œæ‹‰å¤§å¥½åç­”æ¡ˆä¹‹é—´çš„å·®è·ã€‚

## æ ¸å¿ƒåŸç†ä¸å…³é”®å…¬å¼

### 1. å…³é”®å…¬å¼ï¼šDPO æŸå¤±å‡½æ•°

DPO çš„ä¼Ÿå¤§ä¹‹å¤„åœ¨äºå®ƒè¯æ˜äº†å¯ä»¥ç›´æ¥åˆ©ç”¨åå¥½æ•°æ®ä¼˜åŒ–ç­–ç•¥ï¼Œè€Œä¸éœ€è¦è®­ç»ƒæ˜¾å¼çš„å¥–åŠ±æ¨¡å‹ã€‚å…¶ç›®æ ‡å‡½æ•°ä¸ºï¼š

$$L_{DPO}(\pi_\theta; \pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w | x)}{\pi_{ref}(y_w | x)} - \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{ref}(y_l | x)} \right) \right]$$

**å…¬å¼æ‹†è§£ä¸ç†è§£ï¼š**

- **$\pi_\theta$ ä¸ $\pi_{ref}$**ï¼šå½“å‰ä¼˜åŒ–çš„æ¨¡å‹ä¸å†»ç»“çš„å‚è€ƒæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ SFT åçš„æ¨¡å‹ï¼‰ã€‚
- **$y_w$ (Chosen) ä¸ $y_l$ (Rejected)**ï¼šåå¥½å¯¹ä¸­çš„â€œå¥½ç­”æ¡ˆâ€ä¸â€œåç­”æ¡ˆâ€ã€‚
- **$\log \frac{\pi_\theta}{\pi_{ref}}$ (Log-Ratio)**ï¼šè¡¡é‡å½“å‰æ¨¡å‹ç›¸å¯¹äºå‚è€ƒæ¨¡å‹ï¼Œå¯¹æŸä¸ªå›ç­”æ¦‚ç‡çš„â€œæå‡ç¨‹åº¦â€ã€‚
- **åå¥½è¾¹é™… (Preference Margin)**ï¼šæ‹¬å·å†…çš„ä¸¤é¡¹ç›¸å‡ï¼Œä»£è¡¨äº†æ¨¡å‹å¯¹â€œå¥½ç­”æ¡ˆâ€çš„æå‡ç¨‹åº¦æ˜¯å¦è¿œå¤§äºå¯¹â€œåç­”æ¡ˆâ€çš„æå‡ç¨‹åº¦ã€‚
- **$\beta$ (Beta ç³»æ•°)**ï¼šè°ƒèŠ‚å› å­ã€‚æ§åˆ¶å¯¹åå¥½çš„æ•æ„Ÿåº¦ï¼ŒåŒæ—¶ä¹Ÿèµ·åˆ°äº†ç±»ä¼¼ PPO ä¸­ KL æ•£åº¦çš„çº¦æŸä½œç”¨ï¼Œé˜²æ­¢æ¨¡å‹è·‘å¾—å¤ªåã€‚

### 2. æ·±åº¦è§£è¯»ï¼šä¸ºä»€ä¹ˆå®ƒèƒ½å–ä»£å¥–åŠ±æ¨¡å‹ï¼Ÿ

- **éšå«å¥–åŠ± (Implicit Reward)**ï¼šDPO å‘ç°ï¼Œä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ $\pi$ ä¸å¥–åŠ±å‡½æ•°ä¹‹é—´å­˜åœ¨ä¸€ä¸€æ˜ å°„å…³ç³»ã€‚
- **ç›´æ¥å¯¹æ¯”**ï¼šDPO ä¸å»é—®â€œè¿™ä¸ªå›ç­”å¾—å¤šå°‘åˆ†â€ï¼Œè€Œæ˜¯é—®â€œå½“å‰çš„è¿™ä¸ªæ¨¡å‹ï¼Œæ˜¯ä¸æ˜¯æ¯”åŸå§‹æ¨¡å‹æ›´å–œæ¬¢å¥½å›ç­”ã€æ›´è®¨åŒåå›ç­”â€ã€‚
- **å·¥ç¨‹ç®€åŒ–**ï¼šçœå»äº†è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€åœ¨çº¿é‡‡æ ·ã€ä»¥åŠå¤æ‚çš„ Critic è°ƒä¼˜è¿‡ç¨‹ã€‚

## ä¸ç›¸è¿‘æ–¹æ³•åŒºåˆ«

1. ç›¸æ¯” `SFT`ï¼šDPO å­¦â€œç›¸å¯¹åå¥½â€ï¼Œè€Œä¸æ˜¯â€œç»å¯¹æ ‡å‡†ç­”æ¡ˆâ€ã€‚
2. ç›¸æ¯” `PPO/RLHF`ï¼šDPO ä¸éœ€è¦åœ¨çº¿ rolloutsï¼Œå·¥ç¨‹æ›´ç®€æ´ã€‚
3. ç›¸æ¯” `GRPO`ï¼šDPO å¸¸åŸºäºæˆå¯¹åå¥½æ•°æ®ï¼ŒGRPO å¸¸åŸºäºç»„å†…å¤šé‡‡æ ·å¥–åŠ±æ¯”è¾ƒã€‚

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜ï¼šDPO è®­ç»ƒ

### æ–¹å¼ä¸€ï¼šLLaMA Factoryï¼ˆæ¨èï¼‰

**æ•°æ®æ ¼å¼**ï¼ˆåå¥½å¯¹æ ¼å¼ï¼Œåœ¨ `dataset_info.json` ä¸­æ³¨å†Œï¼‰ï¼š

```json
[
  {
    "instruction": "è§£é‡Šé‡å­çº ç¼ ",
    "input": "",
    "chosen": "é‡å­çº ç¼ æ˜¯ä¸€ç§é‡å­åŠ›å­¦ç°è±¡ï¼Œä¸¤ä¸ªç²’å­çš„çŠ¶æ€ç›¸äº’å…³è”...",
    "rejected": "é‡å­çº ç¼ å°±æ˜¯ä¸¤ä¸ªä¸œè¥¿è¿åœ¨ä¸€èµ·ã€‚"
  }
]
```

**è®­ç»ƒé…ç½® YAML**ï¼š

```yaml
### DPO è®­ç»ƒé…ç½®
model_name_or_path: Qwen/Qwen2.5-7B
stage: dpo                              # å…³é”®ï¼šè®¾ä¸º dpo
do_train: true
finetuning_type: lora

### DPO ç‰¹æœ‰å‚æ•°
pref_beta: 0.1                          # Î² ç³»æ•°ï¼Œæ§åˆ¶åå¥½æ•æ„Ÿåº¦ï¼ˆé»˜è®¤ 0.1ï¼‰
pref_loss: sigmoid                      # æŸå¤±ç±»å‹ï¼šsigmoid / hinge / ipo

### LoRA
lora_rank: 64
lora_target: all

### æ•°æ®
dataset: my_dpo_data                    # åå¥½å¯¹æ•°æ®é›†
template: qwen
cutoff_len: 2048

### è®­ç»ƒ
per_device_train_batch_size: 1          # DPO éœ€è¦åŒæ—¶åŠ è½½ chosen + rejectedï¼Œæ˜¾å­˜ç¿»å€
gradient_accumulation_steps: 16
num_train_epochs: 2.0
learning_rate: 5.0e-6                   # DPO å­¦ä¹ ç‡é€šå¸¸æ¯” SFT ä½ä¸€ä¸ªæ•°é‡çº§
bf16: true
output_dir: saves/qwen2.5-7b/lora/dpo
```

```bash
llamafactory-cli train dpo_config.yaml
```

> **æ˜¾å­˜æ³¨æ„**ï¼šDPO éœ€åŒæ—¶ç»´æŠ¤ Policy + Reference ä¸¤ä¸ªæ¨¡å‹ï¼Œæ˜¾å­˜éœ€æ±‚çº¦ä¸º SFT çš„ **2 å€**ã€‚

### æ–¹å¼äºŒï¼šTRL åº“ï¼ˆHuggingFaceï¼‰

```python
from trl import DPOTrainer, DPOConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B", device_map="auto")
ref_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B", device_map="auto")  # å†»ç»“å‚è€ƒæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B")

# 2. åŠ è½½åå¥½æ•°æ®
dataset = load_dataset("json", data_files="data/dpo_pairs.json")

# 3. è®­ç»ƒé…ç½®
training_args = DPOConfig(
    output_dir="saves/dpo",
    beta=0.1,                           # KL çº¦æŸå¼ºåº¦
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    learning_rate=5e-6,
    num_train_epochs=2,
    bf16=True,
)

# 4. å¯åŠ¨ DPO è®­ç»ƒ
trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=training_args,
    train_dataset=dataset["train"],
    tokenizer=tokenizer,
)
trainer.train()
```

---

## åŸå§‹è„šæœ¬è¿è¡Œ

```bash
cd <YOUR_PROJECT_ROOT>/post_train/alignment/dpo
conda activate finetune
# çº¯æ–‡æ¡£ä»“åº“ï¼šå†å²è„šæœ¬å‘½ä»¤å·²å½’æ¡£
```

## è¾“å‡ºç»“æœ

é»˜è®¤è¾“å‡ºåˆ° `output/dpo_metrics`ï¼ŒåŒ…å«ï¼š

- `training_metrics.csv`
- `training_curves.png`
- `summary.json`
- `log_history.json`
