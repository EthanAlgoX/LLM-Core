# DeepSpeed ä¸“é¢˜

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šè®­ç»ƒå·¥ç¨‹ä¼˜åŒ–ï¼ˆTraining Optimizationï¼‰ã€‚
- **ç±»å‹**ï¼šå¤§è§„æ¨¡æ·±åº¦å­¦ä¹ ç³»ç»Ÿæ¡†æ¶ã€‚
- **ä½œç”¨**ï¼šDeepSpeed æ˜¯å¾®è½¯å¼€å‘çš„é«˜æ€§èƒ½è®­ç»ƒåº“ã€‚å®ƒé€šè¿‡ **ZeRO (Zero Redundancy Optimizer)** ç­‰çªç ´æ€§æŠ€æœ¯ï¼Œæå¤§åœ°é™ä½äº†è®­ç»ƒè¶…å¤§æ¨¡å‹æ‰€éœ€çš„æ˜¾å­˜ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æœ‰é™çš„ç¡¬ä»¶èµ„æºä¸Šè®­ç»ƒå‡ºæ›´å¤§çš„æ¨¡å‹ã€‚

## ä»€ä¹ˆæ˜¯ DeepSpeedï¼Ÿ

DeepSpeed æ˜¯å¤§æ¨¡å‹è®­ç»ƒçš„â€œè¶…çº§å†…å­˜ç®¡ç†å™¨â€ã€‚
åœ¨æ™®é€šçš„åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ¯ä¸ª GPU éƒ½ä¼šå®Œæ•´åœ°ä¿å­˜ä¸€ä»½ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ã€‚å¯¹äºåƒäº¿çº§æ¨¡å‹ï¼Œè¿™ä¼šç¬é—´æ’‘çˆ†æ˜¾å­˜ã€‚DeepSpeed çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**â€œæ—¢ç„¶æ˜¯åˆ†å¸ƒå¼ï¼Œä¸ºä»€ä¹ˆä¸æŠŠè¿™äº›æ•°æ®ä¹Ÿåˆ†å¸ƒå¼€æ¥å­˜å‘¢ï¼Ÿâ€**

## ZeRO ä¼˜åŒ–é˜¶æ®µ (Stages)

1. **ZeRO-1 (Optimizer State Partitioning)**ï¼š
   - å°†ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚ Momentum, Varianceï¼‰åˆ‡åˆ†å¹¶åˆ†å¸ƒåˆ°ä¸åŒ GPU ä¸Šã€‚
2. **ZeRO-2 (Gradient Partitioning)**ï¼š
   - åœ¨ ZeRO-1 çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å°†æ¢¯åº¦åˆ†å¸ƒå­˜å‚¨ã€‚
3. **ZeRO-3 (Parameter Partitioning)**ï¼š
   - åœ¨ ZeRO-2 çš„åŸºç¡€ä¸Šï¼Œå°†æ¨¡å‹å‚æ•°æœ¬èº«ä¹Ÿåˆ‡åˆ†åˆ†å¸ƒã€‚è¿™æ„å‘³ç€æ¯å¼ å¡åªå­˜æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œéœ€è¦æ—¶å†ä¸´æ—¶æ‹‰å–ã€‚

## å…³é”®é›†æˆæ­¥éª¤

1. **é…ç½® JSON ç¼–å†™**ï¼š
   - å®šä¹‰ `zero_optimization` çº§åˆ«ã€æ··åˆç²¾åº¦ (fp16/bf16)ã€æ¢¯åº¦ç´¯åŠ æ­¥æ•°ç­‰ã€‚
2. **Engine åˆå§‹åŒ–**ï¼š
   - è°ƒç”¨ `deepspeed.initialize`ï¼Œå°†æ™®é€šçš„ PyTorch Model å’Œ Optimizer åŒ…è£…æˆä¸€ä¸ª `DeepSpeedEngine`ã€‚
3. **è®­ç»ƒé€»è¾‘é‡æ„**ï¼š
   - ä½¿ç”¨ `engine.backward(loss)` ä»£æ›¿ `loss.backward()`ã€‚
   - ä½¿ç”¨ `engine.step()` è‡ªåŠ¨å¤„ç†å‚æ•°æ›´æ–°ã€æ¢¯åº¦æ¸…é›¶å’Œæ¢¯åº¦ç´¯åŠ ã€‚

## æ ¸å¿ƒæ•°å­¦æ”¶ç›Š

### æ˜¾å­˜å‹ç¼©æ¯”

$$Memory_{ZeRO3} \approx \frac{Memory_{Baseline}}{N}$$

- å…¶ä¸­ $N$ ä¸ºå¹¶è¡Œçš„ GPU æ•°é‡ã€‚ç†è®ºä¸Šï¼ŒZeRO-3 å¯ä»¥å°†æ˜¾å­˜å ç”¨é™ä½è‡³åŸå…ˆçš„ $1/N$ã€‚

## ä¸ç›¸è¿‘æ–¹æ³•åŒºåˆ«

1. ç›¸æ¯” `Megatron`ï¼šDeepSpeed ä¾§é‡ç³»ç»Ÿä¼˜åŒ–ä¸ ZeROï¼›Megatronå¼ºè°ƒæ¨¡å‹å¹¶è¡Œåˆ‡åˆ†ã€‚
2. ç›¸æ¯” `CUDA`ï¼šCUDA æ˜¯åº•å±‚ç¡¬ä»¶ä¸ç®—å­ï¼›DeepSpeed æ˜¯è®­ç»ƒç³»ç»Ÿå±‚ã€‚
3. ç›¸æ¯” `mixed_precision`ï¼šæ··åˆç²¾åº¦æ˜¯æŠ€æœ¯ç‚¹ï¼ŒDeepSpeed æ˜¯æ•´ä½“è®­ç»ƒæ¡†æ¶ã€‚

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜

### Step 1: ZeRO é…ç½®æ–‡ä»¶

```json
{
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "overlap_comm": true,
    "contiguous_gradients": true
  },
  "bf16": {
    "enabled": true
  },
  "gradient_accumulation_steps": 8,
  "gradient_clipping": 1.0,
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "wall_clock_breakdown": false
}
```

**ZeRO Stage é€‰æ‹©æŒ‡å—**ï¼š

| Stage | åˆ‡åˆ†å†…å®¹ | å•å¡å¯è®­è§„æ¨¡ (8Ã—A100-80G) | é€šä¿¡å¼€é”€ |
| --- | --- | --- | --- |
| ZeRO-1 | ä¼˜åŒ–å™¨çŠ¶æ€ | ~30B | ä½ |
| ZeRO-2 | + æ¢¯åº¦ | ~60B | ä¸­ |
| ZeRO-3 | + å‚æ•° | ~100B+ | é«˜ |

### Step 2: PyTorch é›†æˆä»£ç 

```python
import deepspeed
import torch
from transformers import AutoModelForCausalLM

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B")
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

# 2. DeepSpeed å¼•æ“åˆå§‹åŒ–
model_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    optimizer=optimizer,
    config="ds_config.json",             # æŒ‡å‘ä¸Šé¢çš„ JSON
)

# 3. è®­ç»ƒå¾ªç¯ï¼ˆæ›¿æ¢åŸç”Ÿ PyTorchï¼‰
for batch in dataloader:
    inputs = batch["input_ids"].to(model_engine.device)
    labels = batch["labels"].to(model_engine.device)

    outputs = model_engine(input_ids=inputs, labels=labels)
    loss = outputs.loss

    model_engine.backward(loss)          # æ›¿ä»£ loss.backward()
    model_engine.step()                  # æ›¿ä»£ optimizer.step() + zero_grad()
```

### Step 3: å¯åŠ¨å‘½ä»¤

```bash
# å•æœºå¤šå¡
deepspeed --num_gpus=4 train.py --deepspeed ds_config.json

# å¤šæœºå¤šå¡ï¼ˆhostfile æ–¹å¼ï¼‰
deepspeed --hostfile=hostfile.txt --num_gpus=8 train.py --deepspeed ds_config.json
```

`hostfile.txt` æ ¼å¼ï¼š

```text
node1 slots=8
node2 slots=8
```

### ä¸ LLaMA Factory / HuggingFace Trainer é›†æˆ

```yaml
# åœ¨ LLaMA Factory YAML ä¸­å¯ç”¨ DeepSpeed
deepspeed: ds_config.json               # è‡ªåŠ¨å–ä»£é»˜è®¤åˆ†å¸ƒå¼åç«¯
```

```python
# åœ¨ HuggingFace TrainingArguments ä¸­å¯ç”¨
from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="saves/model",
    deepspeed="ds_config.json",          # ä¸€è¡Œå³å¯
    bf16=True,
    per_device_train_batch_size=2,
)
```

---

## åŸå§‹è„šæœ¬è¿è¡Œ

```bash
cd <YOUR_PROJECT_ROOT>/post_train/systems/deepspeed
conda activate finetune
python code/deepspeed.py
```

## è¾“å‡ºç»“æœ

é»˜è®¤è¾“å‡ºåˆ° `output/deepspeed_metrics`ï¼ŒåŒ…å«ï¼š

- `training_metrics.csv`
- `training_curves.png`
- `summary.json`
- `deepspeed_config_auto.json`
