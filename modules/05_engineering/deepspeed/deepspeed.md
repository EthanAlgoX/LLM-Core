# DeepSpeed ä¸“é¢˜

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šZeRO æŠŠä¼˜åŒ–å™¨çŠ¶æ€ç­‰åˆ†æ•£åˆ°æ¯å¼ å¡ï¼Œäººäººå„æ‹¿ä¸€ä»½ä¸é‡å¤ï¼Œçœæ‰å†—ä½™æ˜¾å­˜

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šè®­ç»ƒå·¥ç¨‹ä¼˜åŒ–ï¼ˆTraining Optimizationï¼‰ã€‚
- **ç±»å‹**ï¼šå¤§è§„æ¨¡æ·±åº¦å­¦ä¹ ç³»ç»Ÿæ¡†æ¶ã€‚
- **ä½œç”¨**ï¼šDeepSpeed æ˜¯å¾®è½¯å¼€å‘çš„é«˜æ€§èƒ½è®­ç»ƒåº“ã€‚å®ƒé€šè¿‡ **ZeRO (Zero Redundancy Optimizer)** ç­‰çªç ´æ€§æŠ€æœ¯ï¼Œæå¤§åœ°é™ä½äº†è®­ç»ƒè¶…å¤§æ¨¡å‹æ‰€éœ€çš„æ˜¾å­˜ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æœ‰é™çš„ç¡¬ä»¶èµ„æºä¸Šè®­ç»ƒå‡ºæ›´å¤§çš„æ¨¡å‹ã€‚

## å®šä¹‰ä¸ç›®æ ‡

DeepSpeed æ˜¯å¤§æ¨¡å‹è®­ç»ƒçš„â€œè¶…çº§å†…å­˜ç®¡ç†å™¨â€ã€‚
åœ¨æ™®é€šçš„åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ¯ä¸ª GPU éƒ½ä¼šå®Œæ•´åœ°ä¿å­˜ä¸€ä»½ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ã€‚å¯¹äºåƒäº¿çº§æ¨¡å‹ï¼Œè¿™ä¼šç¬é—´æ’‘çˆ†æ˜¾å­˜ã€‚DeepSpeed çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**â€œæ—¢ç„¶æ˜¯åˆ†å¸ƒå¼ï¼Œä¸ºä»€ä¹ˆä¸æŠŠè¿™äº›æ•°æ®ä¹Ÿåˆ†å¸ƒå¼€æ¥å­˜å‘¢ï¼Ÿâ€**

## ZeRO ä¼˜åŒ–é˜¶æ®µ (Stages)

1. **ZeRO-1 (Optimizer State Partitioning)**ï¼š
   - å°†ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚ Momentum, Varianceï¼‰åˆ‡åˆ†å¹¶åˆ†å¸ƒåˆ°ä¸åŒ GPU ä¸Šã€‚
2. **ZeRO-2 (Gradient Partitioning)**ï¼š
   - åœ¨ ZeRO-1 çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å°†æ¢¯åº¦åˆ†å¸ƒå­˜å‚¨ã€‚
3. **ZeRO-3 (Parameter Partitioning)**ï¼š
   - åœ¨ ZeRO-2 çš„åŸºç¡€ä¸Šï¼Œå°†æ¨¡å‹å‚æ•°æœ¬èº«ä¹Ÿåˆ‡åˆ†åˆ†å¸ƒã€‚è¿™æ„å‘³ç€æ¯å¼ å¡åªå­˜æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œéœ€è¦æ—¶å†ä¸´æ—¶æ‹‰å–ã€‚

## é€‚ç”¨åœºæ™¯ä¸è¾¹ç•Œ

- **é€‚ç”¨åœºæ™¯**ï¼šç”¨äºåˆ†å¸ƒå¼è®­ç»ƒã€æ¨ç†åŠ é€Ÿä¸ç³»ç»Ÿç“¶é¢ˆå®šä½ã€‚
- **ä¸é€‚ç”¨åœºæ™¯**ï¼šä¸é€‚ç”¨äºç¼ºå°‘æ€§èƒ½è§‚æµ‹æŒ‡æ ‡çš„â€œç›²è°ƒâ€ä¼˜åŒ–ã€‚
- **ä½¿ç”¨è¾¹ç•Œ**ï¼šä¼˜åŒ–ç»“è®ºå—ç¡¬ä»¶æ‹“æ‰‘ã€å¹¶è¡Œç­–ç•¥ä¸è¯·æ±‚åˆ†å¸ƒå½±å“ã€‚

## å…³é”®æ­¥éª¤

1. **é…ç½® JSON ç¼–å†™**ï¼š
   - å®šä¹‰ `zero_optimization` çº§åˆ«ã€æ··åˆç²¾åº¦ (fp16/bf16)ã€æ¢¯åº¦ç´¯åŠ æ­¥æ•°ç­‰ã€‚
2. **Engine åˆå§‹åŒ–**ï¼š
   - è°ƒç”¨ `deepspeed.initialize`ï¼Œå°†æ™®é€šçš„ PyTorch Model å’Œ Optimizer åŒ…è£…æˆä¸€ä¸ª `DeepSpeedEngine`ã€‚
3. **è®­ç»ƒé€»è¾‘é‡æ„**ï¼š
   - ä½¿ç”¨ `engine.backward(loss)` ä»£æ›¿ `loss.backward()`ã€‚
   - ä½¿ç”¨ `engine.step()` è‡ªåŠ¨å¤„ç†å‚æ•°æ›´æ–°ã€æ¢¯åº¦æ¸…é›¶å’Œæ¢¯åº¦ç´¯åŠ ã€‚

## æ ¸å¿ƒæ•°å­¦æ”¶ç›Š

### æ˜¾å­˜å‹ç¼©æ¯”

$$Memory_{ZeRO3} \approx \frac{Memory_{Baseline}}{N}$$

- å…¶ä¸­ $N$ ä¸ºå¹¶è¡Œçš„ GPU æ•°é‡ã€‚ç†è®ºä¸Šï¼ŒZeRO-3 å¯ä»¥å°†æ˜¾å­˜å ç”¨é™ä½è‡³åŸå…ˆçš„ $1/N$ã€‚

## ä¸ç›¸è¿‘æ–¹æ³•åŒºåˆ«

1. ç›¸æ¯” `Megatron`ï¼šDeepSpeed ä¾§é‡ç³»ç»Ÿä¼˜åŒ–ä¸ ZeROï¼›Megatronå¼ºè°ƒæ¨¡å‹å¹¶è¡Œåˆ‡åˆ†ã€‚
2. ç›¸æ¯” `CUDA`ï¼šCUDA æ˜¯åº•å±‚ç¡¬ä»¶ä¸ç®—å­ï¼›DeepSpeed æ˜¯è®­ç»ƒç³»ç»Ÿå±‚ã€‚
3. ç›¸æ¯” `mixed_precision`ï¼šæ··åˆç²¾åº¦æ˜¯æŠ€æœ¯ç‚¹ï¼ŒDeepSpeed æ˜¯æ•´ä½“è®­ç»ƒæ¡†æ¶ã€‚

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜

### Step 1: ZeRO é…ç½®æ–‡ä»¶

```json
{
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "overlap_comm": true,
    "contiguous_gradients": true
  },
  "bf16": {
    "enabled": true
  },
  "gradient_accumulation_steps": 8,
  "gradient_clipping": 1.0,
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "wall_clock_breakdown": false
}
```

**ZeRO Stage é€‰æ‹©æŒ‡å—**ï¼š

| Stage | åˆ‡åˆ†å†…å®¹ | å•å¡å¯è®­è§„æ¨¡ (8Ã—A100-80G) | é€šä¿¡å¼€é”€ |
| --- | --- | --- | --- |
| ZeRO-1 | ä¼˜åŒ–å™¨çŠ¶æ€ | ~30B | ä½ |
| ZeRO-2 | + æ¢¯åº¦ | ~60B | ä¸­ |
| ZeRO-3 | + å‚æ•° | ~100B+ | é«˜ |

### Step 2: PyTorch é›†æˆä»£ç 

```python
import deepspeed
import torch
from transformers import AutoModelForCausalLM

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B")
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

# 2. DeepSpeed å¼•æ“åˆå§‹åŒ–
model_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    optimizer=optimizer,
    config="ds_config.json",             # æŒ‡å‘ä¸Šé¢çš„ JSON
)

# 3. è®­ç»ƒå¾ªç¯ï¼ˆæ›¿æ¢åŸç”Ÿ PyTorchï¼‰
for batch in dataloader:
    inputs = batch["input_ids"].to(model_engine.device)
    labels = batch["labels"].to(model_engine.device)

    outputs = model_engine(input_ids=inputs, labels=labels)
    loss = outputs.loss

    model_engine.backward(loss)          # æ›¿ä»£ loss.backward()
    model_engine.step()                  # æ›¿ä»£ optimizer.step() + zero_grad()
```

### Step 3: å¯åŠ¨å‘½ä»¤

```python
# å…³é”®æ­¥éª¤ä»£ç ï¼ˆç¤ºæ„ï¼‰
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```

## è¾“å‡ºç»“æœ

é»˜è®¤è¾“å‡ºåˆ° `output/deepspeed_metrics`ï¼ŒåŒ…å«ï¼š

- `training_metrics.csv`
- `training_curves.png`
- `summary.json`
- `deepspeed_config_auto.json`

---
## å…³é”®å…¬å¼ï¼ˆé€»è¾‘è¡¨è¾¾ï¼‰

`GlobalBatch = micro_batch * grad_accum * data_parallel`

ç¬¦å·è¯´æ˜ï¼š
- `micro_batch`ï¼šå•å¡æ¯æ­¥æ ·æœ¬æ•°ã€‚
- `grad_accum`ï¼šæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚
- `data_parallel`ï¼šæ•°æ®å¹¶è¡Œå‰¯æœ¬æ•°ã€‚
## å…³é”®æ­¥éª¤ä»£ç ï¼ˆçº¯æ–‡æ¡£ç¤ºä¾‹ï¼‰

```python
# å…³é”®æµç¨‹ç¤ºæ„ï¼ˆä¸å…·ä½“å·¥ç¨‹å®ç°è§£è€¦ï¼‰
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```

## å·¥ç¨‹å®ç°è¦ç‚¹

- å…ˆå»ºç«‹åŸºå‡†ï¼ˆTTFT/åå/æ˜¾å­˜ï¼‰ï¼Œå†åšåˆ†é¡¹ä¼˜åŒ–ã€‚
- å¹¶è¡Œç­–ç•¥ã€ç²¾åº¦ç­–ç•¥ä¸ç®—å­ä¼˜åŒ–è¦ååŒè¯„ä¼°ã€‚
- ä¿ç•™å‹æµ‹è„šæœ¬ä¸é…ç½®å¿«ç…§ï¼Œç¡®ä¿ä¼˜åŒ–å¯å¤éªŒã€‚

## å¸¸è§é”™è¯¯ä¸æ’æŸ¥

- **ç—‡çŠ¶**ï¼šååæå‡ä½†å»¶è¿Ÿæ¶åŒ–ã€‚  
  **åŸå› **ï¼šæ‰¹å¤„ç†ç­–ç•¥åå‘ååï¼Œç‰ºç‰²äº†å•è¯·æ±‚æ—¶å»¶ã€‚  
  **è§£å†³**ï¼šæŒ‰ä¸šåŠ¡ç›®æ ‡æ‹†åˆ†å»¶è¿Ÿ/ååæ¡£ä½å¹¶åˆ†åˆ«è°ƒå‚ã€‚
- **ç—‡çŠ¶**ï¼šå¤šæœºè®­ç»ƒæ•ˆç‡ä½ã€‚  
  **åŸå› **ï¼šé€šä¿¡å¼€é”€æˆ–å¹¶è¡Œåˆ’åˆ†ä¸ç¡¬ä»¶æ‹“æ‰‘ä¸åŒ¹é…ã€‚  
  **è§£å†³**ï¼šé‡æ’å¹¶è¡Œç»´åº¦å¹¶ç”¨ profiler å®šä½é€šä¿¡çƒ­ç‚¹ã€‚

## å‚è€ƒèµ„æ–™

- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
- [DeepSpeed](https://www.deepspeed.ai/)

