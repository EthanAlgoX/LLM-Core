# æ··åˆç²¾åº¦è®­ç»ƒï¼ˆMixed Precisionï¼‰

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šç”¨ä½ç²¾åº¦æµ®ç‚¹æ•°ä»£æ›¿ FP32ï¼Œæ˜¾å­˜çœä¸€åŠï¼Œé€Ÿåº¦ç¿»å€ï¼Œæ•ˆæœå‡ ä¹ä¸å˜

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šè®­ç»ƒå·¥ç¨‹ä¼˜åŒ–ï¼ˆTraining Optimizationï¼‰ã€‚
- **ç±»å‹**ï¼šæ··åˆç²¾åº¦ï¼ˆMixed Precisionï¼‰æ•°å€¼è®¡ç®—ã€‚
- **ä½œç”¨**ï¼šæ··åˆç²¾åº¦è®­ç»ƒæ—¨åœ¨é€šè¿‡ç»“åˆ FP16ï¼ˆåŠç²¾åº¦ï¼‰å’Œ FP32ï¼ˆå•ç²¾åº¦ï¼‰æ¥æ˜¾è‘—æé«˜æ¨¡å‹è®­ç»ƒé€Ÿåº¦å¹¶é™ä½æ˜¾å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒä¸å…¨ç²¾åº¦è®­ç»ƒç›¸å½“çš„æ”¶æ•›ç²¾åº¦ã€‚

## ä»€ä¹ˆæ˜¯æ··åˆç²¾åº¦ï¼Ÿ

æ··åˆç²¾åº¦æ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒçš„â€œå¹³è¡¡æœ¯â€ã€‚

- **FP32 (Single Precision)**ï¼šç²¾åº¦é«˜ï¼ŒèŒƒå›´å¹¿ï¼Œä½†å ç”¨å†…å­˜å¤šä¸”è®¡ç®—æ…¢ã€‚
- **FP16/BF16 (Half Precision)**ï¼šç²¾åº¦è¾ƒä½ï¼ŒèŒƒå›´çª„ï¼Œä½†è®¡ç®—å¿«ä¸”çœå†…å­˜ã€‚
**ç­–ç•¥**ï¼šåœ¨è®¡ç®—å¯†é›†å‹ä¸”å¯¹ç²¾åº¦ä¸æ•æ„Ÿçš„æ“ä½œï¼ˆå¦‚çŸ©é˜µä¹˜æ³•ï¼‰ä¸­ä½¿ç”¨ FP16/BF16ï¼Œè€Œåœ¨æ•°å€¼èŒƒå›´æ•æ„Ÿçš„ç´¯è®¡æ“ä½œï¼ˆå¦‚æƒé‡æ›´æ–°ï¼‰ä¸­ä¿ç•™ FP32ã€‚

## å…³é”®æ­¥éª¤

1. **ç»´æŠ¤ FP32 æƒé‡å‰¯æœ¬ (Master Weights)**ï¼š
   - åœ¨å†…å­˜/æ˜¾å­˜ä¸­ä¿ç•™ä¸€ä»½ FP32 çš„æƒé‡å‰¯æœ¬ï¼Œç”¨äºåœ¨æ›´æ–°æ—¶ä¿æŒç²¾åº¦ã€‚
2. **å‰å‘ä¸åå‘è®¡ç®— (FP16/BF16)**ï¼š
   - å°†æƒé‡è½¬æ¢ä¸º FP16ï¼Œæ‰§è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ã€‚
3. **æŸå¤±ç¼©æ”¾ (Loss Scaling - é’ˆå¯¹ FP16)**ï¼š
   - ä¸ºäº†é˜²æ­¢ FP16 å› è¡¨ç¤ºèŒƒå›´è¿‡çª„å¯¼è‡´æ¢¯åº¦ä¸‹æº¢ï¼ˆå˜ä¸º 0ï¼‰ï¼Œåœ¨è®¡ç®— Loss åå…ˆä¹˜ä¸€ä¸ªå¾ˆå¤§çš„ç¼©æ”¾å› å­ $S$ã€‚
4. **æ¢¯åº¦æ›´æ–° (FP32)**ï¼š
   - å¾—åˆ° FP16 æ¢¯åº¦åï¼Œå°†å…¶è¿˜åŸï¼ˆUnscaleï¼‰å¹¶è½¬æ¢å› FP32ï¼Œç„¶åä½œç”¨äº Master Weightsã€‚

## å…³é”®å…¬å¼

### 1. æŸå¤±ç¼©æ”¾å…¬å¼

$$\mathrm{Scaled\_Loss} = \mathrm{Loss} \times S$$

$$\mathrm{Update\_Gradient} = \frac{\nabla_{\theta_{FP16}} (\mathrm{Scaled\_Loss})}{S}$$

- é€šè¿‡ $S$ å°†å¾®å°çš„æ¢¯åº¦â€œé¡¶â€å› FP16 çš„è¡¨ç¤ºåŒºé—´å†…ã€‚

### 2. BF16 vs FP16

- **FP16**ï¼š5 ä½æŒ‡æ•°ï¼Œ10 ä½å°¾æ•°ã€‚èŒƒå›´çª„ï¼Œå¿…é¡»é…åˆ **Loss Scaling**ã€‚
- **BF16**ï¼š8 ä½æŒ‡æ•°ï¼Œ7 ä½å°¾æ•°ã€‚èŒƒå›´ä¸ FP32 ä¸€è‡´ï¼Œç²¾åº¦ç•¥ä½ã€‚ç”±äºå…¶èŒƒå›´ä¼˜åŠ¿ï¼Œé€šå¸¸**ä¸éœ€è¦** Loss Scalingï¼Œæ˜¯å¤§æ¨¡å‹è®­ç»ƒçš„é¦–é€‰ã€‚

## ä¸ç›¸è¿‘æ–¹æ³•åŒºåˆ«

1. ç›¸æ¯” `CUDA`ï¼šæ··åˆç²¾åº¦æ˜¯æ•°å€¼ç­–ç•¥ï¼Œä¸æ˜¯ç¡¬ä»¶ API æœ¬èº«ã€‚
2. ç›¸æ¯” `DeepSpeed`ï¼šæ··åˆç²¾åº¦æ˜¯å±€éƒ¨æŠ€æœ¯ç‚¹ï¼Œå¯è¢« DeepSpeed é›†æˆã€‚
3. ç›¸æ¯”ç®—æ³•æ¨¡å—ï¼šä¸æ”¹å˜ç›®æ ‡å‡½æ•°ï¼Œä»…æ”¹å˜è®¡ç®—æ–¹å¼ã€‚

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜

### PyTorch AMPï¼ˆè‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰

```python
import torch
from torch.cuda.amp import autocast, GradScaler

model = MyModel().cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
scaler = GradScaler()  # FP16 ä¸“ç”¨çš„ Loss Scaler

for batch in dataloader:
    inputs, labels = batch["input_ids"].cuda(), batch["labels"].cuda()

    # è‡ªåŠ¨å°†éƒ¨åˆ†è®¡ç®—è½¬ä¸º FP16
    with autocast(dtype=torch.float16):
        outputs = model(inputs, labels=labels)
        loss = outputs.loss

    # Loss Scaling + åå‘ä¼ æ’­
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
```

### BF16 è®­ç»ƒï¼ˆA100/H100 æ¨èï¼‰

```python
# BF16 ä¸éœ€è¦ GradScalerï¼ˆèŒƒå›´ä¸ FP32 ä¸€è‡´ï¼‰
with autocast(dtype=torch.bfloat16):
    outputs = model(inputs, labels=labels)
    loss = outputs.loss

loss.backward()
optimizer.step()
optimizer.zero_grad()
```

### HuggingFace Trainer ä¸­å¯ç”¨

```python
from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="saves/model",
    bf16=True,                  # å¯ç”¨ BF16ï¼ˆA100+ï¼‰
    # fp16=True,                # æˆ–å¯ç”¨ FP16ï¼ˆV100/T4ï¼‰
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
)
```

### ç²¾åº¦å¯¹æ¯”åŸºå‡†æµ‹è¯•

```python
import torch
import time

def benchmark_precision(dtype, n=1000):
    """å¯¹æ¯”ä¸åŒç²¾åº¦çš„çŸ©é˜µä¹˜æ³•æ€§èƒ½"""
    a = torch.randn(4096, 4096, device="cuda", dtype=dtype)
    b = torch.randn(4096, 4096, device="cuda", dtype=dtype)

    # é¢„çƒ­
    for _ in range(10):
        _ = torch.mm(a, b)
    torch.cuda.synchronize()

    start = time.time()
    for _ in range(n):
        _ = torch.mm(a, b)
    torch.cuda.synchronize()

    elapsed = time.time() - start
    print(f"{dtype}: {elapsed:.3f}s ({n/elapsed:.0f} ops/s)")

benchmark_precision(torch.float32)   # FP32 åŸºå‡†
benchmark_precision(torch.float16)   # FP16ï¼ˆV100 çº¦ 2x æé€Ÿï¼‰
benchmark_precision(torch.bfloat16)  # BF16ï¼ˆA100 çº¦ 2x æé€Ÿï¼‰
```

---

## å…³é”®æ­¥éª¤ä»£ç ï¼ˆçº¯æ–‡æ¡£ç¤ºä¾‹ï¼‰

```python
# å…³é”®æ­¥éª¤ä»£ç ï¼ˆç¤ºæ„ï¼‰
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```

---
## å®šä¹‰ä¸ç›®æ ‡

- **å®šä¹‰**ï¼šæœ¬èŠ‚ä¸»é¢˜ç”¨äºè§£é‡Šè¯¥æ¨¡å—çš„æ ¸å¿ƒæ¦‚å¿µä¸å®ç°æ€è·¯ã€‚
- **ç›®æ ‡**ï¼šå¸®åŠ©è¯»è€…å¿«é€Ÿå»ºç«‹é—®é¢˜æŠ½è±¡ã€æ–¹æ³•è·¯å¾„ä¸å·¥ç¨‹è½åœ°æ–¹å¼ã€‚
