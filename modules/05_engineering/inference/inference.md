# æ¨ç†ä¼˜åŒ– (Inference Optimization)

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šè®©ç”¨æˆ·æé—®å’Œ AI åå­—åˆ†å¼€è·‘ï¼Œæ˜¾å­˜æ›´çœã€ååæ›´é«˜

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šæ¨ç†ä¸éƒ¨ç½² (Deployment)ã€‚
- **ç±»å‹**ï¼šååä¸å»¶è¿Ÿä¼˜åŒ–ã€‚
- **ä½œç”¨**ï¼šéšç€ LLM å‚æ•°é‡ and ç”¨æˆ·å¹¶å‘çš„å¢åŠ ï¼Œå¦‚ä½•é€šè¿‡å·¥ç¨‹æ‰‹æ®µé™ä½é¦–å­—å»¶è¿Ÿ (TTFT) and æå‡ç”Ÿæˆååæ˜¯æŠ€æœ¯è§£ææ ¸å¿ƒã€‚

## æ ¸å¿ƒæ¦‚å¿µï¼šKV Cache

### ç—›ç‚¹

è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œæ¯ä¸ªæ–° Token éƒ½è¦é‡æ–°è®¡ç®—ä¹‹å‰æ‰€æœ‰ Token çš„è¡¨ç¤ºï¼Œå¯¼è‡´é‡å¤è®¡ç®—ã€‚

### ç­–ç•¥

å°†å‰å‘è®¡ç®—ä¸­çš„ Key å’Œ Value ç¼“å­˜ä¸‹æ¥ï¼Œåç»­ Token ä»…éœ€ä¸ç¼“å­˜è¿›è¡Œè®¡ç®—ã€‚

- **æ˜¾å­˜å ç”¨å…¬å¼**ï¼š
  $$\mathrm{Mem}_{KV} = 2 \times \mathrm{layers} \times \mathrm{heads} \times \mathrm{hidden\_dim} \times \mathrm{seq\_len} \times \mathrm{precision\_bytes}$$

- **æŠ€æœ¯è§£æç‚¹**ï¼šLlama 7B (fp16) å¤„ç† 1024 é•¿åº¦çº¦å ç”¨ 0.5GB æ˜¾å­˜ã€‚

## æ¨ç†æ¡†æ¶ (Inference Frameworks)

| æ¡†æ¶ | æ ¸å¿ƒæŠ€æœ¯ç‚¹ | åœºæ™¯ä¼˜åŠ¿ |
| :--- | :--- | :--- |
| **vLLM** | **PagedAttention**ï¼šè§£å†³ KV Cache æ˜¾å­˜ç¢ç‰‡åŒ–ï¼Œæå‡ Batch åå 2-10xã€‚ | é«˜å¹¶å‘ã€äº‘ç«¯ç”Ÿäº§ç¯å¢ƒã€‚ |
| **sglang** | **RadixAttention**ï¼šå‰ç¼€ç¼“å­˜ä¸ç«¯åˆ°ç«¯ç¼–è¯‘ä¼˜åŒ–ã€‚ | å¤æ‚æŒ‡ä»¤æµã€é•¿å¯¹è¯ç¼“å­˜å…±äº«åœºæ™¯ã€‚ |
| **TensorRT-LLM** | æ·±åº¦ç®—å­èåˆ (In-flight Batching) ä¸ç¡¬ä»¶æè‡´ä¼˜åŒ–ã€‚ | NVIDIA ç¡¬ä»¶ç¯å¢ƒä¸‹çš„æè‡´ä½å»¶è¿Ÿã€‚ |

## æ€§èƒ½è°ƒä¼˜ (Performance Tuning)

### 1. GPU æ€§èƒ½ä¼˜åŒ–

- **ç®—å­èåˆ (Operator Fusion)**ï¼šå‡å°‘ HBM (æ˜¾å­˜) ä¸ SRAM (è®¡ç®—æ ¸) é—´çš„æ•°æ®æ¬ç§»ã€‚
- **Flash Attention**ï¼šé€šè¿‡åˆ†å—è®¡ç®—ä¼˜åŒ– IO ç“¶é¢ˆã€‚
- **å¹¶è¡Œç­–ç•¥**ï¼šæµæ°´çº¿å¹¶è¡Œ (PP)ã€å¼ é‡å¹¶è¡Œ (TP) ä¸ æ•°æ®å¹¶è¡Œ (DP) çš„ååŒã€‚

### 2. CPU æ€§èƒ½ä¼˜åŒ–

- **æ¨¡å‹é‡åŒ–**ï¼šä½¿ç”¨ **GGUF** æ ¼å¼è¿›è¡Œ 4-bit é‡åŒ–ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å¸¦å®½å‹åŠ›ã€‚
- **SIMD æŒ‡ä»¤é›†**ï¼šåˆ©ç”¨ AVX-512 ç­‰å‘é‡åŒ–æŒ‡ä»¤åŠ é€ŸçŸ©é˜µä¹˜æ³•ã€‚

## é‡åŒ–æ·±åº¦åˆ†æ (Quantization)

- **åé‡åŒ– (PTQ)**ï¼šåœ¨è®­ç»ƒå®Œæˆåç›´æ¥å¯¹æƒé‡/æ¿€æ´»è¿›è¡Œé‡åŒ–ï¼ˆå¦‚ GPTQ, AWQï¼‰ã€‚
- **é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)**ï¼šåœ¨è®­ç»ƒä¸­æ¨¡æ‹Ÿé‡åŒ–è¯¯å·®ï¼Œé€šå¸¸ç²¾åº¦æŸå¤±æœ€å°ã€‚
- **ç²¾åº¦å¯¹æ ‡**ï¼š**å®šç‚¹é‡åŒ– (INT8/INT4)** ä¸ **æµ®ç‚¹é‡åŒ– (FP8)** çš„æ•°å€¼ç¨³å®šæ€§æƒè¡¡ã€‚

## æŠ€æœ¯æ ¸å¿ƒè§£æ

1. **å¦‚ä½•é™ä½é¦–å­—å»¶è¿Ÿ (TTFT)ï¼Ÿ**
   - ä½¿ç”¨ Flash Attentionã€‚
   - Prefill é˜¶æ®µå¹¶è¡ŒåŒ–è®¡ç®—ã€æŠ•æœºé‡‡æ ·éªŒè¯ã€‚
2. **æ˜¾å­˜è¶³å¤Ÿæ—¶ï¼Œå¦‚ä½•æå‡ååï¼Ÿ**
   - ä½¿ç”¨ Continuous Batching (vLLM) åŠ¨æ€è°ƒåº¦ã€‚
   - å¢åŠ å¹¶å‘è¯·æ±‚é‡ï¼Œåˆ©ç”¨ PagedAttention æå‡åˆ©ç”¨ç‡ã€‚
3. **Pined Memory ä¸æ€§èƒ½ï¼Ÿ**
   - é”é¡µå†…å­˜ï¼Œæ¶ˆé™¤ CPU æ•°æ®åˆ° GPU çš„é©±åŠ¨æ‹·è´æŸè€—ï¼Œæå‡æ¬è¿é€Ÿåº¦ã€‚

---

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜

### vLLM éƒ¨ç½²ï¼ˆç”Ÿäº§æ¨èï¼‰

```bash
# å®‰è£…
pip install vllm

# å¯åŠ¨ OpenAI å…¼å®¹ API æœåŠ¡
vllm serve Qwen/Qwen2.5-7B \
    --tensor-parallel-size 2 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.9 \
    --port 8000
```

```python
# Python å®¢æˆ·ç«¯è°ƒç”¨
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")
response = client.chat.completions.create(
    model="Qwen/Qwen2.5-7B",
    messages=[{"role": "user", "content": "ä»€ä¹ˆæ˜¯ PagedAttentionï¼Ÿ"}],
    max_tokens=512,
    temperature=0.7,
)
print(response.choices[0].message.content)
```

### vLLM ç¦»çº¿æ‰¹é‡æ¨ç†

```python
from vllm import LLM, SamplingParams

llm = LLM(model="Qwen/Qwen2.5-7B", tensor_parallel_size=2)
sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=512)

prompts = ["è§£é‡Š KV Cache çš„åŸç†", "PagedAttention å¦‚ä½•å‡å°‘æ˜¾å­˜ç¢ç‰‡ï¼Ÿ"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Response: {output.outputs[0].text}\n")
```

### SGLang éƒ¨ç½²ï¼ˆå¤æ‚ä»»åŠ¡æ¨èï¼‰

```bash
# å®‰è£…
pip install sglang[all]

# å¯åŠ¨æœåŠ¡ï¼ˆè‡ªåŠ¨å¯ç”¨ RadixAttention å‰ç¼€ç¼“å­˜ï¼‰
python -m sglang.launch_server \
    --model-path Qwen/Qwen2.5-7B \
    --tp 2 \
    --port 30000
```

### é‡åŒ–éƒ¨ç½²ï¼ˆGPTQ / AWQï¼‰

```bash
# ä½¿ç”¨ AutoGPTQ é‡åŒ–
pip install auto-gptq

# ç›´æ¥åŠ è½½ç¤¾åŒºå·²é‡åŒ–çš„æ¨¡å‹
vllm serve Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4 \
    --quantization gptq \
    --max-model-len 8192 \
    --port 8000
```

```python
# æˆ–ä½¿ç”¨ transformers åŠ è½½é‡åŒ–æ¨¡å‹
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4",
    device_map="auto",
)
# 7B INT4 é‡åŒ–åä»…éœ€ ~4GB VRAM
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# vLLM å†…ç½® benchmark å·¥å…·
python -m vllm.entrypoints.openai.api_server &

# ä½¿ç”¨ wrk æˆ–å†…ç½®å·¥å…·å‹æµ‹
python -m vllm.benchmark_serving \
    --model Qwen/Qwen2.5-7B \
    --num-prompts 1000 \
    --request-rate 10
```
