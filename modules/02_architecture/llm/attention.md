# Transformer æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanisms)

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šæ³¨æ„åŠ›æœºåˆ¶é€šè¿‡ Q-K åŒ¹é…åŠ¨æ€åˆ†é…ä¿¡æ¯æµï¼ŒGQA/MQA ä¸ FlashAttention åœ¨ç²¾åº¦ä¸æˆæœ¬é—´ç»™å‡ºä¸åŒå·¥ç¨‹æŠ˜ä¸­ã€‚

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šæ¨¡å‹æ¶æ„è®¾è®¡ / æ¨ç†ä¼˜åŒ–ã€‚
- **ç±»å‹**ï¼šç‰¹å¾æå–å†…æ ¸ã€‚
- **ä½œç”¨**ï¼šAttention æ˜¯ Transformer çš„å¿ƒè„ï¼Œè´Ÿè´£å»ºæ¨¡åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»ã€‚æŠ€æœ¯è¯„ä¼°ä¸­å¸¸è€ƒå„ç§å˜ä½“ï¼ˆMHA/GQA/MQAï¼‰ä»¥åŠå·¥ç¨‹ä¼˜åŒ–ï¼ˆFlash Attentionï¼‰ã€‚

## æ ¸å¿ƒå˜ä½“å¯¹æ¯”

| æ¨¡å¼ | å…¨ç§° | é”®å€¼å¯¹å…±äº« (K/V Sharing) | ä¼˜ç‚¹ | ç¼ºç‚¹ |
| :--- | :--- | :--- | :--- | :--- |
| **MHA** | Multi-Head Attention | æ¯ä¸ª Query éƒ½æœ‰ä¸“å±çš„ K, V | è¡¨è¾¾èƒ½åŠ›æœ€å¼º | KV Cache æ˜¾å­˜å ç”¨æå¤§ |
| **MQA** | Multi-Query Attention | æ‰€æœ‰ Query å…±äº«ä¸€ç»„ K, V | æå¤§å‡å°‘æ˜¾å­˜ï¼Œæ¨ç†æå¿« | ç²¾åº¦ä¸‹é™æ˜æ˜¾ï¼ˆå°¤å…¶æ˜¯é•¿æ–‡æœ¬ï¼‰ |
| **GQA** | Grouped-Query Attention | Query åˆ†ç»„ï¼Œæ¯ç»„å…±äº«ä¸€ç»„ K, V | **æŠ˜ä¸­æ–¹æ¡ˆ**ï¼Œç›®å‰ LLM ä¸»æµï¼ˆå¦‚ Llama 3ï¼‰ | å¤æ‚åº¦ä»‹äºä¸¤è€…ä¹‹é—´ |

### ä¸ºä»€ä¹ˆ GQA æ˜¯ç›®å‰çš„ä¸»æµï¼Ÿ

GQA åœ¨ä¿æŒ MHA ç²¾åº¦ï¼ˆå¤šç»„ç‰¹å¾è¡¨è¾¾ï¼‰çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº† KV Cache çš„æ˜¾å­˜å¼€é”€ï¼Œä½¿å¾—é•¿æ–‡æœ¬å¤„ç†å’Œé«˜ååå¹¶å‘æˆä¸ºå¯èƒ½ã€‚

## å·¥ç¨‹ä¼˜åŒ–ï¼šFlash Attention

### æ ¸å¿ƒç—›ç‚¹

ä¼ ç»Ÿçš„ Attention è®¡ç®—å¤æ‚åº¦æ˜¯ $O(N^2)$ï¼Œä¸”åœ¨æ˜¾å­˜å’Œ SRAM ä¹‹é—´é¢‘ç¹è¯»å†™ä¸­é—´çŸ©é˜µ $S = QK^T$ å’Œ $P = \mathrm{softmax}(S)$ï¼Œå¯¼è‡´ **IO å—é™ (Memory Bound)** è€Œéè®¡ç®—å—é™ã€‚

### ä¼˜åŒ–ç­–ç•¥

1. **Tiling (åˆ†å—)**ï¼šå°† $Q, K, V$ åˆ†å—åŠ è½½åˆ° SRAM ä¸­è®¡ç®—ã€‚
2. **Recomputation (é‡è®¡ç®—)**ï¼šåå‘ä¼ æ’­æ—¶ä¸å­˜å‚¨ $N \times N$ çš„ Attention Matrixï¼Œè€Œæ˜¯é‡æ–°è®¡ç®—ï¼Œç”¨è®¡ç®—é‡æ¢æ˜¾å­˜ç©ºé—´ã€‚
3. **IO æ„ŸçŸ¥**ï¼šé€šè¿‡å‡å°‘æ˜¾å­˜è¯»å†™æ¬¡æ•°ï¼Œå®ç° $2\times \sim 4\times$ çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚

## æŠ€æœ¯æ ¸å¿ƒè§£æ

1. **Softmax ä¸ºä»€ä¹ˆéœ€è¦å‡å» Maxï¼Ÿ**
   - ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œé˜²æ­¢æŒ‡æ•°çˆ†ç‚¸æº¢å‡ºã€‚
2. **RoPE (æ—‹è½¬ä½ç½®ç¼–ç ) çš„ä¼˜åŠ¿ï¼Ÿ**
   - å…·å¤‡å¤–æ¨æ€§ï¼ˆRelative Positionï¼‰ï¼Œé€šè¿‡å¤æ•°ä¹˜æ³•å®ç°ï¼Œå¯¹é•¿æ–‡æœ¬å‹å¥½ã€‚
3. **KV Cache æ˜¾å­˜å¦‚ä½•è®¡ç®—ï¼Ÿ**
   - $2 \times \mathrm{layers} \times \mathrm{heads} \times \mathrm{dim} \times \mathrm{precision}$ (é’ˆå¯¹æ¯ä¸ª Token)ã€‚

---

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜

### Flash Attention ä½¿ç”¨

```python
# æ–¹å¼ä¸€ï¼šPyTorch åŸç”Ÿï¼ˆ2.0+ï¼‰
import torch
import torch.nn.functional as F

q = torch.randn(1, 32, 4096, 128, device="cuda", dtype=torch.bfloat16)  # [B, H, N, D]
k = torch.randn(1, 32, 4096, 128, device="cuda", dtype=torch.bfloat16)
v = torch.randn(1, 32, 4096, 128, device="cuda", dtype=torch.bfloat16)

# è‡ªåŠ¨å¯ç”¨ Flash Attentionï¼ˆSDPAï¼‰
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    output = F.scaled_dot_product_attention(q, k, v, is_causal=True)

# æ–¹å¼äºŒï¼šflash-attn åº“
from flash_attn import flash_attn_func

# [B, N, H, D] æ ¼å¼
q = q.transpose(1, 2)  # â†’ [1, 4096, 32, 128]
k = k.transpose(1, 2)
v = v.transpose(1, 2)
output = flash_attn_func(q, k, v, causal=True)
```

### GQA (Grouped-Query Attention) å®ç°

```python
import torch.nn as nn

class GroupedQueryAttention(nn.Module):
    """GQA: Query åˆ†ç»„å…±äº« KVï¼ŒLlama 3 / Qwen2.5 æ ‡é…"""
    def __init__(self, d_model=4096, n_heads=32, n_kv_heads=8):
        super().__init__()
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.head_dim = d_model // n_heads
        self.n_rep = n_heads // n_kv_heads   # æ¯ç»„ KV è¢«å¤šå°‘ä¸ª Q å…±äº«

        self.wq = nn.Linear(d_model, n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(d_model, n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(d_model, n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x):
        B, N, _ = x.shape
        q = self.wq(x).view(B, N, self.n_heads, self.head_dim)
        k = self.wk(x).view(B, N, self.n_kv_heads, self.head_dim)
        v = self.wv(x).view(B, N, self.n_kv_heads, self.head_dim)

        # æ‰©å±• KV å¤´ä»¥åŒ¹é… Q å¤´æ•°é‡
        k = k.repeat_interleave(self.n_rep, dim=2)  # [B, N, 8, D] â†’ [B, N, 32, D]
        v = v.repeat_interleave(self.n_rep, dim=2)

        # è½¬ä¸º [B, H, N, D] ç”¨äº SDPA
        q, k, v = [t.transpose(1, 2) for t in (q, k, v)]
        output = F.scaled_dot_product_attention(q, k, v, is_causal=True)
        output = output.transpose(1, 2).contiguous().view(B, N, -1)
        return self.wo(output)

# å¯¹æ¯”æ˜¾å­˜ï¼šMHA 32 KV heads vs GQA 8 KV heads â†’ KV Cache çœ 75%
```

### RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰

```python
import torch

def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):
    """é¢„è®¡ç®— RoPE é¢‘ç‡"""
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))
    t = torch.arange(end)
    freqs = torch.outer(t, freqs)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # å¤æ•°å½¢å¼
    return freqs_cis

def apply_rotary_emb(xq, xk, freqs_cis):
    """å°† RoPE åº”ç”¨åˆ° Q å’Œ K"""
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(-2)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(-2)
    return xq_out.type_as(xq), xk_out.type_as(xk)

# ç”¨æ³•ï¼šfreqs_cis = precompute_freqs_cis(128, 8192)  # dim=128, max_len=8192
```

---
## å®šä¹‰ä¸ç›®æ ‡

- **å®šä¹‰**ï¼šTransformer æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanisms) å±äºâ€œæ¨¡å‹æ¶æ„æ¨¡å—ï¼Œå…³æ³¨ Transformerã€å¤šæ¨¡æ€ä¸ç”Ÿæˆæœºåˆ¶çš„æ ¸å¿ƒè®¾è®¡ã€‚â€èŒƒç•´ã€‚
- **ç›®æ ‡**ï¼šç†è§£ç»“æ„é€‰æ‹©å¦‚ä½•å½±å“æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€è®­ç»ƒç¨³å®šæ€§ä¸æ¨ç†æ•ˆç‡ã€‚
## é€‚ç”¨åœºæ™¯ä¸è¾¹ç•Œ

- **é€‚ç”¨åœºæ™¯**ï¼šç”¨äºæ¨¡å‹ç»“æ„é€‰å‹ã€æ¨¡å—æ‹†è§£ä¸æ¶æ„åŸç†å­¦ä¹ ã€‚
- **ä¸é€‚ç”¨åœºæ™¯**ï¼šä¸é€‚ç”¨äºè„±ç¦»æ•°æ®ä¸è®­ç»ƒç­–ç•¥å•ç‹¬è¯„ä¼°æœ€ç»ˆèƒ½åŠ›ã€‚
- **ä½¿ç”¨è¾¹ç•Œ**ï¼šå®é™…æ•ˆæœå—å‚æ•°è§„æ¨¡ã€æ•°æ®åˆ†å¸ƒå’Œæ¨ç†ç­–ç•¥å…±åŒå½±å“ã€‚

## å…³é”®æ­¥éª¤

1. ç¡®å®šæ ¸å¿ƒç»“æ„ï¼ˆæ³¨æ„åŠ›ã€ä½ç½®ç¼–ç ã€å½’ä¸€åŒ–ä¸å‰é¦ˆå±‚ï¼‰ã€‚
2. ç»“åˆåºåˆ—é•¿åº¦ä¸æ˜¾å­˜é¢„ç®—è®¾è®¡è®­ç»ƒ/æ¨ç†è·¯å¾„ã€‚
3. é€šè¿‡ç»Ÿä¸€è¯„æµ‹é›†æ¯”è¾ƒç»“æ„å˜ä½“çš„æ•ˆæœä¸æ•ˆç‡å·®å¼‚ã€‚
## å…³é”®å…¬å¼ï¼ˆé€»è¾‘è¡¨è¾¾ï¼‰

`Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V`

ç¬¦å·è¯´æ˜ï¼š
- `Q, K, V`ï¼šæŸ¥è¯¢ã€é”®ã€å€¼å‘é‡ã€‚
- `d_k`ï¼šé”®å‘é‡ç»´åº¦ï¼Œç”¨äºç¼©æ”¾ã€‚
- `softmax`ï¼šå°†æ³¨æ„åŠ›åˆ†æ•°å½’ä¸€åŒ–ã€‚
## å…³é”®æ­¥éª¤ä»£ç ï¼ˆçº¯æ–‡æ¡£ç¤ºä¾‹ï¼‰

```python
# å…³é”®æµç¨‹ç¤ºæ„ï¼ˆä¸å…·ä½“å·¥ç¨‹å®ç°è§£è€¦ï¼‰
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```

## å·¥ç¨‹å®ç°è¦ç‚¹

- ä¼˜å…ˆæ˜ç¡®åºåˆ—é•¿åº¦ã€æ˜¾å­˜é¢„ç®—ä¸ååç›®æ ‡ï¼Œå†åšç»“æ„å†³ç­–ã€‚
- é‡ç‚¹å…³æ³¨ Attention/KV Cache çš„å†…å­˜å¼€é”€ä¸å¹¶è¡Œç­–ç•¥åŒ¹é…ã€‚
- åœ¨ç›¸åŒè¯„æµ‹é›†ä¸Šæ¯”è¾ƒç»“æ„å˜ä½“ï¼Œé¿å…ç»“è®ºè¢«æ•°æ®å·®å¼‚å¹²æ‰°ã€‚

## å¸¸è§é”™è¯¯ä¸æ’æŸ¥

- **ç—‡çŠ¶**ï¼šé•¿åºåˆ—ä¸‹æ˜¾å­˜å¿«é€Ÿçˆ†ç‚¸ã€‚  
  **åŸå› **ï¼šKV Cache ä¸æ³¨æ„åŠ›å¼€é”€è¯„ä¼°ä¸è¶³ã€‚  
  **è§£å†³**ï¼šæå‰åšæ˜¾å­˜é¢„ç®—å¹¶é™åˆ¶ max length æˆ–é‡‡ç”¨æ›´ä¼˜ç¼“å­˜ç­–ç•¥ã€‚
- **ç—‡çŠ¶**ï¼šç»“æ„æ”¹åŠ¨åæ•ˆæœä¸ç¨³å®šã€‚  
  **åŸå› **ï¼šè®­ç»ƒé…ç½®ä¸åˆå§‹åŒ–ç­–ç•¥æœªåŒæ­¥è°ƒæ•´ã€‚  
  **è§£å†³**ï¼šå›ºå®šåŸºçº¿é…ç½®ï¼Œé€é¡¹ ablation å¹¶è®°å½•æ¯æ¬¡æ”¹åŠ¨ã€‚

## ä¸ç›¸è¿‘æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ä¼˜ç‚¹ | å±€é™ | é€‚ç”¨åœºæ™¯ |
| --- | --- | --- | --- |
| æœ¬æ–‡ä¸»é¢˜æ–¹æ³• | ç´§è´´æœ¬èŠ‚é—®é¢˜å®šä¹‰ | ä¾èµ–æ•°æ®ä¸å®ç°è´¨é‡ | é€‚åˆç»“æ„åŒ–è¯„æµ‹ä¸è¿­ä»£ä¼˜åŒ– |
| å¯¹æ¯”æ–¹æ³•A | ä¸Šæ‰‹æˆæœ¬æ›´ä½ | èƒ½åŠ›ä¸Šé™å¯èƒ½å—é™ | å¿«é€ŸåŸå‹ä¸åŸºçº¿å¯¹ç…§ |
| å¯¹æ¯”æ–¹æ³•B | ä¸Šé™æ½œåŠ›æ›´é«˜ | è°ƒå‚ä¸èµ„æºæˆæœ¬æ›´é«˜ | é«˜è¦æ±‚ç”Ÿäº§æˆ–å¤æ‚ä»»åŠ¡åœºæ™¯ |

## å‚è€ƒèµ„æ–™

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [FlashAttention](https://arxiv.org/abs/2205.14135)

