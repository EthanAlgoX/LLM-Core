# Transformer æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanisms)

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šæ¨¡å‹"è¯»æ–‡ç« "æ—¶åŒæ—¶å…³æ³¨å¤šä¸ªè§’åº¦ï¼ŒFlash Attention è®©è¿™æ›´å¿«æ›´çœæ˜¾å­˜

## å®šä½ä¸åˆ†ç±»

- **é˜¶æ®µ**ï¼šæ¨¡å‹æ¶æ„è®¾è®¡ / æ¨ç†ä¼˜åŒ–ã€‚
- **ç±»å‹**ï¼šç‰¹å¾æå–å†…æ ¸ã€‚
- **ä½œç”¨**ï¼šAttention æ˜¯ Transformer çš„å¿ƒè„ï¼Œè´Ÿè´£å»ºæ¨¡åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»ã€‚æŠ€æœ¯è¯„ä¼°ä¸­å¸¸è€ƒå„ç§å˜ä½“ï¼ˆMHA/GQA/MQAï¼‰ä»¥åŠå·¥ç¨‹ä¼˜åŒ–ï¼ˆFlash Attentionï¼‰ã€‚

## æ ¸å¿ƒå˜ä½“å¯¹æ¯”

| æ¨¡å¼ | å…¨ç§° | é”®å€¼å¯¹å…±äº« (K/V Sharing) | ä¼˜ç‚¹ | ç¼ºç‚¹ |
| :--- | :--- | :--- | :--- | :--- |
| **MHA** | Multi-Head Attention | æ¯ä¸ª Query éƒ½æœ‰ä¸“å±çš„ K, V | è¡¨è¾¾èƒ½åŠ›æœ€å¼º | KV Cache æ˜¾å­˜å ç”¨æå¤§ |
| **MQA** | Multi-Query Attention | æ‰€æœ‰ Query å…±äº«ä¸€ç»„ K, V | æå¤§å‡å°‘æ˜¾å­˜ï¼Œæ¨ç†æå¿« | ç²¾åº¦ä¸‹é™æ˜æ˜¾ï¼ˆå°¤å…¶æ˜¯é•¿æ–‡æœ¬ï¼‰ |
| **GQA** | Grouped-Query Attention | Query åˆ†ç»„ï¼Œæ¯ç»„å…±äº«ä¸€ç»„ K, V | **æŠ˜ä¸­æ–¹æ¡ˆ**ï¼Œç›®å‰ LLM ä¸»æµï¼ˆå¦‚ Llama 3ï¼‰ | å¤æ‚åº¦ä»‹äºä¸¤è€…ä¹‹é—´ |

### ä¸ºä»€ä¹ˆ GQA æ˜¯ç›®å‰çš„ä¸»æµï¼Ÿ

GQA åœ¨ä¿æŒ MHA ç²¾åº¦ï¼ˆå¤šç»„ç‰¹å¾è¡¨è¾¾ï¼‰çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº† KV Cache çš„æ˜¾å­˜å¼€é”€ï¼Œä½¿å¾—é•¿æ–‡æœ¬å¤„ç†å’Œé«˜ååå¹¶å‘æˆä¸ºå¯èƒ½ã€‚

## å·¥ç¨‹ä¼˜åŒ–ï¼šFlash Attention

### æ ¸å¿ƒç—›ç‚¹

ä¼ ç»Ÿçš„ Attention è®¡ç®—å¤æ‚åº¦æ˜¯ $O(N^2)$ï¼Œä¸”åœ¨æ˜¾å­˜å’Œ SRAM ä¹‹é—´é¢‘ç¹è¯»å†™ä¸­é—´çŸ©é˜µ $S = QK^T$ å’Œ $P = \mathrm{softmax}(S)$ï¼Œå¯¼è‡´ **IO å—é™ (Memory Bound)** è€Œéè®¡ç®—å—é™ã€‚

### ä¼˜åŒ–ç­–ç•¥

1. **Tiling (åˆ†å—)**ï¼šå°† $Q, K, V$ åˆ†å—åŠ è½½åˆ° SRAM ä¸­è®¡ç®—ã€‚
2. **Recomputation (é‡è®¡ç®—)**ï¼šåå‘ä¼ æ’­æ—¶ä¸å­˜å‚¨ $N \times N$ çš„ Attention Matrixï¼Œè€Œæ˜¯é‡æ–°è®¡ç®—ï¼Œç”¨è®¡ç®—é‡æ¢æ˜¾å­˜ç©ºé—´ã€‚
3. **IO æ„ŸçŸ¥**ï¼šé€šè¿‡å‡å°‘æ˜¾å­˜è¯»å†™æ¬¡æ•°ï¼Œå®ç° $2\times \sim 4\times$ çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚

## æŠ€æœ¯æ ¸å¿ƒè§£æ

1. **Softmax ä¸ºä»€ä¹ˆéœ€è¦å‡å» Maxï¼Ÿ**
   - ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œé˜²æ­¢æŒ‡æ•°çˆ†ç‚¸æº¢å‡ºã€‚
2. **RoPE (æ—‹è½¬ä½ç½®ç¼–ç ) çš„ä¼˜åŠ¿ï¼Ÿ**
   - å…·å¤‡å¤–æ¨æ€§ï¼ˆRelative Positionï¼‰ï¼Œé€šè¿‡å¤æ•°ä¹˜æ³•å®ç°ï¼Œå¯¹é•¿æ–‡æœ¬å‹å¥½ã€‚
3. **KV Cache æ˜¾å­˜å¦‚ä½•è®¡ç®—ï¼Ÿ**
   - $2 \times \mathrm{layers} \times \mathrm{heads} \times \mathrm{dim} \times \mathrm{precision}$ (é’ˆå¯¹æ¯ä¸ª Token)ã€‚

---

## ğŸ› ï¸ å·¥ç¨‹å®æˆ˜

### Flash Attention ä½¿ç”¨

```python
# æ–¹å¼ä¸€ï¼šPyTorch åŸç”Ÿï¼ˆ2.0+ï¼‰
import torch
import torch.nn.functional as F

q = torch.randn(1, 32, 4096, 128, device="cuda", dtype=torch.bfloat16)  # [B, H, N, D]
k = torch.randn(1, 32, 4096, 128, device="cuda", dtype=torch.bfloat16)
v = torch.randn(1, 32, 4096, 128, device="cuda", dtype=torch.bfloat16)

# è‡ªåŠ¨å¯ç”¨ Flash Attentionï¼ˆSDPAï¼‰
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    output = F.scaled_dot_product_attention(q, k, v, is_causal=True)

# æ–¹å¼äºŒï¼šflash-attn åº“
from flash_attn import flash_attn_func

# [B, N, H, D] æ ¼å¼
q = q.transpose(1, 2)  # â†’ [1, 4096, 32, 128]
k = k.transpose(1, 2)
v = v.transpose(1, 2)
output = flash_attn_func(q, k, v, causal=True)
```

### GQA (Grouped-Query Attention) å®ç°

```python
import torch.nn as nn

class GroupedQueryAttention(nn.Module):
    """GQA: Query åˆ†ç»„å…±äº« KVï¼ŒLlama 3 / Qwen2.5 æ ‡é…"""
    def __init__(self, d_model=4096, n_heads=32, n_kv_heads=8):
        super().__init__()
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.head_dim = d_model // n_heads
        self.n_rep = n_heads // n_kv_heads   # æ¯ç»„ KV è¢«å¤šå°‘ä¸ª Q å…±äº«

        self.wq = nn.Linear(d_model, n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(d_model, n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(d_model, n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x):
        B, N, _ = x.shape
        q = self.wq(x).view(B, N, self.n_heads, self.head_dim)
        k = self.wk(x).view(B, N, self.n_kv_heads, self.head_dim)
        v = self.wv(x).view(B, N, self.n_kv_heads, self.head_dim)

        # æ‰©å±• KV å¤´ä»¥åŒ¹é… Q å¤´æ•°é‡
        k = k.repeat_interleave(self.n_rep, dim=2)  # [B, N, 8, D] â†’ [B, N, 32, D]
        v = v.repeat_interleave(self.n_rep, dim=2)

        # è½¬ä¸º [B, H, N, D] ç”¨äº SDPA
        q, k, v = [t.transpose(1, 2) for t in (q, k, v)]
        output = F.scaled_dot_product_attention(q, k, v, is_causal=True)
        output = output.transpose(1, 2).contiguous().view(B, N, -1)
        return self.wo(output)

# å¯¹æ¯”æ˜¾å­˜ï¼šMHA 32 KV heads vs GQA 8 KV heads â†’ KV Cache çœ 75%
```

### RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰

```python
import torch

def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):
    """é¢„è®¡ç®— RoPE é¢‘ç‡"""
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))
    t = torch.arange(end)
    freqs = torch.outer(t, freqs)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # å¤æ•°å½¢å¼
    return freqs_cis

def apply_rotary_emb(xq, xk, freqs_cis):
    """å°† RoPE åº”ç”¨åˆ° Q å’Œ K"""
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(-2)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(-2)
    return xq_out.type_as(xq), xk_out.type_as(xk)

# ç”¨æ³•ï¼šfreqs_cis = precompute_freqs_cis(128, 8192)  # dim=128, max_len=8192
```

---
## å®šä¹‰ä¸ç›®æ ‡

- **å®šä¹‰**ï¼šæœ¬èŠ‚ä¸»é¢˜ç”¨äºè§£é‡Šè¯¥æ¨¡å—çš„æ ¸å¿ƒæ¦‚å¿µä¸å®ç°æ€è·¯ã€‚
- **ç›®æ ‡**ï¼šå¸®åŠ©è¯»è€…å¿«é€Ÿå»ºç«‹é—®é¢˜æŠ½è±¡ã€æ–¹æ³•è·¯å¾„ä¸å·¥ç¨‹è½åœ°æ–¹å¼ã€‚
## å…³é”®æ­¥éª¤

1. æ˜ç¡®è¾“å…¥/è¾“å‡ºä¸ä»»åŠ¡è¾¹ç•Œã€‚
2. æŒ‰æ¨¡å—ä¸»æµç¨‹æ‰§è¡Œæ ¸å¿ƒç®—æ³•æˆ–ç³»ç»Ÿæ­¥éª¤ã€‚
3. è®°å½•æŒ‡æ ‡å¹¶åšå¯¹æ¯”åˆ†æï¼Œå½¢æˆå¯å¤ç”¨ç»“è®ºã€‚
## å…³é”®å…¬å¼ï¼ˆé€»è¾‘è¡¨è¾¾ï¼‰

\[
\text{Result} = \text{Core Method}(\text{Input}, \text{Config}, \text{Constraints})
\]

ç¬¦å·è¯´æ˜ï¼š
- \(\text{Input}\)ï¼šä»»åŠ¡è¾“å…¥ã€‚
- \(\text{Config}\)ï¼šè®­ç»ƒæˆ–æ¨ç†é…ç½®ã€‚
- \(\text{Constraints}\)ï¼šæ–¹æ³•çº¦æŸï¼ˆå¦‚èµ„æºã€ç¨³å®šæ€§æˆ–å®‰å…¨è¾¹ç•Œï¼‰ã€‚
## å…³é”®æ­¥éª¤ä»£ç ï¼ˆçº¯æ–‡æ¡£ç¤ºä¾‹ï¼‰

```python
# å…³é”®æµç¨‹ç¤ºæ„ï¼ˆä¸å…·ä½“å·¥ç¨‹å®ç°è§£è€¦ï¼‰
state = init_state()
for step in range(num_steps):
    state = step_update(state)
metrics = evaluate(state)
```
