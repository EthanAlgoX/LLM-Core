# Transformer Core (å¤§è¯­è¨€æ¨¡åž‹æž¶æž„)

> [!TIP]
> **ä¸€å¥è¯é€šä¿—ç†è§£**ï¼šLLM çš„"èº«ä½“"â€”â€”æ¯ä¸ªè¯æ€Žä¹ˆè¢«æ„ŸçŸ¥å¹¶ç¼–ç æˆå‘é‡

LLM çš„æ ¸å¿ƒæž¶æž„æ˜¯ Transformer Decoder-Only ç»“æž„ã€‚ç†è§£å…¶æ¯ä¸ªç»„ä»¶çš„è®¾è®¡åŠ¨æœºæ˜¯ç†è§£æ‰€æœ‰åŽç»­æŠ€æœ¯çš„åŸºç¡€ã€‚

> **æ ¸å¿ƒå…¬å¼**ï¼š $\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$

---

## æ ¸å¿ƒç»„ä»¶è§£æž

### 1. Multi-Head Self-Attention (MHSA)

- **åŠ¨æœº**ï¼šå•å¤´æ³¨æ„åŠ›åªèƒ½æ•èŽ·ä¸€ç§å…³è”æ¨¡å¼ï¼Œå¤šå¤´å¹¶è¡Œå…è®¸æ¨¡åž‹åŒæ—¶å…³æ³¨ä¸åŒå­ç©ºé—´çš„è¯­ä¹‰å…³ç³»ã€‚
- **è®¡ç®—æµç¨‹**ï¼š
  1. è¾“å…¥ $X$ é€šè¿‡ä¸‰ä¸ªç‹¬ç«‹çº¿æ€§å˜æ¢å¾—åˆ° $Q, K, V$
  2. æ¯ä¸ª Head ç‹¬ç«‹è®¡ç®— Scaled Dot-Product Attention
  3. æ‰€æœ‰ Head çš„è¾“å‡ºæ‹¼æŽ¥åŽç»çº¿æ€§æŠ•å½±å¾—åˆ°æœ€ç»ˆè¾“å‡º
- **å…³é”®å‚æ•°**ï¼š`num_heads`ã€`head_dim = d_model / num_heads`

### 2. ä½ç½®ç¼–ç  (Positional Encoding)

| æ–¹æ¡ˆ | åŽŸç† | ä»£è¡¨æ¨¡åž‹ |
| --- | --- | --- |
| **Sinusoidal (ç»å¯¹)** | å›ºå®šæ­£å¼¦/ä½™å¼¦å‡½æ•°ï¼Œä¸å¯å­¦ä¹  | åŽŸå§‹ Transformer |
| **Learned (å¯å­¦ä¹ )** | æ¯ä¸ªä½ç½®å¯¹åº”ä¸€ä¸ªå¯è®­ç»ƒå‘é‡ | GPT-2, BERT |
| **RoPE (æ—‹è½¬)** | é€šè¿‡æ—‹è½¬çŸ©é˜µç¼–ç ç›¸å¯¹ä½ç½®ï¼Œå¤–æŽ¨æ€§å¼º | LLaMA, Qwen |
| **ALiBi** | åœ¨ Attention åˆ†æ•°ä¸ŠåŠ çº¿æ€§åç½®ï¼Œæ— éœ€ä¿®æ”¹åµŒå…¥ | MPT, BLOOM |

### 3. å½’ä¸€åŒ–ç­–ç•¥ (Normalization)

- **Post-LN**ï¼ˆåŽŸå§‹ Transformerï¼‰ï¼šæ¢¯åº¦ä¸ç¨³å®šï¼Œæ·±å±‚ç½‘ç»œéš¾ä»¥è®­ç»ƒã€‚
- **Pre-LN**ï¼ˆçŽ°ä»£ LLM æ ‡å‡†ï¼‰ï¼šå°† LayerNorm ç§»è‡³æ®‹å·®è¿žæŽ¥ä¹‹å‰ï¼Œæ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ã€‚
- **RMSNorm**ï¼šåŽ»æŽ‰å‡å€¼ä¸­å¿ƒåŒ–ï¼Œåªåš RMS ç¼©æ”¾ï¼Œè®¡ç®—æ›´é«˜æ•ˆï¼Œæ•ˆæžœç›¸å½“ã€‚

### 4. FFN å±‚ (Feed-Forward Network)

- **æ ‡å‡† FFN**ï¼š $\mathrm{FFN}(x) = \mathrm{ReLU}(xW_1 + b_1)W_2 + b_2$ï¼Œéšå±‚ç»´åº¦é€šå¸¸ä¸º $4 \times d_{\mathrm{model}}$ã€‚
- **SwiGLU**ï¼ˆLLaMA ç³»åˆ—ï¼‰ï¼š $\mathrm{SwiGLU}(x) = (\mathrm{Swish}(xW_1) \odot xW_2)W_3$ï¼Œé—¨æŽ§æœºåˆ¶æå‡è¡¨è¾¾èƒ½åŠ›ã€‚

### 5. KV Cacheï¼ˆæŽ¨ç†å…³é”®ï¼‰

- **é—®é¢˜**ï¼šè‡ªå›žå½’ç”Ÿæˆæ—¶ï¼Œæ¯æ­¥éƒ½éœ€è¦é‡æ–°è®¡ç®—æ‰€æœ‰åŽ†å² Token çš„ K/Vï¼Œè®¡ç®—å†—ä½™ã€‚
- **æ–¹æ¡ˆ**ï¼šå°†å·²è®¡ç®—çš„ K/V ç¼“å­˜èµ·æ¥ï¼Œæ¯æ­¥åªè®¡ç®—æ–° Token çš„ K/Vå¹¶è¿½åŠ ã€‚
- **ä»£ä»·**ï¼šæ˜¾å­˜å ç”¨éšåºåˆ—é•¿åº¦çº¿æ€§å¢žé•¿ï¼š $2 \times L \times H \times d \times \mathrm{precision}$ ã€‚

### 6. æ‰©å±•æž¶æž„ï¼šMoE (Mixture of Experts)

å½“ Dense æ¨¡åž‹è§„æ¨¡è¾¾åˆ°ç“¶é¢ˆæ—¶ï¼ŒMoE é€šè¿‡ç¨€ç–åŒ–æå‡å‚æ•°å®¹é‡ã€‚

- **æ ¸å¿ƒç»„ä»¶**ï¼š
  1. **Gate (Router)**ï¼šå†³å®šè¾“å…¥ Token åˆ†é…ç»™å“ªå‡ ä¸ªä¸“å®¶ã€‚
  2. **Experts**ï¼šä¸€ç³»åˆ—ç‹¬ç«‹çš„ FFN å±‚ã€‚
- **å¹¶è¡Œç­–ç•¥ï¼šExpert Parallelism (EP)**ï¼š
  - å°†ä¸åŒçš„ä¸“å®¶åˆ†å¸ƒåœ¨ä¸åŒçš„ GPU ä¸Šã€‚
  - **é€šä¿¡åŽ‹åŠ›**ï¼šå¼•å…¥ **All-to-All** é€šä¿¡ï¼Œåœ¨è·¯ç”± Token æ—¶äº§ç”Ÿå·¨å¤§å¼€é”€ã€‚
- **å…³é”®æŒ‘æˆ˜**ï¼š
  1. **Load Balancing**ï¼šä¸“å®¶åˆ©ç”¨çŽ‡ä¸å‡å¯¼è‡´è®¡ç®—é•¿å°¾ã€‚å¸¸ç”¨ **è¾…åŠ©æŸå¤± (Auxiliary Loss)** å¼ºåˆ¶å‡è¡¡ã€‚
  2. **ç¨€ç–è®¡ç®—ä¼˜åŒ–**ï¼šéœ€è¦å®šåˆ¶çš„å¹¶è¡Œå†…æ ¸ï¼ˆå¦‚ **Fused MoE Kernels**ï¼‰å‡å°‘ç¢Žç‰‡åŒ–è®¡ç®—ã€‚

---

## å·¥ç¨‹å®žçŽ°è¦ç‚¹

- **å‚æ•°é‡ä¼°ç®—**ï¼š $\approx 12 \times d_{\mathrm{model}}^2 \times n_{\mathrm{layers}}$ ï¼ˆå¿½ç•¥ embeddingï¼‰
- **è®¡ç®—é‡ä¼°ç®—**ï¼š $\approx 6 \times N \times T$ FLOPsï¼ˆN=å‚æ•°é‡ï¼ŒT=åºåˆ—é•¿åº¦ï¼‰
- **æ˜¾å­˜åˆ†è§£**ï¼šæƒé‡ + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ¿€æ´»å€¼ï¼Œè®­ç»ƒæ—¶çº¦ä¸ºæŽ¨ç†çš„ 12-16 å€

---

## ðŸ“‚ æ¨¡å—å®žæˆ˜

```python
# å…³é”®æ­¥éª¤ä»£ç ï¼ˆçº¯æ–‡æ¡£ç¤ºä¾‹ï¼‰
x = token_embedding(input_ids)  # [B, T, d_model]
for block in transformer_blocks:
    # Attention å­å±‚ï¼šRMSNorm -> RoPE -> Self-Attention -> Residual
    x = x + block.self_attn(block.rmsnorm_1(x), rope_cache)
    # FFN å­å±‚ï¼šRMSNorm -> MLP -> Residual
    x = x + block.mlp(block.rmsnorm_2(x))

logits = lm_head(final_rmsnorm(x))  # [B, T, vocab_size]
```
